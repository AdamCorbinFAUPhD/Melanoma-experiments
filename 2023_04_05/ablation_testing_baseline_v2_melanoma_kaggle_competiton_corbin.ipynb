{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamCorbinFAUPhD/Melanoma-experiments/blob/main/2023_04_05/ablation_testing_baseline_v2_melanoma_kaggle_competiton_corbin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBFAK6VTD7EF"
      },
      "source": [
        "\n",
        "# 1. Introduction\n",
        "\n",
        "This notebook will cover exploring melanoma classification using the Kaggle melanoma 2020 competition \n",
        "\n",
        "Models used:\n",
        "\n",
        "Datasets used:\n",
        "\n",
        "Experiments to be saved here: https://github.com/AdamCorbinFAUPhD/Melanoma-experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA1d5KVUa_b"
      },
      "source": [
        "## TODO tasks\n",
        "- [X] (DONE 1/2/2023) Check to see if the masks are flipped and rotated?\n",
        "- [X] (DONE 1/3/2023) Create test dataset from [ISIC 2016 task 3](https://challenge.isic-archive.com/landing/2016/39/) skin leasion classificaiton\n",
        "  - [X] (DONE 1/3/2023) Need to complete Fitzpatric type on ISIC 2016\n",
        "  - [X] (DONE 1/3/2023) Maybe need to compute masks as well\n",
        "- [X] (DONE 1/3/2023) Fix the confusion matrix for each skin type\n",
        "- [X] (DONE 1/3/2023) Create sensitvity and specifity metrics\n",
        "- [X] (DONE 1/3/2023) Split up melanoma dataset to have a validation dataset\n",
        "- [X] (DONE 1/4/2023) Retrain wiht new test set\n",
        "- [X] Revalidate using new validation set\n",
        "- [X] (DONE 1/3/2023) Create fairness metrics \n",
        "  - [X] (DONE 1/3/2023) Statistical parity \n",
        "  - [X] (DONE 1/3/2023) Disparate impact \n",
        "  - [X] (DONE 1/3/2023) Equalized odds\n",
        "  - [X] (DONE 1/3/2023) Equal opportunity\n",
        "  - [X] (DONE 1/3/2023) Predictive rate parity     \n",
        "  - [X] (DONE 2/10/2023) Absolute Between-ROC Area\n",
        "- [X] (DONE 2/10/2023) Compute ROC curve\n",
        "- [X] (DONE 2/10/2023) Test using ResNet50\n",
        "- [X] Implement CIRCLe regurlization \n",
        "- [X] Get metrics per FST\n",
        "- [X] Select 5 melanoma and non-melanoma examples to follow through all of these experiments and to view their grad cam results\n",
        "- [ ] Possible look at different XAI if time permits\n",
        "\n",
        "\n",
        "New todos from 1/13\n",
        "eda\n",
        "- [X] get 1 good and 1 bad skin test. Selected 4 per FST\n",
        "- [X] create validation test with 50 of each type\n",
        "Ml testing\n",
        "- [X] select the baseline - ResNet50\n",
        "- [X] (DONE 2/17/2023) some synthetic data from the color transformer\n",
        "- [X] (1) re-run test using new validation dataset. Evaluate as a whole\n",
        "- [X] (2) evaluate based on the FST\n",
        "- [X] (3) get x-ai stuff working again\n",
        "- [X] (3) Select x-ai examples to use for paper. 1 good and 1 bad\n",
        "- [X] (4) train & test using synthetic data, maybe a mix with some real images.\n",
        " - The idea is with in the dataloader have a random percentage to selecte between synthetic vs non. \n",
        "- [X] (4) train using reguralization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L9FO4xtOVO-q"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Constants\n",
        "use_data_cached = True\n",
        "generate_masks = False\n",
        "output_size = 1\n",
        "num_workers = 2\n",
        "mask_path = Path(\"/input/melanoma_masks/results1\")\n",
        "\n",
        "regularization_alpha = 0.1\n",
        "generate_synthetic_dataset_flag = False\n",
        "\n",
        "\n",
        "synthetici_data_path = Path(\"/content/kaggle_2020/transformed/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Different ablasion testing\n",
        "model_type = \"b2\" #b2, b4, resnet\n",
        "\n",
        "# when use_synthetic_data_percentage is 0 and use_regularization == False then we are in baseline mode\n",
        "use_synthetic_data_percentage = 0.0\n",
        "use_regularization = False\n",
        "#----\n",
        "batch_size1 = 16 # training batch size 8, 16, 24, 32\n",
        "batch_size2 = 16 # validation batch size\n",
        "\n",
        "\n",
        "selected_optimizer = \"adam\" # adamax, adamw, nadam\n",
        "learning_rate = 0.0005 #.0005 .001 .0015 .002\n",
        "weight_decay = 0.0001\n"
      ],
      "metadata": {
        "id": "fi-bxUS7phYz"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of-muz7MzCnL",
        "outputId": "96901a9e-3d53-435a-efd3-30172ac4273a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CHIMmdjK4J5"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8ZR3uJ1ND7EI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d598db6-8238-4538-9818-0906113e9403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q efficientnet_pytorch             # Convolutional Neural Net from Google Research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0rJ_CkQJRqp",
        "outputId": "e2df0afe-2438-46df-ba76-68ee84238345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting captum\n",
            "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.9/dist-packages (from captum) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from captum) (1.22.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6->captum) (3.10.7)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->captum) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (5.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (4.39.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->captum) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrvTXlWLOa6",
        "outputId": "dfc0efef-8041-4f5f-a1d5-b8d9dd3acdc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.4.6.tar.gz (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from grad-cam) (3.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from grad-cam) (8.4.0)\n",
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from grad-cam) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from grad-cam) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from grad-cam) (1.22.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from grad-cam) (4.7.0.72)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.9/dist-packages (from grad-cam) (0.15.1+cu118)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from grad-cam) (2.0.0+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->grad-cam) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->grad-cam) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->grad-cam) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->grad-cam) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->grad-cam) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->grad-cam) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.1->grad-cam) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.1->grad-cam) (16.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.8.2->grad-cam) (2.27.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (4.39.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->grad-cam) (5.12.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->grad-cam) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->grad-cam) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->grad-cam) (1.10.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->grad-cam) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.4.6-py3-none-any.whl size=38262 sha256=b911b4b5179a740876e20ceb20016f0f292e44844b01e08c1666baab0bed96f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/ae/bd/221d138b169c4867b2ae5c893474d0b23bae6e4750bc95bc55\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.4.6 ttach-0.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FrZupQMUD_j",
        "outputId": "0ddb9042-0c1a-40ee-cbf7-5b61ac477d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting derm-ita\n",
            "  Downloading derm_ita-0.0.8-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: setuptools>=57 in /usr/local/lib/python3.9/dist-packages (from derm-ita) (67.6.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from derm-ita) (1.22.4)\n",
            "Requirement already satisfied: scikit-image>=0.19 in /usr/local/lib/python3.9/dist-packages (from derm-ita) (0.19.3)\n",
            "Requirement already satisfied: Pillow>=8.4 in /usr/local/lib/python3.9/dist-packages (from derm-ita) (8.4.0)\n",
            "Collecting patchify>=0.2.3\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19->derm-ita) (2023.3.21)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19->derm-ita) (1.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19->derm-ita) (23.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19->derm-ita) (2.25.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19->derm-ita) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.19->derm-ita) (3.0)\n",
            "Installing collected packages: patchify, derm-ita\n",
            "Successfully installed derm-ita-0.0.8 patchify-0.2.3\n"
          ]
        }
      ],
      "source": [
        "# only needed if we need to re-generate Fitzpatrick info\n",
        "!pip install derm-ita\n",
        "from derm_ita import get_ita, get_kinyanjui_type, get_cropped_center_ita, get_kinyanjui_groh_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FENBRRcBK6Xp"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LjpoYzayD7EK"
      },
      "outputs": [],
      "source": [
        "# System\n",
        "import cv2\n",
        "import os, os.path\n",
        "from PIL import Image              # from RBG to YCbCr\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from enum import Enum\n",
        "import io\n",
        "\n",
        "# Basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg    # to check images\n",
        "# %matplotlib inline\n",
        "from tqdm.notebook import tqdm      # beautiful progression bar\n",
        "\n",
        "# SKlearn\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import FloatTensor, LongTensor\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "from torch.optim import adam, adamax, nadam, adamw\n",
        "\n",
        "# Data Augmentation for Image Preprocessing\n",
        "from albumentations import (ToFloat, Normalize, VerticalFlip, HorizontalFlip, Compose, Resize,\n",
        "                            RandomBrightnessContrast, HueSaturationValue, Blur, GaussNoise,\n",
        "                            Rotate, RandomResizedCrop, Cutout, ShiftScaleRotate)\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torchvision.models import resnet34, resnet50\n",
        "\n",
        "# images\n",
        "import PIL\n",
        "from PIL import ImageStat\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "\n",
        "# XAI\n",
        "from captum.attr import GuidedGradCam\n",
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, BinaryClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS0I89mTEuS4",
        "outputId": "6150c507-040a-4553-b901-b2266c6553d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "275PT4z1D7EK"
      },
      "source": [
        "### Set the Seeds ðŸŒ±\n",
        "> For reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbsja9hMD7EL",
        "outputId": "582e42c5-6632-4fe5-c490-7a2779479734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available now: cuda\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed = 1234):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device available now:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZjfeT6D7EL"
      },
      "source": [
        "# 2. Data Preparation ðŸ“ðŸ“‚\n",
        "\n",
        "> The data we'll work on has a Train and Test .csv files with coresponding .jpg images. Number of data points also increased with other [external sources.](https://www.kaggle.com/nroman/melanoma-external-malignant-256)\n",
        "<img src='https://i.imgur.com/Z06dlVw.png' width=500>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BkDcwhDRy1d"
      },
      "source": [
        "## 2.1 Read in the data ðŸ”Ž"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxjCSXLVRsh2"
      },
      "source": [
        "### Upload kaggle secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jp_uyKL1LXIH"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GmFyWprlL5Vg"
      },
      "outputs": [],
      "source": [
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/kaggle.json /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "O0z7E9oWMAy-"
      },
      "outputs": [],
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G21iS56LczV",
        "outputId": "dd95a387-bd3a-49e8-b0af-dd85a5a208e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 66 Apr  5 09:37 kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!ls -l ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Inlg7ic9ECh8"
      },
      "outputs": [],
      "source": [
        "#!kaggle datasets download -d cdeotte/jpeg-melanoma-256x256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw7PfGWPTqa8",
        "outputId": "b6d4316f-329a-4fb7-e7f2-113a31a3341f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading siim-melanoma-prep-data.zip to /content\n",
            " 98% 66.0M/67.2M [00:04<00:00, 19.2MB/s]\n",
            "100% 67.2M/67.2M [00:04<00:00, 14.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d andradaolteanu/siim-melanoma-prep-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "q3P92OU8WQCY"
      },
      "outputs": [],
      "source": [
        "!mkdir /input\n",
        "!mkdir /working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vl9p8HFWM1AD"
      },
      "outputs": [],
      "source": [
        "!unzip -qq siim-melanoma-prep-data -d ../input/siim-melanoma-prep-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqewX2NiT7PY",
        "outputId": "dd6c04ae-84f4-41d2-ccf0-a60caa795e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading melanoma-external-malignant-256.zip to /content\n",
            "100% 1.01G/1.01G [00:54<00:00, 20.3MB/s]\n",
            "100% 1.01G/1.01G [00:54<00:00, 20.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d nroman/melanoma-external-malignant-256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XFewc7k8T-ih"
      },
      "outputs": [],
      "source": [
        "!unzip -qq melanoma-external-malignant-256 -d ../input/melanoma-external-malignant-256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IWSQtorelLR"
      },
      "source": [
        "### Getting data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "S3jHsHUPeoz5"
      },
      "outputs": [],
      "source": [
        "# cached dataframe\n",
        "#%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/saved_data_2023_01_28_kaggle_melanoma_2020.csv /content/\n",
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/saved_data_2023_02_14_melanoma_2020_kaggle.csv /content/\n",
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/saved_data_2023_02_02_ISIC_2016_test_set.csv /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DBRRxnZKerbA"
      },
      "outputs": [],
      "source": [
        "# generated masks\n",
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/kaggle_melanoma_2020_mask_results1_2023_01_28.zip /content/\n",
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/isic_2016_task3_mask_results2_2023_02_02.zip /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F9f5xhPce79p"
      },
      "outputs": [],
      "source": [
        "!unzip -qq kaggle_melanoma_2020_mask_results1_2023_01_28 -d ../input/melanoma_masks\n",
        "!unzip -qq isic_2016_task3_mask_results2_2023_02_02 -d ../input/isic_2016_masks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/kaggle_2020_transformed_images__2023_02_17.zip /content/"
      ],
      "metadata": {
        "id": "Y2OLyEN8AB8i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq kaggle_2020_transformed_images__2023_02_17.zip -d /content/"
      ],
      "metadata": {
        "id": "95N4D-AeAHzX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPdThRsOYWW2"
      },
      "source": [
        "### Get ISIC 2016 Test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qdBFQxefvzj",
        "outputId": "22dc5855-6402-4fe9-857c-a13d2eda46ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 09:39:30--  https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_Data.zip\n",
            "Resolving isic-challenge-data.s3.amazonaws.com (isic-challenge-data.s3.amazonaws.com)... 3.5.2.122, 52.216.154.252, 52.217.197.1, ...\n",
            "Connecting to isic-challenge-data.s3.amazonaws.com (isic-challenge-data.s3.amazonaws.com)|3.5.2.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 631625308 (602M) [application/zip]\n",
            "Saving to: â€˜ISBI2016_ISIC_Part3_Training_Data.zipâ€™\n",
            "\n",
            "ISBI2016_ISIC_Part3 100%[===================>] 602.36M  12.5MB/s    in 51s     \n",
            "\n",
            "2023-04-05 09:40:22 (11.9 MB/s) - â€˜ISBI2016_ISIC_Part3_Training_Data.zipâ€™ saved [631625308/631625308]\n",
            "\n",
            "--2023-04-05 09:40:22--  https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_GroundTruth.csv\n",
            "Resolving isic-challenge-data.s3.amazonaws.com (isic-challenge-data.s3.amazonaws.com)... 52.217.116.249, 3.5.8.210, 3.5.0.126, ...\n",
            "Connecting to isic-challenge-data.s3.amazonaws.com (isic-challenge-data.s3.amazonaws.com)|52.217.116.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19419 (19K) [text/csv]\n",
            "Saving to: â€˜ISBI2016_ISIC_Part3_Training_GroundTruth.csvâ€™\n",
            "\n",
            "ISBI2016_ISIC_Part3 100%[===================>]  18.96K  82.2KB/s    in 0.2s    \n",
            "\n",
            "2023-04-05 09:40:23 (82.2 KB/s) - â€˜ISBI2016_ISIC_Part3_Training_GroundTruth.csvâ€™ saved [19419/19419]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_Data.zip\n",
        "!wget https://isic-challenge-data.s3.amazonaws.com/2016/ISBI2016_ISIC_Part3_Training_GroundTruth.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "c9jZUKOMg8yw"
      },
      "outputs": [],
      "source": [
        "!unzip -qq ISBI2016_ISIC_Part3_Training_Data.zip -d ../input/isic_2016"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRH0KAk-fy7y"
      },
      "source": [
        "## Organize data into dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BxmCKng9D7EL"
      },
      "outputs": [],
      "source": [
        "# ----- STATICS -----\n",
        "output_size = 1\n",
        "# -------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjt5toXKD7EM",
        "outputId": "61bc074a-657d-47b8-b725-6de65f21356c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from cached data\n",
            "Splitting up the dataset into train,test, validation datasets based on the skin condition, FSK, and patident ID\n",
            "total_train: 28233 74.99203144921377\n",
            "total_val: 9415 25.00796855078623\n",
            "Len Train: 28,233 \n",
            "Len Test: 10,982\n",
            "Len Vald: 9,415\n",
            "Len everything: 37,648\n"
          ]
        }
      ],
      "source": [
        "to_drop = ['path_dicom','path_jpeg', 'diagnosis']\n",
        "\n",
        "if not use_data_cached:\n",
        "    # My Train: with imputed missing values + OHE\n",
        "    my_train = pd.read_csv('../input/siim-melanoma-prep-data/train_clean.csv')\n",
        "\n",
        "    # Drop path columns and Diagnosis (it won't be available during TEST)\n",
        "    # We'll rewrite them once the data is concatenated    \n",
        "    for drop in to_drop:\n",
        "        if drop in my_train.columns :\n",
        "            my_train.drop([drop], axis=1, inplace=True)\n",
        "\n",
        "    # Roman's Train: with added data for Malignant category\n",
        "    roman_train = pd.read_csv('../input/melanoma-external-malignant-256/train_concat.csv')\n",
        "\n",
        "\n",
        "    # --- Before concatenatenating both together, let's preprocess roman_train ---\n",
        "    # Replace NAN with 0 for patient_id\n",
        "    roman_train['patient_id'] = roman_train['patient_id'].fillna(0)\n",
        "\n",
        "    # OHE\n",
        "    to_encode = ['sex', 'anatom_site_general_challenge']\n",
        "    encoded_all = []\n",
        "\n",
        "    roman_train[to_encode[0]] = roman_train[to_encode[0]].astype(str)\n",
        "    roman_train[to_encode[1]] = roman_train[to_encode[1]].astype(str)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    for column in to_encode:\n",
        "        encoded = label_encoder.fit_transform(roman_train[column])\n",
        "        encoded_all.append(encoded)\n",
        "        \n",
        "    roman_train[to_encode[0]] = encoded_all[0]\n",
        "    roman_train[to_encode[1]] = encoded_all[1]\n",
        "\n",
        "    # Give all columns the same name\n",
        "    roman_train.columns = my_train.columns\n",
        "\n",
        "\n",
        "    # --- Concatenate info which is not available in my_train ---\n",
        "    common_images = my_train['dcm_name'].unique()\n",
        "    new_data = roman_train[~roman_train['dcm_name'].isin(common_images)]\n",
        "\n",
        "    # Merge all together\n",
        "    train_df = pd.concat([my_train, new_data], axis=0)\n",
        "else:\n",
        "    print(\"Loading from cached data\")\n",
        "    train_df = pd.read_csv('./saved_data_2023_02_14_melanoma_2020_kaggle.csv')    \n",
        "    test_isic_2016_df = pd.read_csv(\"./saved_data_2023_02_02_ISIC_2016_test_set.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Read in Test data (also cleaned, imputed, OHE) ---\n",
        "test_df = pd.read_csv('../input/siim-melanoma-prep-data/test_clean.csv')\n",
        "\n",
        "# Drop columns\n",
        "for drop in to_drop:\n",
        "    if drop in test_df.columns :\n",
        "        test_df.drop([drop], axis=1, inplace=True)\n",
        "\n",
        "# Create path column to image folder for both Train and Test\n",
        "path_train = '../input/melanoma-external-malignant-256/train/train/'\n",
        "path_test = '../input/melanoma-external-malignant-256/test/test/'\n",
        "\n",
        "if not use_data_cached: \n",
        "    train_df['path_jpg'] = path_train + train_df['dcm_name'] + '.jpg'\n",
        "test_df['path_jpg'] = path_test + test_df['dcm_name'] + '.jpg'\n",
        "\n",
        "\n",
        "# --- Last final thing: NORMALIZE! ---\n",
        "if not use_data_cached:\n",
        "    train_df['age'] = train_df['age'].fillna(-1)\n",
        "    normalized_train = preprocessing.normalize(train_df[['sex', 'age', 'anatomy']])\n",
        "    train_df['sex'] = normalized_train[:, 0]\n",
        "    train_df['age'] = normalized_train[:, 1]\n",
        "    train_df['anatomy'] = normalized_train[:, 2]\n",
        "\n",
        "\n",
        "normalized_test = preprocessing.normalize(test_df[['sex', 'age', 'anatomy']])\n",
        "\n",
        "test_df['sex'] = normalized_test[:, 0]\n",
        "test_df['age'] = normalized_test[:, 1]\n",
        "test_df['anatomy'] = normalized_test[:, 2]\n",
        "\n",
        "train_indexes = []\n",
        "val_indexes = []\n",
        "\n",
        "train_df = train_df.rename(columns={'fizpatrick_skin_type_masked': 'fitzpatrick_skin_type_masked'})\n",
        "train_df = train_df.rename(columns={'fizpatrick_skin_type': 'fitzpatrick_skin_type'})\n",
        "train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "\n",
        "print(\"Splitting up the dataset into train,test, validation datasets based on the skin condition, FSK, and patident ID\")\n",
        "grouped = train_df.groupby([\"target\", \"fitzpatrick_skin_type\"])\n",
        "\n",
        "group_indexes = grouped.indices\n",
        "\n",
        "train_indexes = []\n",
        "test_indexes = []\n",
        "val_indexes = []\n",
        "\n",
        "for group, index_list in group_indexes.items():\n",
        "    # print(f\"index_list: {len(index_list)}\")\n",
        "    index_test = []\n",
        "    index_val = []\n",
        "    if len(index_list) > 1:\n",
        "        index_train, index_test, _, _ = train_test_split(index_list, index_list, test_size=0.25, random_state=42)\n",
        "        train_indexes += list(index_train)\n",
        "        val_indexes += list(index_test)\n",
        "    elif len(index_list) == 1:\n",
        "        train_indexes += list(index_list)\n",
        "\n",
        "print(f\"total_train: {len(train_indexes)} {len(train_indexes) / len(train_df) * 100}\")\n",
        "print(f\"total_val: {len(val_indexes)} {len(val_indexes) / len(train_df) * 100}\")\n",
        "\n",
        "\n",
        "\n",
        "everything_df = train_df\n",
        "\n",
        "valid_df = train_df.iloc[val_indexes]\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "\n",
        "train_df = train_df.iloc[train_indexes]\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "print('Len Train: {:,}'.format(len(train_df)), '\\n' +\n",
        "      'Len Test: {:,}\\n'.format(len(test_df)) +\n",
        "      f\"Len Vald: {len(valid_df):,}\\n\" +\n",
        "      f\"Len everything: {len(everything_df):,}\")\n",
        "\n",
        "\n",
        "# Yay!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UepScBoAh2e8",
        "outputId": "1b430ec6-3abd-4e7c-ece2-b651dc39f3cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       dcm_name          ID       sex       age   anatomy  target  \\\n",
              "0  ISIC_6507492  IP_6134378  0.028375  0.993133  0.113501       0   \n",
              "1  ISIC_1985984  IP_1645936  0.000000  0.999882  0.015383       0   \n",
              "2  ISIC_7233663  IP_3735763  0.019992  0.999600  0.019992       0   \n",
              "3  ISIC_3003627  IP_4898383  0.033023  0.990687  0.132092       0   \n",
              "4  ISIC_0886205  IP_2153088  0.000000  0.991228  0.132164       0   \n",
              "\n",
              "                                            path_jpg        ita  \\\n",
              "0  ../input/melanoma-external-malignant-256/train...  71.273186   \n",
              "1  ../input/melanoma-external-malignant-256/train...  79.947555   \n",
              "2  ../input/melanoma-external-malignant-256/train...  71.449125   \n",
              "3  ../input/melanoma-external-malignant-256/train...  85.556576   \n",
              "4  ../input/melanoma-external-malignant-256/train...  89.958897   \n",
              "\n",
              "   fitzpatrick_skin_type  ita_masked  fitzpatrick_skin_type_masked  \n",
              "0                      1   71.145157                             1  \n",
              "1                      1   79.705372                             1  \n",
              "2                      1   71.544231                             1  \n",
              "3                      1   85.091278                             1  \n",
              "4                      1   89.804639                             1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b2961da-93c7-4985-a25f-cced12f577a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dcm_name</th>\n",
              "      <th>ID</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>anatomy</th>\n",
              "      <th>target</th>\n",
              "      <th>path_jpg</th>\n",
              "      <th>ita</th>\n",
              "      <th>fitzpatrick_skin_type</th>\n",
              "      <th>ita_masked</th>\n",
              "      <th>fitzpatrick_skin_type_masked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_6507492</td>\n",
              "      <td>IP_6134378</td>\n",
              "      <td>0.028375</td>\n",
              "      <td>0.993133</td>\n",
              "      <td>0.113501</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>71.273186</td>\n",
              "      <td>1</td>\n",
              "      <td>71.145157</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_1985984</td>\n",
              "      <td>IP_1645936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999882</td>\n",
              "      <td>0.015383</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>79.947555</td>\n",
              "      <td>1</td>\n",
              "      <td>79.705372</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_7233663</td>\n",
              "      <td>IP_3735763</td>\n",
              "      <td>0.019992</td>\n",
              "      <td>0.999600</td>\n",
              "      <td>0.019992</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>71.449125</td>\n",
              "      <td>1</td>\n",
              "      <td>71.544231</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_3003627</td>\n",
              "      <td>IP_4898383</td>\n",
              "      <td>0.033023</td>\n",
              "      <td>0.990687</td>\n",
              "      <td>0.132092</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>85.556576</td>\n",
              "      <td>1</td>\n",
              "      <td>85.091278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0886205</td>\n",
              "      <td>IP_2153088</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.132164</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>89.958897</td>\n",
              "      <td>1</td>\n",
              "      <td>89.804639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b2961da-93c7-4985-a25f-cced12f577a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b2961da-93c7-4985-a25f-cced12f577a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b2961da-93c7-4985-a25f-cced12f577a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfvp9DpaGAeF"
      },
      "source": [
        "### Sample min count test set\n",
        "\n",
        "Create a test set with 50 of each FST items "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "RXD6bIiRGQSy",
        "outputId": "eb481968-498d-45a3-e687-866bf595e030"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdnElEQVR4nO3deVyNef8/8NdRnbSdE2kdqUZEyD5kNyIkWwwmCuFmypKtacY2zGDMbR1DY1CMfV8yJFIGIZEsI7uMNm7qKFrU9fvDr+vrKFSqk87r+Xhcj8d9Ptfn+lzv6zrH3Wuu87muIxEEQQARERGRGqui6gKIiIiIVI2BiIiIiNQeAxERERGpPQYiIiIiUnsMRERERKT2GIiIiIhI7TEQERERkdpjICIiIiK1x0BEREREao+BiNTSnDlzIJFIymVfnTp1QqdOncTX4eHhkEgk2LVrV7nsf/jw4bC2ti6XfZVUeno6Ro0aBTMzM0gkEkyaNKnEY30Kx1sSb3+OiuL+/fuQSCT473//WzZF/X/Dhw+Hvr7+B/uV5BiIygsDEX3ygoKCIJFIxKVq1aqwsLCAs7MzVqxYgefPn5fKfhISEjBnzhzExMSUynilqSLXVhTz589HUFAQxo0bhz///BPDhg17Z19ra2ul9/vNJTMzs0D/Fy9eYM6cOQgPDy/DIyi6T+m9Sk9Px+zZs9GwYUPo6enByMgITZo0wcSJE5GQkKDS2t71GXh7qSjvO1V8mqougKi0zJ07FzY2NsjJyUFSUhLCw8MxadIkLFmyBAcOHICDg4PYd8aMGfj222+LNX5CQgJ++OEHWFtbo0mTJkXe7ujRo8XaT0m8r7Y//vgDeXl5ZV7DxwgLC0Pr1q0xe/bsIvVv0qQJpkyZUqBdKpUWON4XL17ghx9+AIAKcXWiIn+O3pSTk4MOHTrgxo0b8PT0xPjx45Geno5r165hy5Yt6NevHywsLIo1Zmkew59//qn0euPGjQgNDS3QXr9+/VLbJ1VuDERUafTo0QMtWrQQX/v7+yMsLAy9evVC79698c8//0BHRwcAoKmpCU3Nsv34v3jxArq6upBKpWW6nw/R0tJS6f6LIiUlBfb29kXu/9lnn2Ho0KGFrqtSpXJd+FbV52jfvn24dOkSNm/ejK+//lppXWZmJrKzs4s9Zmkew9vv/9mzZxEaGvrOzwXRh1Su/+cgesuXX36JmTNn4sGDB9i0aZPYXtgcotDQULRr1w6GhobQ19eHnZ0dvvvuOwCv5/20bNkSADBixAjxcnxQUBCA11ceGjZsiOjoaHTo0AG6urritu+aN5Gbm4vvvvsOZmZm0NPTQ+/evfHw4UOlPtbW1hg+fHiBbd8c80O1FTanJiMjA1OmTIGlpSW0tbVhZ2eH//73vxAEQamfRCKBj48P9u3bh4YNG0JbWxsNGjTAkSNHCj/hb0lJSYGXlxdMTU1RtWpVNG7cGBs2bBDX58+nunfvHg4dOiTWfv/+/SKNX5g3j/f+/fswNjYGAPzwww/i+Plfob3ra5Y3z5e1tTV69eqFo0ePokmTJqhatSrs7e2xZ88epf0+ffoUU6dORaNGjaCvrw+ZTIYePXrg8uXLSsdbmp+jzMxMzJkzB3Xr1kXVqlVhbm6O/v37486dO+88P4IgYMyYMZBKpQWO4U35Y7Rt27bAuqpVq0Imk71zWwCIiYmBsbExOnXqhPT09EKPIf892LFjB3766SfUrFkTVatWRZcuXXD79u33jv8hnp6eqFGjBnJycgqs69atG+zs7MTX+Z/zzZs3w87ODlWrVkXz5s1x8uTJAts+evQII0eOhKmpqfjvYf369R9VK1UMvEJEld6wYcPw3Xff4ejRoxg9enShfa5du4ZevXrBwcEBc+fOhba2Nm7fvo3Tp08DeH3Zfe7cuZg1axbGjBmD9u3bAwDatGkjjvG///0PPXr0wODBgzF06FCYmpq+t66ffvoJEokEfn5+SElJwbJly+Dk5ISYmBjxSlZRFKW2NwmCgN69e+PEiRPw8vJCkyZNEBISgmnTpuHRo0dYunSpUv9Tp05hz549+Oabb2BgYIAVK1bAzc0N8fHxMDIyemddL1++RKdOnXD79m34+PjAxsYGO3fuxPDhw5GamoqJEyeifv36+PPPP+Hr64uaNWuKX4Plh5h3ycnJwZMnT5TadHV1oaurq9RmbGyM1atXY9y4cejXrx/69+8PAHBwcICpqWmBr1dSU1MxefJkmJiYKLXfunULgwYNwtixY+Hp6YnAwEAMHDgQR44cQdeuXQEAd+/exb59+zBw4EDY2NggOTkZv//+Ozp27Ijr16/DwsKiVD9Hubm56NWrF44fP47Bgwdj4sSJeP78OUJDQ3H16lXUrl270G1GjhyJ7du3Y+/evXBxcXnnObaysgLw+quoGTNmFOsmhKioKDg7O6NFixbYv3//Bz/PCxcuRJUqVTB16lSkpaVh0aJFcHd3x7lz54q8z7cNGzYMGzduREhICHr16iW2JyUlISwsrMDXsxEREdi+fTsmTJgAbW1trFq1Ct27d8f58+fRsGFDAEBycjJat24tBihjY2McPnwYXl5eUCgUH3UzAFUAAtEnLjAwUAAgREVFvbOPXC4XmjZtKr6ePXu28ObHf+nSpQIA4fHjx+8cIyoqSgAgBAYGFljXsWNHAYAQEBBQ6LqOHTuKr0+cOCEAED777DNBoVCI7Tt27BAACMuXLxfbrKysBE9Pzw+O+b7aPD09BSsrK/H1vn37BADCjz/+qNRvwIABgkQiEW7fvi22ARCkUqlS2+XLlwUAwq+//lpgX29atmyZAEDYtGmT2JadnS04OjoK+vr6SsduZWUluLi4vHe8N/sCKLDMnj270ON9/Pix0vp3ycvLE3r16iXo6+sL165dK7C/3bt3i21paWmCubm50mcqMzNTyM3NVRrz3r17gra2tjB37lyxrbQ+R+vXrxcACEuWLCn0WPL3D0D45ZdfhJycHGHQoEGCjo6OEBIS8t5zIQiC8OLFC8HOzk4AIFhZWQnDhw8X1q1bJyQnJxfo6+npKejp6QmCIAinTp0SZDKZ4OLiImRmZr73GPL/LdSvX1/IysoS25cvXy4AEK5cufLBOvN5e3sr/ZvOzc0VatasKQwaNEip35IlSwSJRCLcvXtXbMv/DF24cEFse/DggVC1alWhX79+YpuXl5dgbm4uPHnyRGnMwYMHC3K5XHjx4kWR66WKh1+ZkVrQ19d/791mhoaGAID9+/eXeAKytrY2RowYUeT+Hh4eMDAwEF8PGDAA5ubm+Ouvv0q0/6L666+/oKGhgQkTJii1T5kyBYIg4PDhw0rtTk5OSlcbHBwcIJPJcPfu3Q/ux8zMDEOGDBHbtLS0MGHCBKSnpyMiIqLEx9CqVSuEhoYqLR4eHiUeDwDmzZuH4OBgBAUFFZjPZGFhgX79+omvZTIZPDw8cOnSJSQlJQF4/f7nz1/Kzc3F//73P/Gr14sXLxa5jqJ+jnbv3o0aNWpg/PjxBda9fTUnOzsbAwcORHBwMP766y9069btg+Pr6Ojg3LlzmDZtGoDXd3N6eXnB3Nwc48ePR1ZWVoFtTpw4AWdnZ3Tp0gV79uyBtrb2B/cDvP768M35RflXzj70GXufKlWqwN3dHQcOHFD6t79582a0adMGNjY2Sv0dHR3RvHlz8XWtWrXQp08fhISEIDc3F4IgYPfu3XB1dYUgCHjy5Im4ODs7Iy0trVjvM1U8DESkFtLT05XCx9sGDRqEtm3bYtSoUTA1NcXgwYOxY8eOYoWjzz77rFiTRuvUqaP0WiKRwNbW9qPmzxTFgwcPYGFhUeB85N+N8+DBA6X2WrVqFRijWrVqePbs2Qf3U6dOnQKTnN+1n+KoUaMGnJyclJbPP/+8xOMdOXIEP/zwA/z9/eHm5lZgva2tbYGQUbduXQAQ36+8vDwsXboUderUgba2NmrUqAFjY2PExsYiLS2tyLUU9XN0584d2NnZFenmgAULFmDfvn3YtWtXse60k8vlWLRoEe7fv4/79+9j3bp1sLOzw8qVKzFv3jylvpmZmXBxcUHTpk2xY8eOYv1bePszVq1aNQD44GfsQzw8PPDy5Uvs3bsXABAXF4fo6OhCH+vw9r9H4PV7/OLFCzx+/BiPHz9Gamoq1qxZA2NjY6UlP8CmpKR8VL2kWgxEVOn9+++/SEtLg62t7Tv76Ojo4OTJkzh27BiGDRuG2NhYDBo0CF27dkVubm6R9lOceT9F9a55G0WtqTRoaGgU2i68NQH7U3Xv3j24u7uja9eu+PHHH0s8zvz58zF58mR06NABmzZtQkhICEJDQ9GgQYNiBeuy+Bw5OztDT08PixYtKvRZTUVhZWWFkSNH4vTp0zA0NMTmzZuV1mtra8PFxQXnzp0r8qT7fGX1GbO3t0fz5s3FGyo2bdoEqVSKr776qthj5b+HQ4cOLXB1Mn8pbAI6fTo4qZoqvfyJs87Ozu/tV6VKFXTp0gVdunTBkiVLMH/+fHz//fc4ceIEnJycSv3J1rdu3VJ6LQgCbt++rfS8pGrVqiE1NbXAtg8ePFC6IlKc2qysrHDs2DE8f/5c6SrRjRs3xPWlwcrKCrGxscjLy1O6SlTa+/mQ952bly9fon///jA0NMTWrVvfecv+7du3IQiC0lg3b94EAPGOtF27dqFz585Yt26d0rapqamoUaNGkeopjtq1a+PcuXPIycn54KMVWrdujbFjx6JXr14YOHAg9u7dW+LHTlSrVg21a9fG1atXldolEgk2b96MPn36YODAgTh8+HCFeO6Th4cHJk+ejMTERGzZsgUuLi7iFag3vf3vEXj9Huvq6oqT/A0MDJCbmwsnJ6cyr5vKH68QUaUWFhaGefPmwcbGBu7u7u/s9/Tp0wJt+Q/Ny58roaenBwCFBpSS2Lhxo9Lchl27diExMRE9evQQ22rXro2zZ88qPfMlODi4wO35xamtZ8+eyM3NxcqVK5Xaly5dColEorT/j9GzZ08kJSVh+/btYturV6/w66+/Ql9fHx07diyV/XxI/p1nhZ2bsWPH4ubNm9i7d2+hfyTzJSQkiF+7AIBCocDGjRvRpEkTmJmZAXh9lePtKxo7d+7Eo0ePlNpK63Pk5uaGJ0+eFHgfgcKvrDg5OWHbtm04cuQIhg0b9sGrVpcvXy5wJx/wOoxfv35d6bb1fPm38rds2RKurq44f/58MY6obAwZMgQSiQQTJ07E3bt33/mcosjISKU5QA8fPsT+/fvRrVs3aGhoQENDA25ubti9e3eBMAgAjx8/LrNjoPLBK0RUaRw+fBg3btzAq1evkJycjLCwMISGhsLKygoHDhxA1apV37nt3LlzcfLkSbi4uMDKygopKSlYtWoVatasiXbt2gF4HU4MDQ0REBAAAwMD6OnpoVWrVgUmZxZV9erV0a5dO4wYMQLJyclYtmwZbG1tlR4NMGrUKOzatQvdu3fHV199hTt37mDTpk0FbqkuTm2urq7o3Lkzvv/+e9y/fx+NGzfG0aNHsX//fkyaNKnQ27VLYsyYMfj9998xfPhwREdHw9raGrt27cLp06exbNmy987pKk06Ojqwt7fH9u3bUbduXVSvXh0NGzbEgwcPsHHjRri5uSE2NhaxsbHiNvr6+ujbt6/4um7duvDy8kJUVBRMTU2xfv16JCcnIzAwUOzTq1cvzJ07FyNGjECbNm1w5coVbN68ucDcptL6HHl4eGDjxo2YPHkyzp8/j/bt2yMjIwPHjh3DN998gz59+hTYpm/fvggMDISHhwdkMhl+//33d44fGhqK2bNno3fv3mjdujX09fVx9+5drF+/HllZWZgzZ06h2+no6CA4OBhffvklevTogYiICPG2dVUwNjZG9+7dsXPnThgaGr7zUQMNGzaEs7Oz0m33AMSnnAOvHw9w4sQJtGrVCqNHj4a9vT2ePn2Kixcv4tixY4X+hxV9QlR2fxtRKcm/7T5/kUqlgpmZmdC1a1dh+fLlSrd353v7tvvjx48Lffr0ESwsLASpVCpYWFgIQ4YMEW7evKm03f79+wV7e3tBU1NT6dbpjh07Cg0aNCi0vnfdarx161bB399fMDExEXR0dAQXFxfhwYMHBbZfvHix8Nlnnwna2tpC27ZthQsXLhQY8321vX0buiAIwvPnzwVfX1/BwsJC0NLSEurUqSP88ssv4u3a+QAI3t7eBWp61+MA3pacnCyMGDFCqFGjhiCVSoVGjRoVert5cW+7f1/fwo73zJkzQvPmzQWpVCregv/25+bN5c3t8/cXEhIiODg4CNra2kK9evWEnTt3Ku0jMzNTmDJlimBubi7o6OgIbdu2FSIjI4v1XhXncyQIr2+N//777wUbGxtBS0tLMDMzEwYMGCDcuXNHEATl2+7ftGrVKgGAMHXq1Heex7t37wqzZs0SWrduLZiYmAiampqCsbGx4OLiIoSFhRU45/m33ed78uSJYG9vL5iZmQm3bt0q9Bjy/y28fS7z6y7ss/Iub992/6b8R1qMGTOm0PX5n/NNmzYJderUEbS1tYWmTZsKJ06cKNA3OTlZ8Pb2FiwtLcVz3qVLF2HNmjVFrpUqJokgVJKZkUREZcDa2hoNGzZEcHCwqkuhEtq/fz/69u2LkydPirf0v0kikcDb27vQrx9JfXAOERERVWp//PEHPv/8c/Hrb6LCcA4RERFVStu2bUNsbCwOHTqE5cuXl/qdolS5MBAREVGlNGTIEOjr68PLywvffPONqsuhCo5ziIiIiEjtcQ4RERERqT0GIiIiIlJ7nENUBHl5eUhISICBgQEn5REREX0iBEHA8+fPYWFh8c6f5snHQFQECQkJsLS0VHUZREREVAIPHz5EzZo139uHgagI8n9i4OHDh5DJZCquhoiIiIpCoVDA0tKySD8VxEBUBPlfk8lkMgYiIiKiT0xRprtwUjURERGpPQYiIiIiUnsMRGXM2toaEomkwOLt7Q0A6NSpU4F1Y8eOFbcPCgoqdHuJRIKUlBQAQHh4eKHrk5KSVHLMREREnxrOISpjUVFRyM3NFV9fvXoVXbt2xcCBA8W20aNHY+7cueJrXV1d8X8PGjQI3bt3Vxpz+PDhyMzMhImJiVJ7XFyc0hynt9cTERFR4RiIypixsbHS64ULF6J27dro2LGj2KarqwszM7NCt9fR0YGOjo74+vHjxwgLC8O6desK9DUxMYGhoWHpFE5ERKRG+JVZOcrOzsamTZswcuRIpRnvmzdvRo0aNdCwYUP4+/vjxYsX7xxj48aN0NXVxYABAwqsa9KkCczNzdG1a1ecPn26TI6BiIioMuIVonK0b98+pKamYvjw4WLb119/DSsrK1hYWCA2NhZ+fn6Ii4vDnj17Ch1j3bp1+Prrr5WuGpmbmyMgIAAtWrRAVlYW1q5di06dOuHcuXNo1qxZWR8WERHRJ4+/dl8ECoUCcrkcaWlpH/UcImdnZ0ilUhw8ePCdfcLCwtClSxfcvn0btWvXVloXGRmJNm3a4MKFC2jevPl799WxY0fUqlULf/75Z4nrJSIi+pQV5+83vzIrJw8ePMCxY8cwatSo9/Zr1aoVAOD27dsF1q1duxZNmjT5YBgCgC+++KLQMYiIiKggBqJyEhgYCBMTE7i4uLy3X0xMDIDXX4O9KT09HTt27ICXl1eR9hcTE1NgDCIiIioc5xCVg7y8PAQGBsLT0xOamv93yu/cuYMtW7agZ8+eMDIyQmxsLHx9fdGhQwc4ODgojbF9+3a8evUKQ4cOLTD+smXLYGNjgwYNGiAzMxNr165FWFgYjh49WubHRkREVBkwEJWDY8eOIT4+HiNHjlRql0qlOHbsGJYtW4aMjAxYWlrCzc0NM2bMKDDGunXr0L9//0Jvq8/OzsaUKVPw6NEj6OrqwsHBAceOHUPnzp3L6pCIiIgqFU6qLoLSmlRNRERE5YeTqomIiIiKgYGIiIiI1B7nEFUArq4l3/Y9jzQiIiKiIuIVIiIiIlJ7DERERESk9hiIiIiISO0xEBEREZHaYyAiIiIitcdARERERGqPgYiIiIjUHgMRERERqT0GIiIiIlJ7DERERESk9hiIiIiISO0xEBEREZHaYyAiIiIitcdARERERGqPgYiIiIjUHgMRERERqT0GIiIiIlJ7DERERESk9hiIiIiISO0xEBEREZHaYyAiIiIitafSQGRtbQ2JRFJg8fb2BgBkZmbC29sbRkZG0NfXh5ubG5KTk5XGiI+Ph4uLC3R1dWFiYoJp06bh1atXSn3Cw8PRrFkzaGtrw9bWFkFBQeV1iERERPQJUGkgioqKQmJioriEhoYCAAYOHAgA8PX1xcGDB7Fz505EREQgISEB/fv3F7fPzc2Fi4sLsrOzcebMGWzYsAFBQUGYNWuW2OfevXtwcXFB586dERMTg0mTJmHUqFEICQkp34MlIiKiCksiCIKg6iLyTZo0CcHBwbh16xYUCgWMjY2xZcsWDBgwAABw48YN1K9fH5GRkWjdujUOHz6MXr16ISEhAaampgCAgIAA+Pn54fHjx5BKpfDz88OhQ4dw9epVcT+DBw9Gamoqjhw5UqS6FAoF5HI50tLSIJPJSv24XV1Lvu3Bg6VXBxERUWVSnL/fFWYOUXZ2NjZt2oSRI0dCIpEgOjoaOTk5cHJyEvvUq1cPtWrVQmRkJAAgMjISjRo1EsMQADg7O0OhUODatWtinzfHyO+TP0ZhsrKyoFAolBYiIiKqvCpMINq3bx9SU1MxfPhwAEBSUhKkUikMDQ2V+pmamiIpKUns82YYyl+fv+59fRQKBV6+fFloLQsWLIBcLhcXS0vLjz08IiIiqsAqTCBat24devToAQsLC1WXAn9/f6SlpYnLw4cPVV0SERERlSFNVRcAAA8ePMCxY8ewZ88esc3MzAzZ2dlITU1VukqUnJwMMzMzsc/58+eVxsq/C+3NPm/fmZacnAyZTAYdHZ1C69HW1oa2tvZHHxcRERF9GirEFaLAwECYmJjAxcVFbGvevDm0tLRw/PhxsS0uLg7x8fFwdHQEADg6OuLKlStISUkR+4SGhkImk8He3l7s8+YY+X3yxyAiIiJSeSDKy8tDYGAgPD09oan5fxes5HI5vLy8MHnyZJw4cQLR0dEYMWIEHB0d0bp1awBAt27dYG9vj2HDhuHy5csICQnBjBkz4O3tLV7hGTt2LO7evYvp06fjxo0bWLVqFXbs2AFfX1+VHC8RERFVPCr/yuzYsWOIj4/HyJEjC6xbunQpqlSpAjc3N2RlZcHZ2RmrVq0S12toaCA4OBjjxo2Do6Mj9PT04Onpiblz54p9bGxscOjQIfj6+mL58uWoWbMm1q5dC2dn53I5PiIiIqr4KtRziCoqPoeIiIjo0/NJPoeIiIiISFUYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrWn8kD06NEjDB06FEZGRtDR0UGjRo1w4cIFcb0gCJg1axbMzc2ho6MDJycn3Lp1S2mMp0+fwt3dHTKZDIaGhvDy8kJ6erpSn9jYWLRv3x5Vq1aFpaUlFi1aVC7HR0RERBWfSgPRs2fP0LZtW2hpaeHw4cO4fv06Fi9ejGrVqol9Fi1ahBUrViAgIADnzp2Dnp4enJ2dkZmZKfZxd3fHtWvXEBoaiuDgYJw8eRJjxowR1ysUCnTr1g1WVlaIjo7GL7/8gjlz5mDNmjXlerxERERUMUkEQRBUtfNvv/0Wp0+fxt9//13oekEQYGFhgSlTpmDq1KkAgLS0NJiamiIoKAiDBw/GP//8A3t7e0RFRaFFixYAgCNHjqBnz574999/YWFhgdWrV+P7779HUlISpFKpuO99+/bhxo0bH6xToVBALpcjLS0NMpmslI7+/7i6lnzbgwdLrw4iIqLKpDh/v1V6hejAgQNo0aIFBg4cCBMTEzRt2hR//PGHuP7evXtISkqCk5OT2CaXy9GqVStERkYCACIjI2FoaCiGIQBwcnJClSpVcO7cObFPhw4dxDAEAM7OzoiLi8OzZ88K1JWVlQWFQqG0EBERUeWl0kB09+5drF69GnXq1EFISAjGjRuHCRMmYMOGDQCApKQkAICpqanSdqampuK6pKQkmJiYKK3X1NRE9erVlfoUNsab+3jTggULIJfLxcXS0rIUjpaIiIgqKpUGory8PDRr1gzz589H06ZNMWbMGIwePRoBAQGqLAv+/v5IS0sTl4cPH6q0HiIiIipbKg1E5ubmsLe3V2qrX78+4uPjAQBmZmYAgOTkZKU+ycnJ4jozMzOkpKQorX/16hWePn2q1KewMd7cx5u0tbUhk8mUFiIiIqq8VBqI2rZti7i4OKW2mzdvwsrKCgBgY2MDMzMzHD9+XFyvUChw7tw5ODo6AgAcHR2RmpqK6OhosU9YWBjy8vLQqlUrsc/JkyeRk5Mj9gkNDYWdnZ3SHW1ERESknlQaiHx9fXH27FnMnz8ft2/fxpYtW7BmzRp4e3sDACQSCSZNmoQff/wRBw4cwJUrV+Dh4QELCwv07dsXwOsrSt27d8fo0aNx/vx5nD59Gj4+Phg8eDAsLCwAAF9//TWkUim8vLxw7do1bN++HcuXL8fkyZNVdehERERUgWiqcuctW7bE3r174e/vj7lz58LGxgbLli2Du7u72Gf69OnIyMjAmDFjkJqainbt2uHIkSOoWrWq2Gfz5s3w8fFBly5dUKVKFbi5uWHFihXierlcjqNHj8Lb2xvNmzdHjRo1MGvWLKVnFREREZH6UulziD4VfA4RERHRp+eTeQ4RERERUUXAQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1J5KA9GcOXMgkUiUlnr16onrMzMz4e3tDSMjI+jr68PNzQ3JyclKY8THx8PFxQW6urowMTHBtGnT8OrVK6U+4eHhaNasGbS1tWFra4ugoKDyODwiIiL6RKj8ClGDBg2QmJgoLqdOnRLX+fr64uDBg9i5cyciIiKQkJCA/v37i+tzc3Ph4uKC7OxsnDlzBhs2bEBQUBBmzZol9rl37x5cXFzQuXNnxMTEYNKkSRg1ahRCQkLK9TiJiIio4tJUeQGamjAzMyvQnpaWhnXr1mHLli348ssvAQCBgYGoX78+zp49i9atW+Po0aO4fv06jh07BlNTUzRp0gTz5s2Dn58f5syZA6lUioCAANjY2GDx4sUAgPr16+PUqVNYunQpnJ2dy/VYiYiIqGJS+RWiW7duwcLCAp9//jnc3d0RHx8PAIiOjkZOTg6cnJzEvvXq1UOtWrUQGRkJAIiMjESjRo1gamoq9nF2doZCocC1a9fEPm+Okd8nf4zCZGVlQaFQKC1ERERUeak0ELVq1QpBQUE4cuQIVq9ejXv37qF9+/Z4/vw5kpKSIJVKYWhoqLSNqakpkpKSAABJSUlKYSh/ff669/VRKBR4+fJloXUtWLAAcrlcXCwtLUvjcImIiKiCUulXZj169BD/t4ODA1q1agUrKyvs2LEDOjo6KqvL398fkydPFl8rFAqGIiIiokpM5V+ZvcnQ0BB169bF7du3YWZmhuzsbKSmpir1SU5OFuccmZmZFbjrLP/1h/rIZLJ3hi5tbW3IZDKlhYiIiCqvChWI0tPTcefOHZibm6N58+bQ0tLC8ePHxfVxcXGIj4+Ho6MjAMDR0RFXrlxBSkqK2Cc0NBQymQz29vZinzfHyO+TPwYRERGRSgPR1KlTERERgfv37+PMmTPo168fNDQ0MGTIEMjlcnh5eWHy5Mk4ceIEoqOjMWLECDg6OqJ169YAgG7dusHe3h7Dhg3D5cuXERISghkzZsDb2xva2toAgLFjx+Lu3buYPn06bty4gVWrVmHHjh3w9fVV5aETERFRBaLSOUT//vsvhgwZgv/9738wNjZGu3btcPbsWRgbGwMAli5diipVqsDNzQ1ZWVlwdnbGqlWrxO01NDQQHByMcePGwdHREXp6evD09MTcuXPFPjY2Njh06BB8fX2xfPly1KxZE2vXruUt90RERCSSCIIgqLqIik6hUEAulyMtLa1M5hO5upZ824MHS68OIiKiyqQ4f78r1BwiIiIiIlVgICIiIiK1x0BEREREao+BiIiIiNQeAxERERGpPQYiIiIiUnsMRERERKT2GIiIiIhI7ZUoEN29e7e06yAiIiJSmRIFIltbW3Tu3BmbNm1CZmZmaddEREREVK5KFIguXrwIBwcHTJ48GWZmZvjPf/6D8+fPl3ZtREREROWiRIGoSZMmWL58ORISErB+/XokJiaiXbt2aNiwIZYsWYLHjx+Xdp1EREREZeajJlVramqif//+2LlzJ37++Wfcvn0bU6dOhaWlJTw8PJCYmFhadRIRERGVmY8KRBcuXMA333wDc3NzLFmyBFOnTsWdO3cQGhqKhIQE9OnTp7TqJCIiIiozmiXZaMmSJQgMDERcXBx69uyJjRs3omfPnqhS5XW+srGxQVBQEKytrUuzViIiIqIyUaJAtHr1aowcORLDhw+Hubl5oX1MTEywbt26jyqOiIiIqDyUKBDdunXrg32kUik8PT1LMjwRERFRuSrRHKLAwEDs3LmzQPvOnTuxYcOGjy6KiIiIqDyVKBAtWLAANWrUKNBuYmKC+fPnf3RRREREROWpRIEoPj4eNjY2BdqtrKwQHx//0UURERERlacSBSITExPExsYWaL98+TKMjIw+uigiIiKi8lSiQDRkyBBMmDABJ06cQG5uLnJzcxEWFoaJEydi8ODBpV0jERERUZkq0V1m8+bNw/3799GlSxdoar4eIi8vDx4eHpxDRERERJ+cEgUiqVSK7du3Y968ebh8+TJ0dHTQqFEjWFlZlXZ9RERERGWuRIEoX926dVG3bt3SqoWIiIhIJUoUiHJzcxEUFITjx48jJSUFeXl5SuvDwsJKpTgiIiKi8lCiQDRx4kQEBQXBxcUFDRs2hEQiKe26iIiIiMpNiQLRtm3bsGPHDvTs2bO06yEiIiIqdyW67V4qlcLW1ra0ayEiIiJSiRIFoilTpmD58uUQBKG06yEiIiIqdyX6yuzUqVM4ceIEDh8+jAYNGkBLS0tp/Z49e0qlOCIiIqLyUKJAZGhoiH79+pV2LUREREQqUaJAFBgYWNp1EBEREalMieYQAcCrV69w7Ngx/P7773j+/DkAICEhAenp6aVWHBEREVF5KNEVogcPHqB79+6Ij49HVlYWunbtCgMDA/z888/IyspCQEBAaddJREREVGZKdIVo4sSJaNGiBZ49ewYdHR2xvV+/fjh+/HipFUdERERUHkoUiP7++2/MmDEDUqlUqd3a2hqPHj0qUSELFy6ERCLBpEmTxLbMzEx4e3vDyMgI+vr6cHNzQ3JystJ28fHxcHFxga6uLkxMTDBt2jS8evVKqU94eDiaNWsGbW1t2NraIigoqEQ1EhERUeVUokCUl5eH3NzcAu3//vsvDAwMij1eVFQUfv/9dzg4OCi1+/r64uDBg9i5cyciIiKQkJCA/v37i+tzc3Ph4uKC7OxsnDlzBhs2bEBQUBBmzZol9rl37x5cXFzQuXNnxMTEYNKkSRg1ahRCQkKKXScRERFVTiUKRN26dcOyZcvE1xKJBOnp6Zg9e3axf84jPT0d7u7u+OOPP1CtWjWxPS0tDevWrcOSJUvw5Zdfonnz5ggMDMSZM2dw9uxZAMDRo0dx/fp1bNq0CU2aNEGPHj0wb948/Pbbb8jOzgYABAQEwMbGBosXL0b9+vXh4+ODAQMGYOnSpSU5dCIiIqqEShSIFi9ejNOnT8Pe3h6ZmZn4+uuvxa/Lfv7552KN5e3tDRcXFzg5OSm1R0dHIycnR6m9Xr16qFWrFiIjIwEAkZGRaNSoEUxNTcU+zs7OUCgUuHbtmtjn7bGdnZ3FMQqTlZUFhUKhtBAREVHlVaK7zGrWrInLly9j27ZtiI2NRXp6Ory8vODu7q40yfpDtm3bhosXLyIqKqrAuqSkJEilUhgaGiq1m5qaIikpSezzZhjKX5+/7n19FAoFXr58WWi9CxYswA8//FDk4yAiIqJPW4kCEQBoampi6NChJd7xw4cPMXHiRISGhqJq1aolHqcs+Pv7Y/LkyeJrhUIBS0tLFVZEREREZalEgWjjxo3vXe/h4fHBMaKjo5GSkoJmzZqJbbm5uTh58iRWrlyJkJAQZGdnIzU1VekqUXJyMszMzAAAZmZmOH/+vNK4+Xehvdnn7TvTkpOTIZPJ3nk1S1tbG9ra2h88BiIiIqocShSIJk6cqPQ6JycHL168gFQqha6ubpECUZcuXXDlyhWlthEjRqBevXrw8/ODpaUltLS0cPz4cbi5uQEA4uLiEB8fD0dHRwCAo6MjfvrpJ6SkpMDExAQAEBoaCplMBnt7e7HPX3/9pbSf0NBQcQwiIiKiEgWiZ8+eFWi7desWxo0bh2nTphVpDAMDAzRs2FCpTU9PD0ZGRmK7l5cXJk+ejOrVq0Mmk2H8+PFwdHRE69atAby+283e3h7Dhg3DokWLkJSUhBkzZsDb21u8wjN27FisXLkS06dPx8iRIxEWFoYdO3bg0KFDJTl0IiIiqoRK/Ftmb6tTpw4WLlxY4OrRx1i6dCl69eoFNzc3dOjQAWZmZtizZ4+4XkNDA8HBwdDQ0ICjoyOGDh0KDw8PzJ07V+xjY2ODQ4cOITQ0FI0bN8bixYuxdu1aODs7l1qdRERE9GmTCIIglNZgMTEx6NChQ6W7TV2hUEAulyMtLQ0ymazUx3d1Lfm2Bw+WXh1ERESVSXH+fpfoK7MDBw4ovRYEAYmJiVi5ciXatm1bkiGJiIiIVKZEgahv375KryUSCYyNjfHll19i8eLFpVEXERERUbkpUSDKy8sr7TqIiIiIVKbUJlUTERERfapKdIXozac4f8iSJUtKsgsiIiKiclOiQHTp0iVcunQJOTk5sLOzAwDcvHkTGhoaSk+elkgkpVMlERERURkqUSBydXWFgYEBNmzYgGrVqgF4/bDGESNGoH379pgyZUqpFklERERUlkr0HKLPPvsMR48eRYMGDZTar169im7duiEhIaHUCqwI+BwiIiKiT09x/n6XaFK1QqHA48ePC7Q/fvwYz58/L8mQRERERCpTokDUr18/jBgxAnv27MG///6Lf//9F7t374aXlxf69+9f2jUSERERlakSzSEKCAjA1KlT8fXXXyMnJ+f1QJqa8PLywi+//FKqBRIRERGVtY/6LbOMjAzcuXMHAFC7dm3o6emVWmEVCecQERERfXrKfA5RvsTERCQmJqJOnTrQ09NDKf5OLBEREVG5KVEg+t///ocuXbqgbt266NmzJxITEwEAXl5evOWeiIiIPjklCkS+vr7Q0tJCfHw8dHV1xfZBgwbhyJEjpVYcERERUXko0aTqo0ePIiQkBDVr1lRqr1OnDh48eFAqhRERERGVlxJdIcrIyFC6MpTv6dOn0NbW/uiiiIiIiMpTiQJR+/btsXHjRvG1RCJBXl4eFi1ahM6dO5dacURERETloURfmS1atAhdunTBhQsXkJ2djenTp+PatWt4+vQpTp8+Xdo1EhEREZWpEl0hatiwIW7evIl27dqhT58+yMjIQP/+/XHp0iXUrl27tGskIiIiKlPFvkKUk5OD7t27IyAgAN9//31Z1ERERERUrop9hUhLSwuxsbFlUQsRERGRSpToK7OhQ4di3bp1pV0LERERkUqUaFL1q1evsH79ehw7dgzNmzcv8BtmS5YsKZXiiIiIiMpDsQLR3bt3YW1tjatXr6JZs2YAgJs3byr1kUgkpVcdERERUTkoViCqU6cOEhMTceLECQCvf6pjxYoVMDU1LZPiiIiIiMpDseYQvf1r9ocPH0ZGRkapFkRERERU3ko0qTrf2wGJiIiI6FNUrEAkkUgKzBHinCEiIiL61BVrDpEgCBg+fLj4A66ZmZkYO3ZsgbvM9uzZU3oVEhEREZWxYgUiT09PpddDhw4t1WKIiIiIVKFYgSgwMLCs6iAiIiJSmY+aVE1ERERUGTAQERERkdpjICIiIiK1p9JAtHr1ajg4OEAmk0Emk8HR0RGHDx8W12dmZsLb2xtGRkbQ19eHm5sbkpOTlcaIj4+Hi4sLdHV1YWJigmnTpuHVq1dKfcLDw9GsWTNoa2vD1tYWQUFB5XF4RERE9IlQaSCqWbMmFi5ciOjoaFy4cAFffvkl+vTpg2vXrgEAfH19cfDgQezcuRMRERFISEhA//79xe1zc3Ph4uKC7OxsnDlzBhs2bEBQUBBmzZol9rl37x5cXFzQuXNnxMTEYNKkSRg1ahRCQkLK/XiJiIioYpIIFexx09WrV8cvv/yCAQMGwNjYGFu2bMGAAQMAADdu3ED9+vURGRmJ1q1b4/Dhw+jVqxcSEhLE31MLCAiAn58fHj9+DKlUCj8/Pxw6dAhXr14V9zF48GCkpqbiyJEjRapJoVBALpcjLS0NMpms1I/Z1bXk2x48WHp1EBERVSbF+ftdYeYQ5ebmYtu2bcjIyICjoyOio6ORk5MDJycnsU+9evVQq1YtREZGAgAiIyPRqFEjpR+XdXZ2hkKhEK8yRUZGKo2R3yd/jMJkZWVBoVAoLURERFR5qTwQXblyBfr6+tDW1sbYsWOxd+9e2NvbIykpCVKpFIaGhkr9TU1NkZSUBABISkpSCkP56/PXva+PQqHAy5cvC61pwYIFkMvl4mJpaVkah0pEREQVlMoDkZ2dHWJiYnDu3DmMGzcOnp6euH79ukpr8vf3R1pamrg8fPhQpfUQERFR2SrWk6rLglQqha2tLQCgefPmiIqKwvLlyzFo0CBkZ2cjNTVV6SpRcnIyzMzMAABmZmY4f/680nj5d6G92eftO9OSk5Mhk8mgo6NTaE3a2tri77URERFR5afyK0Rvy8vLQ1ZWFpo3bw4tLS0cP35cXBcXF4f4+Hg4OjoCABwdHXHlyhWkpKSIfUJDQyGTyWBvby/2eXOM/D75YxARERGp9AqRv78/evTogVq1auH58+fYsmULwsPDERISArlcDi8vL0yePBnVq1eHTCbD+PHj4ejoiNatWwMAunXrBnt7ewwbNgyLFi1CUlISZsyYAW9vb/EKz9ixY7Fy5UpMnz4dI0eORFhYGHbs2IFDhw6p8tCJiIioAlFpIEpJSYGHhwcSExMhl8vh4OCAkJAQdO3aFQCwdOlSVKlSBW5ubsjKyoKzszNWrVolbq+hoYHg4GCMGzcOjo6O0NPTg6enJ+bOnSv2sbGxwaFDh+Dr64vly5ejZs2aWLt2LZydncv9eImIiKhiqnDPIaqI+BwiIiKiT88n+RwiIiIiIlVhICIiIiK1x0BEREREao+BiIiIiNQeAxERERGpPQYiIiIiUnsMRERERKT2GIiIiIhI7TEQERERkdpjICIiIiK1x0BEREREao+BiIiIiNQeAxERERGpPQYiIiIiUnsMRERERKT2GIiIiIhI7TEQERERkdpjICIiIiK1x0BEREREao+BiIiIiNQeAxERERGpPQYiIiIiUnsMRERERKT2GIiIiIhI7TEQERERkdpjICIiIiK1x0BEREREao+BiIiIiNQeAxERERGpPQYiIiIiUnsMRERERKT2GIiIiIhI7TEQERERkdpjICIiIiK1x0BEREREak+lgWjBggVo2bIlDAwMYGJigr59+yIuLk6pT2ZmJry9vWFkZAR9fX24ubkhOTlZqU98fDxcXFygq6sLExMTTJs2Da9evVLqEx4ejmbNmkFbWxu2trYICgoq68MjIiKiT4RKA1FERAS8vb1x9uxZhIaGIicnB926dUNGRobYx9fXFwcPHsTOnTsRERGBhIQE9O/fX1yfm5sLFxcXZGdn48yZM9iwYQOCgoIwa9Yssc+9e/fg4uKCzp07IyYmBpMmTcKoUaMQEhJSrsdLREREFZNEEARB1UXke/z4MUxMTBAREYEOHTogLS0NxsbG2LJlCwYMGAAAuHHjBurXr4/IyEi0bt0ahw8fRq9evZCQkABTU1MAQEBAAPz8/PD48WNIpVL4+fnh0KFDuHr1qrivwYMHIzU1FUeOHPlgXQqFAnK5HGlpaZDJZKV+3K6uJd/24MHSq4OIiKgyKc7f7wo1hygtLQ0AUL16dQBAdHQ0cnJy4OTkJPapV68eatWqhcjISABAZGQkGjVqJIYhAHB2doZCocC1a9fEPm+Okd8nfwwiIiJSb5qqLiBfXl4eJk2ahLZt26Jhw4YAgKSkJEilUhgaGir1NTU1RVJSktjnzTCUvz5/3fv6KBQKvHz5Ejo6OkrrsrKykJWVJb5WKBQff4BERERUYVWYK0Te3t64evUqtm3bpupSsGDBAsjlcnGxtLRUdUlERERUhipEIPLx8UFwcDBOnDiBmjVriu1mZmbIzs5GamqqUv/k5GSYmZmJfd6+6yz/9Yf6yGSyAleHAMDf3x9paWni8vDhw48+RiIiIqq4VBqIBEGAj48P9u7di7CwMNjY2Citb968ObS0tHD8+HGxLS4uDvHx8XB0dAQAODo64sqVK0hJSRH7hIaGQiaTwd7eXuzz5hj5ffLHeJu2tjZkMpnSQkRERJWXSucQeXt7Y8uWLdi/fz8MDAzEOT9yuRw6OjqQy+Xw8vLC5MmTUb16dchkMowfPx6Ojo5o3bo1AKBbt26wt7fHsGHDsGjRIiQlJWHGjBnw9vaGtrY2AGDs2LFYuXIlpk+fjpEjRyIsLAw7duzAoUOHVHbsREREVHGo9LZ7iURSaHtgYCCGDx8O4PWDGadMmYKtW7ciKysLzs7OWLVqlfh1GAA8ePAA48aNQ3h4OPT09ODp6YmFCxdCU/P/8l54eDh8fX1x/fp11KxZEzNnzhT38SG87Z6IiOjTU5y/3xXqOUQVFQMRERHRp+eTfQ4RERERkSowEBEREZHaYyAiIiIitcdARERERGqPgYiIiIjUHgMRERERqT0GIiIiIlJ7DERERESk9hiIiIiISO0xEBEREZHaYyAiIiIitcdARERERGqPgYiIiIjUHgMRERERqT0GIiIiIlJ7DERERESk9hiIiIiISO0xEBEREZHaYyAiIiIitcdARERERGqPgYiIiIjUHgMRERERqT0GIiIiIlJ7DERERESk9hiIiIiISO0xEBEREZHaYyAiIiIitcdARERERGqPgYiIiIjUHgMRERERqT0GIiIiIlJ7DERERESk9hiIiIiISO0xEBEREZHaYyAiIiIitcdARERERGpPpYHo5MmTcHV1hYWFBSQSCfbt26e0XhAEzJo1C+bm5tDR0YGTkxNu3bql1Ofp06dwd3eHTCaDoaEhvLy8kJ6ertQnNjYW7du3R9WqVWFpaYlFixaV9aERERHRJ0SlgSgjIwONGzfGb7/9Vuj6RYsWYcWKFQgICMC5c+egp6cHZ2dnZGZmin3c3d1x7do1hIaGIjg4GCdPnsSYMWPE9QqFAt26dYOVlRWio6Pxyy+/YM6cOVizZk2ZHx8RERF9GiSCIAiqLgIAJBIJ9u7di759+wJ4fXXIwsICU6ZMwdSpUwEAaWlpMDU1RVBQEAYPHox//vkH9vb2iIqKQosWLQAAR44cQc+ePfHvv//CwsICq1evxvfff4+kpCRIpVIAwLfffot9+/bhxo0bRapNoVBALpcjLS0NMpms1I/d1bXk2x48WHp1EBERVSbF+ftdYecQ3bt3D0lJSXBychLb5HI5WrVqhcjISABAZGQkDA0NxTAEAE5OTqhSpQrOnTsn9unQoYMYhgDA2dkZcXFxePbsWaH7zsrKgkKhUFqIiIio8qqwgSgpKQkAYGpqqtRuamoqrktKSoKJiYnSek1NTVSvXl2pT2FjvLmPty1YsAByuVxcLC0tP/6AiIiIqMKqsIFIlfz9/ZGWliYuDx8+VHVJREREVIYqbCAyMzMDACQnJyu1Jycni+vMzMyQkpKitP7Vq1d4+vSpUp/CxnhzH2/T1taGTCZTWoiIiKjyqrCByMbGBmZmZjh+/LjYplAocO7cOTg6OgIAHB0dkZqaiujoaLFPWFgY8vLy0KpVK7HPyZMnkZOTI/YJDQ2FnZ0dqlWrVk5HQ6rw6NEjDB06FEZGRtDR0UGjRo1w4cIFpT7//PMPevfuDblcDj09PbRs2RLx8fHi+jt37qBfv34wNjaGTCbDV199VSBgExHRp0+lgSg9PR0xMTGIiYkB8HoidUxMDOLj4yGRSDBp0iT8+OOPOHDgAK5cuQIPDw9YWFiId6LVr18f3bt3x+jRo3H+/HmcPn0aPj4+GDx4MCwsLAAAX3/9NaRSKby8vHDt2jVs374dy5cvx+TJk1V01FQenj17hrZt20JLSwuHDx/G9evXsXjxYqUQfOfOHbRr1w716tVDeHg4YmNjMXPmTFStWhXA68dCdOvWDRKJBGFhYTh9+jSys7Ph6uqKvLw8VR0aERGVAZXedh8eHo7OnTsXaPf09ERQUBAEQcDs2bOxZs0apKamol27dli1ahXq1q0r9n369Cl8fHxw8OBBVKlSBW5ublixYgX09fXFPrGxsfD29kZUVBRq1KiB8ePHw8/Pr8h18rb7T8+3336L06dP4++//35nn8GDB0NLSwt//vlnoeuPHj2KHj164NmzZ+L7npaWhmrVquHo0aNKd0ASEVHFU5y/3xXmOUQVGQPRp8fe3h7Ozs74999/ERERgc8++wzffPMNRo8eDQDIy8uDXC7H9OnTcerUKVy6dAk2Njbw9/cXr0AePHgQ/fr1Q0ZGBrS1tQG8fiSDnp4eZsyYgTlz5qjo6IiIqCgqxXOIiD7G3bt3sXr1atSpUwchISEYN24cJkyYgA0bNgAAUlJSkJ6ejoULF6J79+44evQo+vXrh/79+yMiIgIA0Lp1a+jp6cHPzw8vXrxARkYGpk6ditzcXCQmJqry8IiIqJQxEFGllJeXh2bNmmH+/Plo2rQpxowZg9GjRyMgIEBcDwB9+vSBr68vmjRpgm+//Ra9evUS+xgbG2Pnzp04ePAg9PX1IZfLkZqaimbNmqFKFf7TISKqTDRVXQBRWTA3N4e9vb1SW/369bF7924AQI0aNaCpqVlon1OnTomvu3Xrhjt37uDJkyfQ1NSEoaEhzMzM8Pnnn5f9QRARUblhIKJKqW3btoiLi1Nqu3nzJqysrAAAUqkULVu2fG+fN9WoUQPA68c6pKSkoHfv3mVUORERqQIDEVVKvr6+aNOmDebPn4+vvvoK58+fx5o1a7BmzRqxz7Rp0zBo0CB06NABnTt3xpEjR3Dw4EGEh4eLfQIDA1G/fn0YGxsjMjISEydOhK+vL+zs7FRwVEREVFYYiKhSatmyJfbu3Qt/f3/MnTsXNjY2WLZsGdzd3cU+/fr1Q0BAABYsWIAJEybAzs4Ou3fvRrt27cQ+cXFx8Pf3x9OnT2FtbY3vv/8evr6+qjgkIiIqQ7ztvgh42z0REdH7LVy4EP7+/pg4cSKWLVsGAOjUqZN4526+//znP+LNKwAQHx+PcePG4cSJE9DX14enpycWLFgATc2Pv2ZTnL/fvEJEREREHyUqKgq///47HBwcCqwbPXo05s6dK77W1dUV/3dubi5cXFxgZmaGM2fOIDExER4eHtDS0sL8+fPLpfZ8vHeYiIiISiw9PR3u7u74448/Cv2NUF1dXZiZmYnLm1dqjh49iuvXr2PTpk1o0qQJevTogXnz5uG3335DdnZ2eR4GAxGpD1fXki/0bqtXr4aDgwNkMhlkMhkcHR1x+PBhcX1SUhKGDRsGMzMz6OnpoVmzZuLjD/I9ffoU7u7ukMlkMDQ0hJeXF9LT08v7UIioBLy9veHi4vLOnzPavHkzatSogYYNG8Lf3x8vXrwQ10VGRqJRo0YwNTUV25ydnaFQKHDt2rUyr/1N/MqMiD5KzZo1sXDhQtSpUweCIGDDhg3o06cPLl26hAYNGsDDwwOpqak4cOAAatSogS1btuCrr77ChQsX0LRpUwCAu7s7EhMTERoaipycHIwYMQJjxozBli1bVHx0RPQ+27Ztw8WLFxEVFVXo+q+//hpWVlawsLBAbGws/Pz8EBcXhz179gB4/R9Mb4YhAOLrpKSksi3+LQxERPRRXN+6hPbTTz9h9erVOHv2LBo0aIAzZ85g9erV+OKLLwAAM2bMwNKlSxEdHY2mTZvin3/+wZEjRxAVFYUWLVoAAH799Vf07NkT//3vf2FhYVHux0REH/bw4UNMnDgRoaGhqFq1aqF9xowZI/7vRo0awdzcHF26dMGdO3dQu3bt8iq1SPiVGRGVmtzcXGzbtg0ZGRlwdHQEALRp0wbbt2/H06dPkZeXh23btiEzMxOdOnUC8PqSuaGhoRiGAMDJyQlVqlTBuXPnVHEYRFQE0dHRSElJQbNmzaCpqQlNTU1ERERgxYoV0NTURG5uboFtWrVqBQC4ffs2AMDMzAzJyclKffJfm5mZlfERKOMVIiL6aFeuXIGjoyMyMzOhr6+PvXv3ij+LsmPHDgwaNAhGRkbQ1NSErq4u9u7dC1tbWwCvL4ubmJgojaepqYnq1auX+yVzIiq6Ll264MqVK0ptI0aMQL169eDn5wcNDY0C28TExAB4/fNKAODo6IiffvoJKSkp4v8PhIaGQiaTFfhppbLGQEREH83Ozg4xMTFIS0vDrl274OnpiYiICNjb22PmzJlITU3FsWPHUKNGDezbtw9fffUV/v77bzRq1EjVpRNRCRkYGKBhw4ZKbXp6ejAyMkLDhg1x584dbNmyBT179oSRkRFiY2Ph6+uLDh06iLfnd+vWDfb29hg2bBgWLVqEpKQkzJgxA97e3tDW1i7X42EgIqKPJpVKxSs+zZs3R1RUFJYvX47p06dj5cqVuHr1Kho0aAAAaNy4Mf7++2/89ttvCAgIgJmZGVJSUpTGe/XqFZ4+fVrul8yJqPRIpVIcO3YMy5YtQ0ZGBiwtLeHm5oYZM2aIfTQ0NBAcHIxx48bB0dERenp68PT0VHpuUXlhICKiUpeXl4esrCzx9toqVZSnK2poaCAvLw/A60vmqampiI6ORvPmzQG8/hHdvLw8cb4BEX0a3vwtSEtLywJPqS6MlZUV/vrrrzKsqmg4qZqIPoq/vz9OnjyJ+/fv48qVK/D390d4eDjc3d1Rr1492Nra4j//+Q/Onz+PO3fuYPHixQgNDUXfvn0BAPXr10f37t0xevRonD9/HqdPn4aPjw8GDx7MO8zesGDBArRs2RIGBgYwMTFB3759ERcXp9RnzZo16NSpE2QyGSQSCVJTU985XlZWFpo0aQKJRCLO6yBSZwxERPRRUlJS4OHhATs7O3Tp0gVRUVEICQlB165doaWlhb/++gvGxsZwdXWFg4MDNm7ciA0bNqBnz57iGJs3b0a9evXQpUsX9OzZE+3atcOaNWtUeFQVT0REBLy9vXH27FnxeU3dunVDRkaG2OfFixfo3r07vvvuuw+ON336dAZOojfwx12LgD/uWjnwPFNl8vjxY5iYmCAiIgIdOnRQWhceHo7OnTvj2bNnMDQ0LLDt4cOHMXnyZOzevRsNGjTApUuX0KRJk/IpnCq1ivb/s/xxVyKiSi4tLQ0AUL169WJtl5ycjNGjR2Pfvn1KP7JJpO74lRkR0ScmLy8PkyZNQtu2bQvc9vw+giBg+PDhGDt2rNKDMEnZyZMn4erqCgsLC0gkEuzbt09cl5OTAz8/PzRq1Ah6enqwsLCAh4cHEhISxD7h4eGQSCSFLu/6iQtSPV4hIqJSVdEumVdG3t7euHr1Kk6dOlWs7X799Vc8f/4c/v7+ZVRZ5ZCRkYHGjRtj5MiR6N+/v9K6Fy9e4OLFi5g5cyYaN26MZ8+eYeLEiejduzcuXLgA4PXT2RMTE5W2mzlzJo4fP84gWoExEBERfUJ8fHwQHByMkydPombNmsXaNiwsDJGRkQUeeNeiRQu4u7tjw4YNpVnqJ6tHjx7o0aNHoevkcjlCQ0OV2lauXIkvvvgC8fHxqFWrFqRSqdIztHJycrB//36MHz8eEomkTGunkmMgIiL6BAiCgPHjx2Pv3r0IDw+HjY1NscdYsWIFfvzxR/F1QkICnJ2dsX37dj7z6SOkpaVBIpEUOoEdAA4cOID//e9/GDFiRPkWRsXCQERE9Anw9vbGli1bsH//fhgYGIi/8yaXy6GjowPg9e/CJSUliT+ceeXKFRgYGKBWrVqoXr06atWqpTSmvr4+AKB27drFvtpEr2VmZsLPzw9Dhgx5511M69atg7OzM89xBcdJ1UREn4DVq1cjLS0NnTp1grm5ubhs375d7BMQEICmTZti9OjRAIAOHTqgadOmOHDggKrKrtRycnLw1VdfQRAErF69utA+//77L0JCQuDl5VXO1VFx8QoREdEnoCiPjJszZw7mzJlT5DGtra2LNC4VlB+GHjx4gLCwsHdeHQoMDISRkRF69+5dzhVScTEQERERFUN+GLp16xZOnDgBIyOjQvsJgoDAwEB4eHhAS0urnKuk4mIgIiIiekN6ero4DwsA7t27h5iYGFSvXh3m5uYYMGAALl68iODgYOTm5orzuapXrw6pVCpuFxYWhnv37mHUqFHlfgxUfAxERESfoI953hPAZz69z4ULF9C5c2fx9eTJkwEAnp6emDNnjjgn6+2fOzlx4gQ6deokvl63bh3atGmDevXqlXnN9PEYiIiIiN7QqVOn986tKuq8qy1btpRWSVQOeJcZERERqT0GIiIiIlJ7/MqMiIjoPfj7fOpBra4Q/fbbb7C2tkbVqlXRqlUrnD9/XtUlERERUQWgNoFo+/btmDx5MmbPno2LFy+icePGcHZ2RkpKiqpLIyIiIhVTm0C0ZMkSjB49GiNGjIC9vT0CAgKgq6uL9evXq7o0IiIiUjG1CETZ2dmIjo6Gk5OT2FalShU4OTkhMjJShZURERFRRaAWk6qfPHmC3NxcmJqaKrWbmprixo0bBfpnZWUhKytLfJ2WlgYAUCgUZVJfTk7Jty2jkiolnufywfNcPj7mPAM818XBz3TRVbRzlf93uyjPjlKLQFRcCxYswA8//FCg3dLSUgXVvJ9cruoK1APPc/ngeS4/PNflg+e56MryXD1//hzyD+xALQJRjRo1oKGhgeTkZKX25ORkmJmZFejv7+8vPqodAPLy8vD06VMYGRlBIpGUam0KhQKWlpZ4+PDhO38tmT4ez3P54HkuHzzP5YfnunyU1XkWBAHPnz+HhYXFB/uqRSCSSqVo3rw5jh8/jr59+wJ4HXKOHz8OHx+fAv21tbWhra2t1GZoaFimNcpkMv5jKwc8z+WD57l88DyXH57r8lEW5/lDV4byqUUgAl7/OJ+npydatGiBL774AsuWLUNGRgZGjBih6tKIiIhIxdQmEA0aNAiPHz/GrFmzkJSUhCZNmuDIkSMFJloTERGR+lGbQAQAPj4+hX5Fpkra2tqYPXt2ga/oqHTxPJcPnufywfNcfniuy0dFOM8SoSj3ohERERFVYmrxYEYiIiKi92EgIiIiIrXHQERERERqj4GIiIiI1B4DkYqcPHkSrq6usLCwgEQiwb59+1RdUqW0YMECtGzZEgYGBjAxMUHfvn0RFxen6rIqndWrV8PBwUF8qJqjoyMOHz6s6rIqvYULF0IikWDSpEmqLqVSmTNnDiQSidJSr149VZdVKT169AhDhw6FkZERdHR00KhRI1y4cEEltTAQqUhGRgYaN26M3377TdWlVGoRERHw9vbG2bNnERoaipycHHTr1g0ZGRmqLq1SqVmzJhYuXIjo6GhcuHABX375Jfr06YNr166purRKKyoqCr///jscHBxUXUql1KBBAyQmJorLqVOnVF1SpfPs2TO0bdsWWlpaOHz4MK5fv47FixejWrVqKqlHrZ5DVJH06NEDPXr0UHUZld6RI0eUXgcFBcHExATR0dHo0KGDiqqqfFxdXZVe//TTT1i9ejXOnj2LBg0aqKiqyis9PR3u7u74448/8OOPP6q6nEpJU1Oz0N+6pNLz888/w9LSEoGBgWKbjY2NyurhFSJSK2lpaQCA6tWrq7iSyis3Nxfbtm1DRkYGHB0dVV1OpeTt7Q0XFxc4OTmpupRK69atW7CwsMDnn38Od3d3xMfHq7qkSufAgQNo0aIFBg4cCBMTEzRt2hR//PGHyurhFSJSG3l5eZg0aRLatm2Lhg0bqrqcSufKlStwdHREZmYm9PX1sXfvXtjb26u6rEpn27ZtuHjxIqKiolRdSqXVqlUrBAUFwc7ODomJifjhhx/Qvn17XL16FQYGBqour9K4e/cuVq9ejcmTJ+O7775DVFQUJkyYAKlUCk9Pz3Kvh4GI1Ia3tzeuXr3KuQBlxM7ODjExMUhLS8OuXbvg6emJiIgIhqJS9PDhQ0ycOBGhoaGoWrWqqsuptN6czuDg4IBWrVrBysoKO3bsgJeXlworq1zy8vLQokULzJ8/HwDQtGlTXL16FQEBASoJRPzKjNSCj48PgoODceLECdSsWVPV5VRKUqkUtra2aN68ORYsWIDGjRtj+fLlqi6rUomOjkZKSgqaNWsGTU1NaGpqIiIiAitWrICmpiZyc3NVXWKlZGhoiLp16+L27duqLqVSMTc3L/AfTPXr11fZ15O8QkSVmiAIGD9+PPbu3Yvw8HCVTthTN3l5ecjKylJ1GZVKly5dcOXKFaW2ESNGoF69evDz84OGhoaKKqvc0tPTcefOHQwbNkzVpVQqbdu2LfAYlJs3b8LKykol9TAQqUh6errSf23cu3cPMTExqF69OmrVqqXCyioXb29vbNmyBfv374eBgQGSkpIAAHK5HDo6OiqurvLw9/dHjx49UKtWLTx//hxbtmxBeHg4QkJCVF1apWJgYFBg/puenh6MjIw4L64UTZ06Fa6urrCyskJCQgJmz54NDQ0NDBkyRNWlVSq+vr5o06YN5s+fj6+++grnz5/HmjVrsGbNGtUUJJBKnDhxQgBQYPH09FR1aZVKYecYgBAYGKjq0iqVkSNHClZWVoJUKhWMjY2FLl26CEePHlV1WWqhY8eOwsSJE1VdRqUyaNAgwdzcXJBKpcJnn30mDBo0SLh9+7aqy6qUDh48KDRs2FDQ1tYW6tWrJ6xZs0ZltUgEQRBUE8WIiIiIKgZOqiYiIiK1x0BEREREao+BiIiIiNQeAxERERGpPQYiIiIiUnsMRERERKT2GIiIiIhI7TEQEX0COnXqhEmTJqm6jGIJDw+HRCJBampqkfqXxTHOmTMHTZo0eef6oKAgGBoaluo+KwuJRIJ9+/apugyicsNARFRBDB8+HBKJpMBy+/Zt7NmzB/PmzRP7WltbY9myZeVe44cCxpvatGmDxMREyOXyMqtn7969aN26NeRyOQwMDNCgQYNihapBgwbh5s2bJd7/u96z/MXa2rrEY5dEdnY2atSogYULFxa6ft68eTA1NUVOTk651kX0KWAgIqpAunfvjsTERKXFxsYG1atXh4GBgarLK7KcnBxIpVKYmZlBIpGUyT6OHz+OQYMGwc3NDefPn0d0dDR++umnYv2x19HRgYmJSYlrWL58udJ7BQCBgYHi66ioqBKPXRJSqRRDhw5FYGBggXWCICAoKAgeHh7Q0tIq17qIPgUMREQViLa2NszMzJQWDQ0Npa+TOnXqhAcPHsDX11e8EpHfXthVivv37wN4/RXI6tWr0aNHD+jo6ODzzz/Hrl27lPbv5+eHunXrQldXF59//jlmzpwpBoygoCD88MMPuHz5sjh2UFCQ0ti9e/eGnp4efvrpp0K/Mjt9+jQ6deoEXV1dVKtWDc7Oznj27Fmh5+LQoUOQy+XYvHlzoesPHjyItm3bYtq0abCzs0PdunXRt29f/Pbbb+88v3fu3MHnn38OHx8fMSC8+ZVZ/hWwP//8E9bW1pDL5Rg8eDCeP39e6HhyuVzpvQIAQ0NDmJmZ4bvvvsOIESOU+ufk5MDExATr1q0D8Po98/HxgY+PD+RyOWrUqIGZM2fizV9UysrKwtSpU/HZZ59BT08PrVq1Qnh4+DuP0cvLCzdv3sSpU6eU2iMiInD37l14eXkhKioKXbt2RY0aNSCXy9GxY0dcvHjxnWMW9l7GxMQofb4A4NSpU2jfvj10dHRgaWmJCRMmICMjQ1y/atUq1KlTB1WrVoWpqSkGDBjwzn0SlTcGIqJPzJ49e1CzZk3MnTtX6crEnj17lK5W9O/fH3Z2djA1NRW3nTlzJtzc3HD58mW4u7tj8ODB+Oeff8T1BgYGCAoKwvXr17F8+XL88ccfWLp0KYDXXy9NmTIFDRo0EPcxaNAgcds5c+agX79+uHLlCkaOHFmg7piYGHTp0gX29vaIjIzEqVOn4Orqitzc3AJ9t2zZgiFDhmDz5s1wd3cv9DyYmZnh2rVruHr1apHOW2xsLNq1a4evv/4aK1eufOeVqzt37mDfvn0IDg5GcHAwIiIi3vkV1PuMGjUKR44cEd8fAAgODsaLFy+UztuGDRugqamJ8+fPY/ny5ViyZAnWrl0rrvfx8UFkZCS2bduG2NhYDBw4EN27d8etW7cK3W+jRo3QsmVLrF+/Xqk9MDAQbdq0Qb169fD8+XN4enri1KlTOHv2LOrUqYOePXu+M/gVxZ07d9C9e3e4ubkhNjYW27dvx6lTp+Dj4wMAuHDhAiZMmIC5c+ciLi4OR44cQYcOHUq8P6JSp7KflSUiJZ6enoKGhoagp6cnLgMGDBAEoeAvmltZWQlLly5951hLliwRDA0Nhbi4OLENgDB27Filfq1atRLGjRv3znF++eUXoXnz5uLr2bNnC40bNy7QD4AwadIkpbYTJ04IAIRnz54JgiAIQ4YMEdq2bfvOfeUf48qVKwW5XC6Eh4e/s68gCEJ6errQs2dPAYBgZWUlDBo0SFi3bp2QmZlZoN7Tp08L1apVE/773/8qjREYGCjI5XKl/rq6uoJCoRDbpk2bJrRq1eq9teQDIOzdu1d8bW9vL/z888/ia1dXV2H48OFKx1y/fn0hLy9PbPPz8xPq168vCIIgPHjwQNDQ0BAePXqktJ8uXboI/v7+76wjICBA0NfXF54/fy4IgiAoFApBV1dXWLt2baH9c3NzBQMDA+HgwYOFHsvb76UgCMKlS5cEAMK9e/cEQRAELy8vYcyYMUrj/v3330KVKlWEly9fCrt37xZkMpnSuSWqSHiFiKgC6dy5M2JiYsRlxYoVxR7j8OHD+Pbbb7F9+3bUrVtXaZ2jo2OB129eIdq+fTvatm0LMzMz6OvrY8aMGYiPjy/Sflu0aPHe9flXiN5n165d8PX1RWhoKDp27Pjevnp6ejh06BBu376NGTNmQF9fH1OmTMEXX3yBFy9eiP3i4+PRtWtXzJo1C1OmTPngcVhbWyvN1zI3N0dKSsoHtyvMqFGjxPk8ycnJOHz4cIGrZ61bt1a6WuXo6Ihbt24hNzcXV65cQW5uLurWrQt9fX1xiYiIwJ07d9653yFDhiA3Nxc7duwA8Pp9rVKlinhlKjk5GaNHj0adOnUgl8shk8mQnp5e5Pe6MJcvX0ZQUJBSnc7OzsjLy8O9e/fQtWtXWFlZ4fPPP8ewYcOwefNmpfeJSNUYiIgqED09Pdja2oqLubl5sba/fv06Bg8ejIULF6Jbt27F2jYyMhLu7u7o2bMngoODcenSJXz//ffIzs4ucu3vo6Oj88ExmjZtCmNjY6xfv15pHs371K5dG6NGjcLatWtx8eJFXL9+Hdu3bxfXGxsb44svvsDWrVuhUCg+ON7bE44lEgny8vKKVMvbPDw8cPfuXURGRmLTpk2wsbFB+/bti7x9eno6NDQ0EB0drRSU//nnHyxfvvyd28lkMgwYMEAMY4GBgfjqq6+gr68PAPD09ERMTAyWL1+OM2fOICYmBkZGRu98r6tUef2n4s335O3J6+np6fjPf/6jVOfly5dx69Yt1K5dGwYGBrh48SK2bt0Kc3NzzJo1C40bNy7yYxmIyhoDEdEnSCqVFph78+TJE7i6usLNzQ2+vr6Fbnf27NkCr+vXrw8AOHPmDKysrPD999+jRYsWqFOnDh48ePDB/RaVg4MDjh8//t4+tWvXxokTJ7B//36MHz++2PuwtraGrq6u0kReHR0dBAcHo2rVqnB2dv6oeTLFZWRkhL59+yIwMBBBQUEFJlkDwLlz55Re58/p0dDQQNOmTZGbm4uUlBSloGxraytO4n4XLy8vnDp1CsHBwThz5gy8vLzEdadPn8aECRPQs2dPNGjQANra2njy5Mk7xzI2NgYApflQMTExSn2aNWuG69evF6jT1tYWUqkUAKCpqQknJycsWrQIsbGxuH//PsLCwt57HETlhYGI6BNkbW2NkydP4tGjR+IfMjc3N+jq6mLOnDlISkoSlzcDzM6dO7F+/XrcvHkTs2fPxvnz58VJr3Xq1EF8fDy2bduGO3fuYMWKFdi7d2+B/d67dw8xMTF48uQJsrKyilyzv78/oqKi8M033yA2NhY3btzA6tWrC/whrlu3Lk6cOIHdu3e/95lCc+bMwfTp0xEeHo579+7h0qVLGDlyJHJyctC1a1elvvlfr2lqaqJHjx5IT08vct0fa9SoUdiwYQP++ecfeHp6FlgfHx+PyZMnIy4uDlu3bsWvv/6KiRMnAnh9Ltzd3eHh4YE9e/bg3r17OH/+PBYsWIBDhw69d78dOnSAra0tPDw8UK9ePbRp00ZcV6dOHfz555/4559/cO7cObi7u7/3Cp6trS0sLS0xZ84c3Lp1C4cOHcLixYuV+vj5+eHMmTPw8fFBTEwMbt26hf3794ufr+DgYKxYsQIxMTF48OABNm7ciLy8PNjZ2RX5XBKVJQYiok/Q3Llzcf/+fdSuXVv8r/eTJ0/i6tWrsLKygrm5ubg8fPhQ3O6HH37Atm3b4ODggI0bN2Lr1q2wt7cHAPTu3Ru+vr7w8fFBkyZNcObMGcycOVNpv25ubujevTs6d+4MY2NjbN26tcg1161bF0ePHsXly5fxxRdfwNHREfv374empmaBvnZ2dggLC8PWrVvfOe+nY8eOuHv3rvgHv0ePHkhKSsLRo0cL/SOrr6+Pw4cPQxAEuLi4KF1FKktOTk4wNzeHs7MzLCwsCqz38PDAy5cv8cUXX8Db2xsTJ07EmDFjxPWBgYHw8PDAlClTYGdnh759+yIqKgq1atV6734lEglGjhyJZ8+eFZi3tG7dOjx79gzNmjXDsGHDMGHChPc+j0lLSwtbt27FjRs34ODggJ9//hk//vijUh8HBwdERETg5s2baN++PZo2bYpZs2aJx2xoaIg9e/bgyy+/RP369REQEICtW7eiQYMGHzyHROVBIhT1i3oi+qRJJBLs3bsXffv2VXUpaiU9PR2fffYZAgMD0b9/f6V1nTp1QpMmTVTy1HEiUlbwP82IiOij5eXl4cmTJ1i8eDEMDQ3Ru3dvVZdERO/BQEREVAbi4+NhY2ODmjVrIigoqNCvBomo4uBXZkRERKT2OKmaiIiI1B4DEREREak9BiIiIiJSewxEREREpPYYiIiIiEjtMRARERGR2mMgIiIiIrXHQERERERqj4GIiIiI1N7/A58nrmUXnGBvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Select the column you want to plot\n",
        "column = valid_df['fitzpatrick_skin_type_masked']\n",
        "\n",
        "# Plot the distribution of the selected column\n",
        "n, bins, patches = plt.hist(column, bins=30, color='blue', alpha=0.7)\n",
        "\n",
        "# Add labels and titles to your plot\n",
        "plt.xlabel('Fitzpatrick Skin Type Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Fitzpatrick Skin Type')\n",
        "\n",
        "# Add the counts on top of the bars\n",
        "for i, patch in enumerate(patches):\n",
        "    if n[i] != 0:\n",
        "        plt.text(patch.get_x() + patch.get_width()/2,\n",
        "             patch.get_height() + 70,\n",
        "             int(n[i]),\n",
        "             ha='center', color='black')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-MIkDKCGs9H",
        "outputId": "b928ab6c-8f57-4429-e09e-4f387e53bb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 1) 5517\n",
            "(0, 2) 812\n",
            "(0, 3) 585\n",
            "(0, 4) 336\n",
            "(0, 5) 241\n",
            "(0, 6) 646\n",
            "(1, 1) 717\n",
            "(1, 2) 128\n",
            "(1, 3) 82\n",
            "(1, 4) 52\n",
            "(1, 5) 37\n",
            "(1, 6) 262\n",
            "Smallest group selection for a begnin and malinat in a FST 37\n",
            "val_indexes_min_count size: 444 444\n",
            "444\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "grouped = valid_df.groupby([\"target\", \"fitzpatrick_skin_type\"])\n",
        "\n",
        "group_indexes = grouped.indices\n",
        "\n",
        "val_indexes_min_count = []\n",
        "\n",
        "smallest_count = math.inf\n",
        "for group, index_list in group_indexes.items():\n",
        "    size = len(index_list)\n",
        "    print(group, size)\n",
        "    smallest_count = size if size < smallest_count else smallest_count\n",
        "\n",
        "print(f\"Smallest group selection for a begnin and malinat in a FST {smallest_count}\")\n",
        "\n",
        "for group, index_list in group_indexes.items():\n",
        "    \n",
        "    #print(list(random.sample(list(index_list), smallest_count)))\n",
        "    val_indexes_min_count += random.sample(list(index_list), smallest_count)\n",
        "\n",
        "print(f\"val_indexes_min_count size: {len(val_indexes_min_count)} {smallest_count * 6 * 2}\")\n",
        "\n",
        "valid_min_count_df = valid_df.iloc[val_indexes_min_count]\n",
        "valid_min_count_df = valid_min_count_df.reset_index(drop=True)\n",
        "print(len(valid_min_count_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "E3efTfOVhgT_"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    test_isic_2016_df = pd.read_csv('ISBI2016_ISIC_Part3_Training_GroundTruth.csv', header=None)\n",
        "    test_isic_2016_df.columns = [\"dcm_name\", \"target_raw\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZcRso0neh9mq"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    isic_image_path = \"../input/isic_2016/ISBI2016_ISIC_Part3_Training_Data/\"\n",
        "    test_isic_2016_df['path_jpg'] = isic_image_path + test_isic_2016_df['dcm_name'] + '.jpg'\n",
        "    le = LabelEncoder()\n",
        "    test_isic_2016_df['target'] = le.fit_transform(test_isic_2016_df['target_raw'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Wlmniehohvqg",
        "outputId": "280807a3-a15f-46d3-a412-f163d7f75385"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0      dcm_name target_raw  \\\n",
              "0           0  ISIC_0000000     benign   \n",
              "1           1  ISIC_0000001     benign   \n",
              "2           2  ISIC_0000002  malignant   \n",
              "3           3  ISIC_0000004  malignant   \n",
              "4           4  ISIC_0000006     benign   \n",
              "\n",
              "                                            path_jpg  target         ita  \\\n",
              "0  ../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...       0  125.449546   \n",
              "1  ../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...       0   97.399672   \n",
              "2  ../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...       1  139.238798   \n",
              "3  ../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...       1  -89.999949   \n",
              "4  ../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...       0  131.610346   \n",
              "\n",
              "   fizpatrick_skin_type  \n",
              "0                     1  \n",
              "1                     1  \n",
              "2                     1  \n",
              "3                     6  \n",
              "4                     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de6939a3-43cd-4d8b-8733-f970180cf3b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>dcm_name</th>\n",
              "      <th>target_raw</th>\n",
              "      <th>path_jpg</th>\n",
              "      <th>target</th>\n",
              "      <th>ita</th>\n",
              "      <th>fizpatrick_skin_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ISIC_0000000</td>\n",
              "      <td>benign</td>\n",
              "      <td>../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...</td>\n",
              "      <td>0</td>\n",
              "      <td>125.449546</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ISIC_0000001</td>\n",
              "      <td>benign</td>\n",
              "      <td>../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...</td>\n",
              "      <td>0</td>\n",
              "      <td>97.399672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ISIC_0000002</td>\n",
              "      <td>malignant</td>\n",
              "      <td>../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...</td>\n",
              "      <td>1</td>\n",
              "      <td>139.238798</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>ISIC_0000004</td>\n",
              "      <td>malignant</td>\n",
              "      <td>../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...</td>\n",
              "      <td>1</td>\n",
              "      <td>-89.999949</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ISIC_0000006</td>\n",
              "      <td>benign</td>\n",
              "      <td>../input/isic_2016/ISBI2016_ISIC_Part3_Trainin...</td>\n",
              "      <td>0</td>\n",
              "      <td>131.610346</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de6939a3-43cd-4d8b-8733-f970180cf3b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-de6939a3-43cd-4d8b-8733-f970180cf3b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-de6939a3-43cd-4d8b-8733-f970180cf3b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "test_isic_2016_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xObJ_XhSjoj2",
        "outputId": "973038ac-c853-4870-9925-bc5f3269630f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISIC 2016 test set len: 900\n"
          ]
        }
      ],
      "source": [
        "print(f\"ISIC 2016 test set len: {len(test_isic_2016_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzPChs5MD7EM"
      },
      "source": [
        "## 2.2 PyTorch Dataset ðŸ’¾\n",
        "\n",
        "This class retrieves the data from the `train_df` or `test_df` and reads the corresponding information from the image folders.\n",
        "\n",
        "> Note: when reading the images, custom `augmentations` are applied. Train will have a complex transformation, while valid data will have no augmentations. Test WILL have `augmentations` like Train because we're doing **Test Time Augmentations**, meaning that we'll transform the test images, predict and **average** the result.\n",
        "<img src='https://i.imgur.com/UcZBsMJ.png' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9XJmbMJaD7EM"
      },
      "outputs": [],
      "source": [
        "# ----- STATICS -----\n",
        "vertical_flip = 0.5\n",
        "horizontal_flip = 0.5\n",
        "\n",
        "csv_columns = ['sex', 'age', 'anatomy']\n",
        "no_columns = 3\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEzoej1zD7EN",
        "outputId": "7901672b-df31-437c-bf01-26d0570880d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02837522, 0.9931326 , 0.11350087], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Example of csv_data at index=0\n",
        "np.array(train_df.iloc[0][csv_columns].values,dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KskXQLuID7EN"
      },
      "outputs": [],
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, dataframe, vertical_flip, horizontal_flip,\n",
        "                 is_train=True, is_valid=False, is_test=False, generate_skin_transform=False, mask_path=\"\", use_synthetic_data_percentage=0):\n",
        "        self.dataframe, self.is_train, self.is_valid = dataframe, is_train, is_valid\n",
        "        self.vertical_flip, self.horizontal_flip = vertical_flip, horizontal_flip\n",
        "        self.generate_skin_transform = generate_skin_transform\n",
        "        self.mask_path = mask_path\n",
        "        self.use_synthetic_data_percentage = use_synthetic_data_percentage\n",
        "        \n",
        "        # Data Augmentation (custom for each dataset type)\n",
        "        if is_train or is_test:\n",
        "            self.transform = Compose([RandomResizedCrop(height=224, width=224, scale=(0.4, 1.0)),\n",
        "                                      ShiftScaleRotate(rotate_limit=90, scale_limit = [0.8, 1.2]),\n",
        "                                      HorizontalFlip(p = self.horizontal_flip),\n",
        "                                      VerticalFlip(p = self.vertical_flip),\n",
        "                                      HueSaturationValue(sat_shift_limit=[0.7, 1.3], \n",
        "                                                         hue_shift_limit=[-0.1, 0.1]),\n",
        "                                      RandomBrightnessContrast(brightness_limit=[0.7, 1.3],\n",
        "                                                               contrast_limit= [0.7, 1.3]),\n",
        "                                      Normalize(),\n",
        "                                      ToTensorV2()])\n",
        "        else:\n",
        "            self.transform = Compose([Normalize(),\n",
        "                                      ToTensorV2()])\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Select path and read image\n",
        "        image_path = self.dataframe['path_jpg'][index]\n",
        "\n",
        "        # Check to see if we want to use synthetic data. If so the roll and see if we should use synthetic image\n",
        "        if self.use_synthetic_data_percentage > 0:\n",
        "            if self.use_synthetic_data_percentage < random.random():\n",
        "                image_path = str(synthetici_data_path / Path(image_path).name)\n",
        "        #print(image_path)\n",
        "\n",
        "        image_rgb = cv2.imread(image_path) \n",
        "        if image_rgb.shape[:2] != (224,224):\n",
        "            image_rgb = cv2.resize(image_rgb, (224,224))\n",
        "\n",
        "                        \n",
        "        # For this image also import .csv information (sex, age, anatomy)\n",
        "        #csv_data = np.array(self.dataframe.iloc[index][['sex', 'age', 'anatomy']].values, \n",
        "        #                    dtype=np.float32)\n",
        "        csv_data = \"\"\n",
        "        \n",
        "        # Apply transforms        \n",
        "        image = self.transform(image=image_rgb)\n",
        "        # Extract image from dictionary\n",
        "        image = image['image']\n",
        "\n",
        "        ita_value = self.dataframe['ita'][index]\n",
        "        fst = self.dataframe['fitzpatrick_skin_type'][index]\n",
        "\n",
        "        # Transform skin\n",
        "        if self.generate_skin_transform:\n",
        "            # Resize image to model expected input size            \n",
        "            image_rgb = cv2.resize(image_rgb, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "\n",
        "            # Get the mask and resize it to model input size\n",
        "            file_name = Path(image_path).name            \n",
        "            updated_mask_path = self.mask_path / str(file_name).replace(\"jpg\", \"png\")                      \n",
        "            im = Image.open(str(updated_mask_path))            \n",
        "            mask = im.resize((224, 224))\n",
        "            # Due to how the mask was created, it needs to be rotated and flipped\n",
        "            mask = mask.rotate(270)\n",
        "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            # Transform skin image\n",
        "            transformed_image = transform_image(image_rgb, mask, image_ita=ita_value, verbose=False)\n",
        "            #print(\"Transformed image shape\", transformed_image.shape)\n",
        "            \n",
        "            transformed_image = self.transform(image=transformed_image)\n",
        "            transformed_image = transformed_image[\"image\"]\n",
        "\n",
        "            transformed_image_wihtout_mask = transform_image(image_rgb, image_ita=ita_value, verbose=False)\n",
        "            transformed_image_wihtout_mask = self.transform(image=transformed_image_wihtout_mask)\n",
        "            transformed_image_wihtout_mask = transformed_image_wihtout_mask[\"image\"]\n",
        "            #print(\"After torch transform\", transformed_image)\n",
        "        \n",
        "        # If train/valid: image + class | If test: only image\n",
        "        if self.is_train or self.is_valid:\n",
        "            return (image, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask), self.dataframe['target'][index]\n",
        "        else:\n",
        "            return (image, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HKGhO2rM9TI"
      },
      "source": [
        "# Generating Fitzpatrick skin type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "QTrMZ-ydUgRb",
        "outputId": "302b359a-5192-40a2-a00e-bd2722e89fb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       dcm_name          ID       sex       age   anatomy  target  \\\n",
              "0  ISIC_6507492  IP_6134378  0.028375  0.993133  0.113501       0   \n",
              "1  ISIC_1985984  IP_1645936  0.000000  0.999882  0.015383       0   \n",
              "2  ISIC_7233663  IP_3735763  0.019992  0.999600  0.019992       0   \n",
              "3  ISIC_3003627  IP_4898383  0.033023  0.990687  0.132092       0   \n",
              "4  ISIC_0886205  IP_2153088  0.000000  0.991228  0.132164       0   \n",
              "\n",
              "                                            path_jpg        ita  \\\n",
              "0  ../input/melanoma-external-malignant-256/train...  71.273186   \n",
              "1  ../input/melanoma-external-malignant-256/train...  79.947555   \n",
              "2  ../input/melanoma-external-malignant-256/train...  71.449125   \n",
              "3  ../input/melanoma-external-malignant-256/train...  85.556576   \n",
              "4  ../input/melanoma-external-malignant-256/train...  89.958897   \n",
              "\n",
              "   fitzpatrick_skin_type  ita_masked  fitzpatrick_skin_type_masked  \n",
              "0                      1   71.145157                             1  \n",
              "1                      1   79.705372                             1  \n",
              "2                      1   71.544231                             1  \n",
              "3                      1   85.091278                             1  \n",
              "4                      1   89.804639                             1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00bc0403-74cc-4110-b8e2-c2337fc63ffd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dcm_name</th>\n",
              "      <th>ID</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>anatomy</th>\n",
              "      <th>target</th>\n",
              "      <th>path_jpg</th>\n",
              "      <th>ita</th>\n",
              "      <th>fitzpatrick_skin_type</th>\n",
              "      <th>ita_masked</th>\n",
              "      <th>fitzpatrick_skin_type_masked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_6507492</td>\n",
              "      <td>IP_6134378</td>\n",
              "      <td>0.028375</td>\n",
              "      <td>0.993133</td>\n",
              "      <td>0.113501</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>71.273186</td>\n",
              "      <td>1</td>\n",
              "      <td>71.145157</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_1985984</td>\n",
              "      <td>IP_1645936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999882</td>\n",
              "      <td>0.015383</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>79.947555</td>\n",
              "      <td>1</td>\n",
              "      <td>79.705372</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_7233663</td>\n",
              "      <td>IP_3735763</td>\n",
              "      <td>0.019992</td>\n",
              "      <td>0.999600</td>\n",
              "      <td>0.019992</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>71.449125</td>\n",
              "      <td>1</td>\n",
              "      <td>71.544231</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_3003627</td>\n",
              "      <td>IP_4898383</td>\n",
              "      <td>0.033023</td>\n",
              "      <td>0.990687</td>\n",
              "      <td>0.132092</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>85.556576</td>\n",
              "      <td>1</td>\n",
              "      <td>85.091278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0886205</td>\n",
              "      <td>IP_2153088</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991228</td>\n",
              "      <td>0.132164</td>\n",
              "      <td>0</td>\n",
              "      <td>../input/melanoma-external-malignant-256/train...</td>\n",
              "      <td>89.958897</td>\n",
              "      <td>1</td>\n",
              "      <td>89.804639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00bc0403-74cc-4110-b8e2-c2337fc63ffd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00bc0403-74cc-4110-b8e2-c2337fc63ffd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00bc0403-74cc-4110-b8e2-c2337fc63ffd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Hog0xXkuU3np",
        "outputId": "0a28aa70-40b8-4135-8c1a-9ef1a6684650"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'../input/melanoma-external-malignant-256/train/train/ISIC_6507492.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "train_df.iloc[0][\"path_jpg\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oe3Ii3LzTTMs"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    get_cropped_center_ita(image=Image.open(train_df.iloc[0][\"path_jpg\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bozpyKqFVYkR"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    train_df[\"ita\"] = train_df[\"path_jpg\"].apply(lambda x: get_cropped_center_ita(image=Image.open(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hX5yiwQOVZnY"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    train_df[\"fitzpatrick_skin_type\"] = train_df[\"ita\"].apply(lambda ita: get_kinyanjui_groh_type(ita))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Z9UVocUUVZd4"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    train_df.to_csv(\"saved_data_2023_01_28_kaggle_melanoma_2020.csv\")\n",
        "    %cp saved_data_2023_01_28_kaggle_melanoma_2020.csv /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "hUOnvU5-VZMq"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icu6aCdZTFVn"
      },
      "source": [
        "#### Calculate FSK on Melanoma using masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Bmm5j8ziTsZE"
      },
      "outputs": [],
      "source": [
        "def calculate_ita(image_path):        \n",
        "    image_rgb = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
        "    orig_rgb = image_rgb       \n",
        "    image_rgb = cv2.resize(image_rgb, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "\n",
        "    # Get the mask and resize it to model input size\n",
        "    file_name = Path(image_path).name            \n",
        "    updated_mask_path = mask_path / str(file_name).replace(\"jpg\", \"png\")                      \n",
        "    im = Image.open(str(updated_mask_path))            \n",
        "    mask = im.resize((224, 224))\n",
        "    # Due to how the mask was created, it needs to be rotated and flipped\n",
        "    mask = mask.rotate(270)\n",
        "    mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    \n",
        "    mask = mask.convert(\"1\")\n",
        "    \n",
        "    image_rgb[:, :, 0][mask] = 0\n",
        "    image_rgb[:, :, 1][mask] = 0\n",
        "    image_rgb[:, :, 2][mask] = 0\n",
        "    image_pil = Image.fromarray(image_rgb) \n",
        "\n",
        "    #plt.imshow(orig_rgb)\n",
        "    #plt.show()   \n",
        "    ita = get_ita(image_pil)   \n",
        "    # Case where mask is too big and when we remove the boarder the left over image is all mask. \n",
        "    # This case lets turn off removing boarder\n",
        "    if np.isnan(ita):\n",
        "        ita = get_ita(image_pil,remove_boarder=False)\n",
        "        if np.isnan(ita):\n",
        "            ita = get_ita(orig_rgb,remove_boarder=False)    \n",
        "    return ita\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK8Wzx1kTD5U",
        "outputId": "d6515704-b633-4766-d04a-4a725b1031ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79.70537232494848"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# apply masks, get ITA on full image, save to DF, then save off those results\n",
        "calculate_ita(train_df.iloc[1][\"path_jpg\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Ze1nNCg-aW_N"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    everything_df[\"ita_masked\"] = everything_df[\"path_jpg\"].apply(lambda x: calculate_ita(x))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Z-MO8zmsahSq"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    everything_df[\"fitzpatrick_skin_type_masked\"] = everything_df[\"ita_masked\"].apply(lambda ita: get_kinyanjui_groh_type(ita))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "C-PXH6BBaqYh"
      },
      "outputs": [],
      "source": [
        "everything_df.to_csv(\"saved_data_2023_02_14_melanoma_2020_kaggle.csv\")\n",
        "%cp saved_data_2023_02_14_melanoma_2020_kaggle.csv /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcykPXmnkHdR"
      },
      "source": [
        "#### Generate for isic 2016"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uqkWr0OGkMc8"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    test_isic_2016_df[\"ita\"] = test_isic_2016_df[\"path_jpg\"].apply(lambda x: get_cropped_center_ita(image=Image.open(x)))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Wk9a4SA9kTJ8"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    test_isic_2016_df[\"fitzpatrick_skin_type\"] = test_isic_2016_df[\"ita\"].apply(lambda ita: get_kinyanjui_groh_type(ita))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RCtiTvlgkU42"
      },
      "outputs": [],
      "source": [
        "if not use_data_cached:\n",
        "    test_isic_2016_df.to_csv(\"saved_data_2023_02_02_ISIC_2016_test_set.csv\")\n",
        "    %cp saved_data_2023_02_02_ISIC_2016_test_set.csv /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXlGsvovNDsY"
      },
      "source": [
        "# Generating masks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx68maJsYoDx"
      },
      "source": [
        "## Setup DoubleUNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "GhVEp_4nNDS_"
      },
      "outputs": [],
      "source": [
        "path_to_model_h5 = 'model.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc3Oae7HY1tS"
      },
      "source": [
        "### Main Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "LiiXTPJwY1Gc"
      },
      "outputs": [],
      "source": [
        "def squeeze_excite_block(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    channel_axis = -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = Multiply()([init, se])\n",
        "    return x\n",
        "\n",
        "def conv_block(inputs, filters):\n",
        "    x = inputs\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = squeeze_excite_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder1(inputs):\n",
        "    skip_connections = []\n",
        "\n",
        "    model = VGG19(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\n",
        "    for name in names:\n",
        "        skip_connections.append(model.get_layer(name).output)\n",
        "\n",
        "    output = model.get_layer(\"block5_conv4\").output\n",
        "    return output, skip_connections\n",
        "\n",
        "def decoder1(inputs, skip_connections):\n",
        "    num_filters = [256, 128, 64, 32]\n",
        "    skip_connections.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = Concatenate()([x, skip_connections[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "# def encoder2(inputs):\n",
        "#     skip_connections = []\n",
        "#\n",
        "#     output = DenseNet121(include_top=False, weights='imagenet')(inputs)\n",
        "#     model = tf.keras.models.Model(inputs, output)\n",
        "#\n",
        "#     names = [\"input_2\", \"conv1/relu\", \"pool2_conv\", \"pool3_conv\"]\n",
        "#     for name in names:\n",
        "#         skip_connections.append(model.get_layer(name).output)\n",
        "#     output = model.get_layer(\"pool4_conv\").output\n",
        "#\n",
        "#     return output, skip_connections\n",
        "\n",
        "def encoder2(inputs):\n",
        "    num_filters = [32, 64, 128, 256]\n",
        "    skip_connections = []\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = conv_block(x, f)\n",
        "        skip_connections.append(x)\n",
        "        x = MaxPool2D((2, 2))(x)\n",
        "\n",
        "    return x, skip_connections\n",
        "\n",
        "def decoder2(inputs, skip_1, skip_2):\n",
        "    num_filters = [256, 128, 64, 32]\n",
        "    skip_2.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = Concatenate()([x, skip_1[i], skip_2[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "def output_block(inputs):\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    return x\n",
        "\n",
        "def Upsample(tensor, size):\n",
        "    \"\"\"Bilinear upsampling\"\"\"\n",
        "    def _upsample(x, size):\n",
        "        return tf.image.resize(images=x, size=size)\n",
        "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n",
        "\n",
        "def ASPP(x, filter):\n",
        "    shape = x.shape\n",
        "\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n",
        "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n",
        "\n",
        "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "\n",
        "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def build_model(shape):\n",
        "    inputs = Input(shape)\n",
        "    x, skip_1 = encoder1(inputs)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder1(x, skip_1)\n",
        "    outputs1 = output_block(x)\n",
        "\n",
        "    x = inputs * outputs1\n",
        "\n",
        "    x, skip_2 = encoder2(x)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder2(x, skip_1, skip_2)\n",
        "    outputs2 = output_block(x)\n",
        "    outputs = Concatenate()([outputs1, outputs2])\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6vyYTz5Y9So"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "g0F5G1-YZApt"
      },
      "outputs": [],
      "source": [
        "\n",
        "smooth = 1e-15\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + smooth) / (union + smooth)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def focal_loss(y_true, y_pred):\n",
        "    alpha=0.25\n",
        "    gamma=2\n",
        "    def focal_loss_with_logits(logits, targets, alpha, gamma, y_pred):\n",
        "        weight_a = alpha * (1 - y_pred) ** gamma * targets\n",
        "        weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n",
        "        return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(-logits)) * (weight_a + weight_b) + logits * weight_b\n",
        "\n",
        "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
        "    logits = tf.math.log(y_pred / (1 - y_pred))\n",
        "    loss = focal_loss_with_logits(logits=logits, targets=y_true, alpha=alpha, gamma=gamma, y_pred=y_pred)\n",
        "    # or reduce_sum and/or axis=-1\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "-UecnACgZAgi"
      },
      "outputs": [],
      "source": [
        "def load_model_weight(path):\n",
        "    with CustomObjectScope({\n",
        "        'dice_loss': dice_loss,\n",
        "        'dice_coef': dice_coef,\n",
        "        'bce_dice_loss': bce_dice_loss,\n",
        "        'focal_loss': focal_loss,\n",
        "        'iou': iou\n",
        "        }):\n",
        "        model = load_model(path)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XkH4f4FPH0yj"
      },
      "outputs": [],
      "source": [
        "def read_image(x):\n",
        "    if isinstance(x, Image.Image):\n",
        "        # Convert the PIL image to a NumPy array\n",
        "        np_image = np.array(x)\n",
        "\n",
        "        # Convert the NumPy array to a cv2 image\n",
        "        image = cv2.imdecode(np_image, cv2.IMREAD_COLOR)\n",
        "    else: # case when x is a path to a file\n",
        "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "\n",
        "    image = np.clip(image - np.median(image)+127, 0, 255)\n",
        "    image = image/255.0\n",
        "    image = image.astype(np.float32)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    return image\n",
        "\n",
        "def mask_to_3d(mask):\n",
        "    mask = np.squeeze(mask)\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask\n",
        "\n",
        "def parse(y_pred):\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
        "    y_pred = y_pred[..., -1]\n",
        "    y_pred = y_pred.astype(np.float32)\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Yftjon06ZAZw"
      },
      "outputs": [],
      "source": [
        "def generate_mask(model, image_path):\n",
        "    image = Image.open(image_path).resize((512,384))\n",
        "    image.save(resize_folder/image_path.name)\n",
        "    print(f\"saved to: {resize_folder/image_path.name}\")\n",
        "    x = read_image(str(resize_folder/image_path.name))\n",
        "    _, h, w, _ = x.shape\n",
        "\n",
        "    line = np.ones((h, 10, 3)) * 255.0\n",
        "\n",
        "    y_pred1 = parse(model.predict(x)[0][..., -2])\n",
        "    y_pred2 = parse(model.predict(x)[0][..., -1])\n",
        "\n",
        "    all_images = [            \n",
        "            mask_to_3d(y_pred1) * 255.0, line,\n",
        "            mask_to_3d(y_pred2) * 255.0\n",
        "        ]\n",
        "\n",
        "    mask1 = np.concatenate(mask_to_3d(y_pred1) * 255.0, axis=1)\n",
        "    mask2 = np.concatenate(mask_to_3d(y_pred2) * 255.0, axis=1)\n",
        "\n",
        "    cv2.imwrite(f\"results1/{image_path.name.split('.')[0]}.png\", mask1)\n",
        "    cv2.imwrite(f\"results2/{image_path.name.split('.')[0]}.png\", mask2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZDRSAM7ZP6I"
      },
      "source": [
        "### Load and test DoubleU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "y9apuGMxyRgD"
      },
      "outputs": [],
      "source": [
        "!mkdir results\n",
        "!mkdir results1\n",
        "!mkdir results2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "-4vLNYguGumP"
      },
      "outputs": [],
      "source": [
        "if generate_masks:\n",
        "    %cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/DoubleUNet.zip /content/\n",
        "    file_name_with_path = \"/content/DoubleUNet.zip\"\n",
        "    with zipfile.ZipFile(file_name_with_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/\")\n",
        "    \n",
        "    path_to_model_h5 = '/content/DoubleUNet/files/model.h5'\n",
        "    double_u_net_model = load_model_weight(path_to_model_h5)\n",
        "    resize_folder = Path(\"./ISIC_2016_resize\")\n",
        "    resize_folder.mkdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4bWO3HUaUyL"
      },
      "source": [
        "### Generate all masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "j9q_Hvq-Y_w_"
      },
      "outputs": [],
      "source": [
        "if generate_masks:\n",
        "    count = 0\n",
        "    size = len(test_isic_2016_df)\n",
        "    break_point = size\n",
        "    for index, row in test_isic_2016_df.iterrows():\n",
        "        if count == break_point:\n",
        "            break\n",
        "        print(index, count,f\"percent done: {100 * (index + 1)/size} (count/size)({index+1}/{size})\" )\n",
        "        generate_mask(double_u_net_model, Path(test_isic_2016_df.iloc[index][\"path_jpg\"]))\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "hGF4HXBXY_oD"
      },
      "outputs": [],
      "source": [
        "if generate_masks:\n",
        "    # if you want to generate masks and save to google drive then uncomment this.\n",
        "    !zip -r isic_2016_task3_mask_results1_2023_02_02.zip results1\n",
        "    %cp isic_2016_task3_mask_results1_2023_02_02.zip /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/\n",
        "    !zip -r isic_2016_task3_mask_results2_2023_02_02.zip results2\n",
        "    %cp isic_2016_task3_mask_results2_2023_02_02.zip /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcur3T8OUgna"
      },
      "source": [
        "# Skin Transformer\n",
        "\n",
        "This section has functionality to take an image and make the skin darker or lighter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv0lizcOUkC2"
      },
      "source": [
        "## Fitzpatrick Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "vDG962P8UvnU"
      },
      "outputs": [],
      "source": [
        "from numpy.random.mtrand import randint\n",
        "from derm_ita import get_ita, get_kinyanjui_type, get_cropped_center_ita, get_kinyanjui_groh_type\n",
        "\n",
        "\n",
        "Fitzpatrick_Skin_Type = Enum(\"Fitzpatrick_Skin_Type\", [\"_1\",\"_2\",\"_3\",\"_4\",\"_5\",\"_6\"])\n",
        "\n",
        "\n",
        "def random_ita_from_FST(skin_type: Fitzpatrick_Skin_Type):\n",
        "  \"\"\"\n",
        "  This function will return a random number between the ITA ranges provided in\n",
        "  https://openaccess.thecvf.com/content/CVPR2021W/ISIC/papers/Groh_Evaluating_Deep_Neural_Networks_Trained_on_Clinical_Images_in_Dermatology_CVPRW_2021_paper.pdf\n",
        "    For skin types of 6 and 1 there are open ranges. The selected end ranges comes from computing the ITA\n",
        "    on all the ISIC 2017 dataset images and takeing the largest ranges.\n",
        "  :skin_type - input of the Fitzpatrick skin type\n",
        "  \"\"\"\n",
        "  if skin_type == Fitzpatrick_Skin_Type._6:\n",
        "    return randint(-10,10)\n",
        "  elif skin_type == Fitzpatrick_Skin_Type._5:\n",
        "    return randint(10,19)\n",
        "  elif skin_type == Fitzpatrick_Skin_Type._4:\n",
        "    return randint(19,28)\n",
        "  elif skin_type == Fitzpatrick_Skin_Type._3:\n",
        "    return randint(28,41)\n",
        "  elif skin_type == Fitzpatrick_Skin_Type._2:\n",
        "    return randint(41,55)\n",
        "  elif skin_type == Fitzpatrick_Skin_Type._1:\n",
        "    return randint(55,90)\n",
        "\n",
        "def random_FST(exclude=None):\n",
        "    \"\"\"\n",
        "    This function will randomly pick a fitzpatrick skin type \n",
        "    :exclude - given a FST, this will not be chosen when randomly picking a FST\n",
        "    \"\"\"\n",
        "    if exclude is None:\n",
        "        return Fitzpatrick_Skin_Type[f\"_{random.randint(1,6)}\"]\n",
        "    else:\n",
        "        while True:\n",
        "            fst = Fitzpatrick_Skin_Type[f\"_{random.randint(1,6)}\"]\n",
        "            if fst != exclude:\n",
        "                return fst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGcDMyJUntX"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dEzLeKzMUyKk"
      },
      "outputs": [],
      "source": [
        "def safe_convert(x, new_dtype):\n",
        "        info = np.iinfo(new_dtype)\n",
        "        return x.clip(info.min, info.max).astype(new_dtype)\n",
        "        \n",
        "def transform_image(image, mask=None, desired_fst=None, image_ita=None, verbose=False):\n",
        "    \"\"\"\n",
        "    1. Compute ITA of current image and retrieve Fitzpatrick skin type\n",
        "    2. Select random FST thatâ€™s different\n",
        "    3. Select random ITA number within range of selected FST\n",
        "    4. Compute difference  original ITA â€“ new desired ITA\n",
        "    5. Adjusted b = old b value + (difference * .5)\n",
        "    6. Adjusted L = old L value + (difference * .12)\n",
        "    :image - rgb image\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose: print(f\"image type {type(image)}\")\n",
        "    if image_ita is None:\n",
        "        image_ita = get_cropped_center_ita(image)\n",
        "    if verbose: print(f\"ITA {image_ita}\")\n",
        "\n",
        "    fst = Fitzpatrick_Skin_Type[f\"_{get_kinyanjui_groh_type(image_ita)}\"]\n",
        "    if verbose: print(f\"FST {fst}\")\n",
        "\n",
        "    # select random FST unless specified\n",
        "    if desired_fst is None:\n",
        "        random_fst = random_FST(exclude=fst)\n",
        "    else:\n",
        "        random_fst = desired_fst\n",
        "\n",
        "    if verbose: print(f\"Random FST: {random_fst}\")\n",
        "\n",
        "    random_ita = random_ita_from_FST(random_fst)\n",
        "    if verbose: print(f\"Random ita: {random_ita}\")\n",
        "\n",
        "    difference = image_ita - random_ita\n",
        "    if verbose: print(f\"Difference ita: {difference}\")\n",
        "\n",
        "    # convert to boolean array\n",
        "    if mask is not None:\n",
        "        mask = ImageOps.invert(mask)\n",
        "        mask = mask.convert(\"1\")\n",
        "\n",
        "    # convert base image to NumPy array\n",
        "    image_array = np.array(image)\n",
        "\n",
        "    # Convert the base image NumPy arrays to a LAB color space\n",
        "    if image_array.shape[2] == 3:\n",
        "        lab_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2LAB)\n",
        "        if verbose:\n",
        "            image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
        "            print('The image is in the BGR color space')\n",
        "            print(f\"image_array: {image_array.dtype}\")\n",
        "            cv2.imwrite(\"test_02_before_lab.jpg\", image_array)\n",
        "    else:\n",
        "        lab_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)\n",
        "        if verbose:\n",
        "            print('The image is in the BGR color space')\n",
        "            print(f\"image_array: {image_array.dtype}\")\n",
        "            cv2.imwrite(\"test_02_before_lab.jpg\", image_array)\n",
        "\n",
        "    if verbose:\n",
        "        cv2.imwrite(\"test_03_after_lab.jpg\", lab_image)\n",
        "\n",
        "    # Create the modifiers\n",
        "    if random_fst.value < fst.value:\n",
        "        l_modifier  = -.85\n",
        "        b_modifier = -.09\n",
        "        a_modifier = .06\n",
        "    else:\n",
        "        if difference > 25:\n",
        "            b_modifier = .05\n",
        "            l_modifier = -.85\n",
        "        else:\n",
        "            b_modifier = .05\n",
        "            l_modifier = -.75\n",
        "        a_modifier = -.15\n",
        "    if verbose: print(\n",
        "        f\"b_modifier: {b_modifier} random_fst.value {random_fst} {random_fst.value} fst.value {fst} {fst.value}\")\n",
        "\n",
        "    # Convert to int64's incase we have negative numbers due to the modifier\n",
        "    lab_image = safe_convert(lab_image, np.int64)\n",
        "    if mask is not None:\n",
        "        lab_image[:, :, 2][mask] += int(difference * b_modifier)\n",
        "        lab_image[:, :, 1][mask] += int(difference * a_modifier)\n",
        "        lab_image[:, :, 0][mask] += int(difference * l_modifier)\n",
        "    else:\n",
        "        lab_image[:, :, 2] += int(difference * b_modifier)\n",
        "        lab_image[:, :, 1] += int(difference * a_modifier)\n",
        "        lab_image[:, :, 0] += int(difference * l_modifier)\n",
        "\n",
        "    # convert back to uint8's as thats what cv2 expects\n",
        "    lab_image = safe_convert(lab_image, np.uint8)\n",
        "\n",
        "    # Convert the image back to the original color space\n",
        "    adjusted_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    if verbose:\n",
        "        # Save the LAB image to a file\n",
        "        cv2.imwrite(\"test.jpg\", adjusted_image)\n",
        "\n",
        "        image = Image.open(io.BytesIO(open(\"test.jpg\", \"rb\").read()))\n",
        "\n",
        "        image_ita = get_cropped_center_ita(image)\n",
        "        print(f\"Updated ITA {image_ita}\")\n",
        "\n",
        "    # To get a PIL image type you need to do this:\n",
        "    # Image.fromarray(util.img_as_ubyte(rgb))\n",
        "    # source: https://stackoverflow.com/a/55893334\n",
        "    return adjusted_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTxpQMN_Ttip"
      },
      "source": [
        "# Generate synthetic data\n",
        "\n",
        "The idea is to make the full dataset darker. The end target for all images is to be between 4,5,6. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "L1xcmqzMWfat"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. loop over all images\n",
        "2. generate the transformed image\n",
        "3. save to transformed folder\n",
        "\"\"\"\n",
        "\n",
        "def generate_synthetic_data():\n",
        "\n",
        "    transformed_directory = Path('./kaggle_2020/')\n",
        "    #transformed_directory.mkdir()\n",
        "    transformed_directory = Path('./kaggle_2020/transformed')\n",
        "    #transformed_directory.mkdir()\n",
        "\n",
        "\n",
        "\n",
        "    count = len(everything_df)\n",
        "    i = 0\n",
        "    for index, row in everything_df.iterrows():\n",
        "        #print(row)\n",
        "        image_path = row[\"path_jpg\"]\n",
        "        \n",
        "        image_name = row[\"dcm_name\"]\n",
        "        #if \"ISIC_0030672\" not in image_name:\n",
        "        #    continue   \n",
        "        image_ita = everything_df.loc[everything_df[\"dcm_name\"] == image_name,\"ita\"].values[0]\n",
        "        current_fst = row[\"fitzpatrick_skin_type\"]\n",
        "        if current_fst == 1:\n",
        "            desired_fst = Fitzpatrick_Skin_Type._3\n",
        "        elif current_fst == 2:\n",
        "            desired_fst = Fitzpatrick_Skin_Type._4\n",
        "        elif current_fst == 3:\n",
        "            desired_fst = Fitzpatrick_Skin_Type._5\n",
        "        elif current_fst == 4:\n",
        "            desired_fst = Fitzpatrick_Skin_Type._6\n",
        "        elif current_fst == 5:\n",
        "            desired_fst = Fitzpatrick_Skin_Type._6\n",
        "        elif current_fst == 6:\n",
        "            desired_fst = Fitzpatrick_Skin_Type._6\n",
        "            \n",
        "        \n",
        "\n",
        "        \n",
        "        org_image = Image.open(image_path)\n",
        "        org_image = org_image.resize((224, 224))\n",
        "\n",
        "        mask = Image.open(mask_path/str(image_name+\".png\"))\n",
        "        mask = mask.resize((224, 224))\n",
        "        mask = mask.rotate(270)\n",
        "        mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        transformed_image = transform_image(org_image, mask, image_ita=image_ita, desired_fst=desired_fst, verbose=False)\n",
        "        \n",
        "        cv2.imwrite(os.path.join(transformed_directory, image_name+\".jpg\"), transformed_image)\n",
        "        #print(\"----\")    \n",
        "        #print(transformed_image_pil)\n",
        "        print(f\"% complete {i / count * 100: .3f}. {image_name} : {image_ita: 4.5f} curent FST:{current_fst}. Desired: {desired_fst}\")\n",
        "        i += 1\n",
        "    print(\"Image transformation complete\")\n",
        "if generate_synthetic_dataset_flag:\n",
        "    generate_synthetic_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "GPW5ZPSzWqL4"
      },
      "outputs": [],
      "source": [
        "if generate_synthetic_dataset_flag:\n",
        "    !zip -r kaggle_2020_transformed_images__2023_02_17.zip ./kaggle_2020/transformed\n",
        "    %cp kaggle_2020_transformed_images__2023_02_17.zip /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9VCYiLD7EN"
      },
      "source": [
        "# 3. Neural Networks ðŸŽ‡\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF4SkLvEYu23"
      },
      "source": [
        "## 3.1 ResNet50 ðŸ’¾\n",
        "> [For more information about ResNet Architecture, check this link.](https://iq.opengenus.org/resnet50-architecture/#:~:text=ResNet50%20is%20a%20variant%20of,have%20explored%20this%20in%20depth.)\n",
        "\n",
        ">  `Batch Normalization`: [Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "FHtXdv7UD7EN"
      },
      "outputs": [],
      "source": [
        "class ResNet50Network(nn.Module):\n",
        "    def __init__(self, output_size, no_columns):\n",
        "        super().__init__()\n",
        "        self.no_columns, self.output_size = no_columns, output_size\n",
        "        \n",
        "        # Define Feature part (IMAGE)\n",
        "        self.features = resnet50(pretrained=True) # 1000 neurons out\n",
        "        # (CSV data)\n",
        "        self.csv = nn.Sequential(nn.Linear(self.no_columns, 500),\n",
        "                                 nn.BatchNorm1d(500),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(p=0.2))\n",
        "        \n",
        "        # Define Classification part\n",
        "        self.classification = nn.Linear(1000 + 500, output_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, image, csv_data, prints=False):\n",
        "        \n",
        "        if prints: print('Input Image shape:', image.shape, '\\n'+\n",
        "                         'Input csv_data shape:', csv_data.shape)\n",
        "        \n",
        "        # Image CNN\n",
        "        image = self.features(image)\n",
        "        if prints: print('Features Image shape:', image.shape)\n",
        "        \n",
        "        # CSV FNN\n",
        "        csv_data = self.csv(csv_data)\n",
        "        if prints: print('CSV Data:', csv_data.shape)\n",
        "            \n",
        "        # Concatenate layers from image with layers from csv_data\n",
        "        image_csv_data = torch.cat((image, csv_data), dim=1)\n",
        "        \n",
        "        # CLASSIF\n",
        "        out = self.classification(image_csv_data)\n",
        "        if prints: print('Out shape:', out.shape)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26RB0SuU0vyx"
      },
      "source": [
        "### ResNet50NetworkImageOnly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kwaedFgTWG5f"
      },
      "outputs": [],
      "source": [
        "class ResNet50NetworkImageOnly(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # Define Feature part (IMAGE)\n",
        "        self.features = resnet50(pretrained=True) # 1000 neurons out\n",
        "        \n",
        "        # Define Classification part\n",
        "        self.classification = nn.Linear(1000, output_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, image, prints=False):\n",
        "        \n",
        "        if prints: print('Input Image shape:', image.shape, '\\n'+\n",
        "                         'Input csv_data shape:', csv_data.shape)\n",
        "        \n",
        "        # Image CNN\n",
        "        image_features = self.features(image)\n",
        "        if prints: print('Features Image shape:', image.shape)\n",
        "                        \n",
        "        # CLASSIF\n",
        "        out = self.classification(image_features)\n",
        "        if prints: print('Out shape:', out.shape)\n",
        "        \n",
        "        return out, image_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeK-VVPdD7EO"
      },
      "source": [
        "### How is ResNet50() working?\n",
        "> A schema of the example below:\n",
        "<img src='https://i.imgur.com/0iDH8SI.png' width=600>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "jZzBgQpGD7EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96969623-b61e-406b-f3aa-bc0e4908ea3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 210MB/s]\n"
          ]
        }
      ],
      "source": [
        "model_example = ResNet50Network(output_size=output_size, no_columns=no_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "meDJV1T-D7EO"
      },
      "outputs": [],
      "source": [
        "# This section of code is broken due ot the dataset updates\n",
        "if False:\n",
        "    # Data object and Loader\n",
        "    example_data = MelanomaDataset(train_df, vertical_flip=0.5, horizontal_flip=0.5, \n",
        "                                is_train=True, is_valid=False, is_test=False)\n",
        "    example_loader = torch.utils.data.DataLoader(example_data, batch_size = 3, shuffle=True)\n",
        "\n",
        "    # Get a sample\n",
        "    for (image, csv_data), labels in example_loader:\n",
        "        image_example, csv_data_example = image, csv_data\n",
        "        labels_example = torch.tensor(labels, dtype=torch.float32)\n",
        "        break\n",
        "    print('Data shape:', image_example.shape, '| \\n' , csv_data_example)\n",
        "    print('Label:', labels_example, '\\n')\n",
        "\n",
        "    # Outputs\n",
        "    out = model_example(image_example, csv_data_example, prints=True)\n",
        "\n",
        "    # Criterion example\n",
        "    criterion_example = nn.BCEWithLogitsLoss()\n",
        "    # Unsqueeze(1) from shape=[3] to shape=[3, 1]\n",
        "    loss = criterion_example(out, labels_example.unsqueeze(1))   \n",
        "    print('Loss:', loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNiXoO9aD7EP"
      },
      "source": [
        "## 3.2 EfficientNet ðŸ’¾\n",
        "\n",
        "> There are multiple EffNet Architectures that can be used, but at the expense of computing more and more parameters.\n",
        "<img src='https://1.bp.blogspot.com/-oNSfIOzO8ko/XO3BtHnUx0I/AAAAAAAAEKk/rJ2tHovGkzsyZnCbwVad-Q3ZBnwQmCFsgCEwYBhgL/s1600/image3.png' width=350>\n",
        "\n",
        "\n",
        "*Note: B7 not working due to low memory*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "SI6ChY3nD7EP"
      },
      "outputs": [],
      "source": [
        "class EfficientNetwork(nn.Module):\n",
        "    def __init__(self, output_size, no_columns, b4=False, b2=False):\n",
        "        super().__init__()\n",
        "        self.b4, self.b2, self.no_columns = b4, b2, no_columns\n",
        "        \n",
        "        # Define Feature part (IMAGE)\n",
        "        if b4:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "        elif b2:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b2')\n",
        "        else:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "        \n",
        "        # (CSV)\n",
        "        self.csv = nn.Sequential(nn.Linear(self.no_columns, 250),\n",
        "                                 nn.BatchNorm1d(250),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(p=0.2),\n",
        "                                 \n",
        "                                 nn.Linear(250, 250),\n",
        "                                 nn.BatchNorm1d(250),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(p=0.2))\n",
        "        \n",
        "        # Define Classification part\n",
        "        if b4:\n",
        "            self.classification = nn.Sequential(nn.Linear(1792 + 250, output_size))\n",
        "        elif b2:\n",
        "            self.classification = nn.Sequential(nn.Linear(1408 + 250, output_size))\n",
        "        else:\n",
        "            self.classification = nn.Sequential(nn.Linear(2560 + 250, output_size))\n",
        "        \n",
        "        \n",
        "    def forward(self, image, csv_data, prints=False):    \n",
        "        \n",
        "        if prints: print('Input Image shape:', image.shape, '\\n'+\n",
        "                         'Input csv_data shape:', csv_data.shape)\n",
        "        \n",
        "        # IMAGE CNN\n",
        "        image = self.features.extract_features(image)\n",
        "        if prints: print('Features Image shape:', image.shape)\n",
        "            \n",
        "        if self.b4:\n",
        "            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1792)\n",
        "        elif self.b2:\n",
        "            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1408)\n",
        "        else:\n",
        "            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 2560)\n",
        "        if prints: print('Image Reshaped shape:', image.shape)\n",
        "            \n",
        "        # CSV FNN\n",
        "        csv_data = self.csv(csv_data)\n",
        "        if prints: print('CSV Data:', csv_data.shape)\n",
        "            \n",
        "        # Concatenate\n",
        "        image_csv_data = torch.cat((image, csv_data), dim=1)\n",
        "        \n",
        "        # CLASSIF\n",
        "        out = self.classification(image_csv_data)\n",
        "        if prints: print('Out shape:', out.shape)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_d0mGSOJ39P"
      },
      "source": [
        "### EfficientNetworkImageOnly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "o7k5lKHtwlBM"
      },
      "outputs": [],
      "source": [
        "class EfficientNetworkImageOnly(nn.Module):\n",
        "    def __init__(self, output_size, b4=False, b2=False):\n",
        "        super().__init__()\n",
        "        self.b4, self.b2 = b4, b2\n",
        "        \n",
        "        # Define Feature part (IMAGE)\n",
        "        if b4:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "        elif b2:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b2')\n",
        "        else:\n",
        "            self.features = EfficientNet.from_pretrained('efficientnet-b7')\n",
        "        \n",
        "        # Define Classification part\n",
        "        if b4:\n",
        "            self.classification = nn.Sequential(nn.Linear(1792, output_size))\n",
        "        elif b2:\n",
        "            self.classification = nn.Sequential(nn.Linear(1408, output_size))\n",
        "        else:\n",
        "            self.classification = nn.Sequential(nn.Linear(2560, output_size))\n",
        "        \n",
        "        \n",
        "    def forward(self, image, prints=False):    \n",
        "        \n",
        "        if prints: print('Input Image shape:', image.shape, '\\n'+\n",
        "                         'Input csv_data shape:', csv_data.shape)\n",
        "        \n",
        "        # IMAGE CNN\n",
        "        image_features = self.features.extract_features(image)\n",
        "        if prints: print('Features Image shape:', image.shape)\n",
        "            \n",
        "        if self.b4:\n",
        "            image_features = F.avg_pool2d(image_features, image_features.size()[2:]).reshape(-1, 1792)\n",
        "        elif self.b2:\n",
        "            image_features = F.avg_pool2d(image_features, image_features.size()[2:]).reshape(-1, 1408)\n",
        "        else:\n",
        "            image_features = F.avg_pool2d(image_features, image_features.size()[2:]).reshape(-1, 2560)\n",
        "        if prints: print('Image Reshaped shape:', image_features.shape)\n",
        "            \n",
        "        \n",
        "        # CLASSIF\n",
        "        out = self.classification(image_features)\n",
        "        if prints: print('Out shape:', out.shape)\n",
        "        \n",
        "        return out, image_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Ynmqxrh5Xkly"
      },
      "outputs": [],
      "source": [
        "class SingleOutputModel(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw-U1DXJD7EP"
      },
      "source": [
        "### How is EfficientNet working?\n",
        "\n",
        "> A schema of the below example:\n",
        "\n",
        "<img src='https://i.imgur.com/dLq9xIs.png' width=600>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fomv63QD7EP",
        "outputId": "41e6c7fa-9e6b-44cf-9e09-17802384c7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b2-8bb594d6.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.1M/35.1M [00:02<00:00, 17.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b2\n"
          ]
        }
      ],
      "source": [
        "# Create an example model - Effnet\n",
        "model_example = EfficientNetwork(output_size=output_size, no_columns=no_columns,\n",
        "                                 b4=False, b2=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "tXYWBVeFD7EP"
      },
      "outputs": [],
      "source": [
        "# this code is broken due to the dataset updates\n",
        "if False:\n",
        "    # Data object and Loader\n",
        "    example_data = MelanomaDataset(train_df, vertical_flip=0.5, horizontal_flip=0.5, \n",
        "                                is_train=True, is_valid=False, is_test=False)\n",
        "    example_loader = torch.utils.data.DataLoader(example_data, batch_size = 3, shuffle=True)\n",
        "\n",
        "    # Get a sample\n",
        "    for (image, csv_data), labels in example_loader:\n",
        "        image_example, csv_data_example = image, csv_data\n",
        "        labels_example = torch.tensor(labels, dtype=torch.float32)\n",
        "        break\n",
        "    print('Data shape:', image_example.shape, '| \\n' , csv_data_example)\n",
        "    print('Label:', labels_example, '\\n')\n",
        "\n",
        "    # Outputs\n",
        "    out = model_example(image_example, csv_data_example, prints=True)\n",
        "\n",
        "    print(\"out\", out)\n",
        "\n",
        "    print(\"lable unzqueeze\", labels_example.unsqueeze(1))\n",
        "    # Criterion example\n",
        "    criterion_example = nn.BCEWithLogitsLoss()\n",
        "    # Unsqueeze(1) from shape=[3] to shape=[3, 1]\n",
        "    loss = criterion_example(out, labels_example.unsqueeze(1))   \n",
        "    print('Loss:', loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dEfkFmRD7EQ"
      },
      "source": [
        "# 4. Training ... ðŸ’»\n",
        "\n",
        "### Prepare OOF and Predictions Matrixes\n",
        "> `OOF` will be used to assess the overall ROC value of the entire Train data (Train + Valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr1lud9KD7EQ",
        "outputId": "93a0b6eb-a2bc-4789-9ac9-0ef2103d32cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oof shape: (28233, 1) \n",
            "predictions shape: torch.Size([10982, 1])\n"
          ]
        }
      ],
      "source": [
        "# ----- STATICS -----\n",
        "train_len = len(train_df)\n",
        "test_len = len(test_df)\n",
        "# -------------------\n",
        "\n",
        "\n",
        "# Out of Fold Predictions\n",
        "oof = np.zeros(shape = (train_len, 1))\n",
        "\n",
        "# Predictions\n",
        "preds_submission = torch.zeros(size = (test_len, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "print('oof shape:', oof.shape, '\\n' +\n",
        "      'predictions shape:', preds_submission.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_G7eXoHD7EQ"
      },
      "source": [
        "### GroupKFold() ðŸ¦—ðŸ¦—ðŸ¦—\n",
        "> K-fold iterator variant with non-overlapping groups. The same group **will not appear** in two different folds (the number of distinct groups has to be at least equal to the number of folds).\n",
        "\n",
        "> We're using `patient_id` for our grouping column: there are multiple patients with *multiple images taken*, so we need to be careful with that.\n",
        "<img src='https://i.imgur.com/MfFdoMt.png' width=400>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "DHLdF-rED7EQ"
      },
      "outputs": [],
      "source": [
        "# ----- STATICS -----\n",
        "k = 6              # number of folds in Group K Fold\n",
        "# -------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "q0AKwCUED7EQ"
      },
      "outputs": [],
      "source": [
        "# Create Object\n",
        "group_fold = GroupKFold(n_splits = k)\n",
        "\n",
        "# Generate indices to split data into training and test set.\n",
        "folds = group_fold.split(X = np.zeros(train_len), \n",
        "                         y = train_df['target'], \n",
        "                         groups = train_df['ID'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFyHARFWD7EQ"
      },
      "source": [
        "## 4.1 Training Loop... ðŸ”‹\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "Before, I would like to thank again to Roman for the notebook <a href='https://www.kaggle.com/nroman/melanoma-pytorch-starter-efficientnet/output'>Melanoma. Pytorch starter. EfficientNet</a>. The following Training Loop is very much taken from him, but I FINALLY understood what is TTA, how to optimize the learning rate and how to use K Fold in Deep Learning. I am very happy with what I learned during this process, so I am ready to take it further and even maybe make it better?\n",
        "<p>Next step would be to also incorporate the data from the .csv file</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "* `ReduceLROnPlateau()`: Reduce learning rate when a metric has stopped improving. Here `patience` is set to 1, meaning that if 1 model doesn't improve, then the `lr` will decrease by a factor of 0.2.\n",
        "* `patience`: Early Stopping Patience (how many epochs to wait with no improvement until it stops)\n",
        "* `TTA`: Test Time Augmentation Rounds (creating multiple augmented copies of each image in the test set, having the model make a prediction for each, then returning an ensemble of those predictions)\n",
        "\n",
        "> The following chunk of code is quite big, so I made a schema of the overall steps we're taking within it:\n",
        "<img src='https://i.imgur.com/hoeuHs1.png' width=600>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "bq0rrgrRD7ER"
      },
      "outputs": [],
      "source": [
        "# ----- STATICS -----\n",
        "epochs = 15\n",
        "patience = 3\n",
        "TTA = 3\n",
        "num_workers = 2\n",
        "lr_patience = 1            # 1 model not improving until lr is decreasing\n",
        "lr_factor = 0.4            # by how much the lr is decreasing\n",
        "\n",
        "version = 'v04'             # to keep tabs on versions\n",
        "# -------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "3SoESWNqD7ER"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_folds(preds_submission, model, mask_path, version = 'v1', model_name=\"\"):\n",
        "    # Creates a .txt file that will contain the logs\n",
        "    f = open(f\"logs_{version}.txt\", \"w+\")\n",
        "\n",
        "    k = 6\n",
        "    # Create Object\n",
        "    group_fold = GroupKFold(n_splits = k)\n",
        "\n",
        "    # Generate indices to split data into training and test set.\n",
        "    folds = group_fold.split(X = np.zeros(train_len), \n",
        "                            y = train_df['target'], \n",
        "                            groups = train_df['ID'].tolist())\n",
        "    \n",
        "    print(\"Starting to train\")\n",
        "    for fold, (train_index, valid_index) in enumerate(folds):\n",
        "        # Append to .txt\n",
        "        with open(f\"logs_{version}.txt\", 'a+') as f:\n",
        "            print('-'*10, 'Fold:', fold+1, '-'*10, file=f)\n",
        "        print('-'*10, 'Fold:', fold+1, '-'*10)\n",
        "\n",
        "\n",
        "        # --- Create Instances ---\n",
        "        # Best ROC score in this fold\n",
        "        best_roc = None\n",
        "        # Reset patience before every fold\n",
        "        patience_f = patience\n",
        "        \n",
        "        # Initiate the model\n",
        "        model = model\n",
        "        #print(f\"weight_decay {weight_decay}. learning_rate {learning_rate}\")\n",
        "\n",
        "        if selected_optimizer == \"adam\":\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif selected_optimizer == \"adamax\":\n",
        "            optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif selected_optimizer == \"adamw\":\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif selected_optimizer == \"nadam\":\n",
        "            optimizer = torch.optim.Nadam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "        scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max', \n",
        "                                      patience=lr_patience, verbose=True, factor=lr_factor)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "        # --- Read in Data ---\n",
        "        train_data = train_df.iloc[train_index].reset_index(drop=True)\n",
        "        valid_data = train_df.iloc[valid_index].reset_index(drop=True)\n",
        "\n",
        "        # Create Data instances\n",
        "        train = MelanomaDataset(train_data, vertical_flip=vertical_flip, horizontal_flip=horizontal_flip, \n",
        "                                is_train=True, is_valid=False, is_test=False, generate_skin_transform=True, mask_path=mask_path,\n",
        "                                use_synthetic_data_percentage=use_synthetic_data_percentage)\n",
        "        valid = MelanomaDataset(valid_data, vertical_flip=vertical_flip, horizontal_flip=horizontal_flip, \n",
        "                                is_train=False, is_valid=True, is_test=False, generate_skin_transform=True, mask_path=mask_path, \n",
        "                                use_synthetic_data_percentage=use_synthetic_data_percentage)\n",
        "        # Read in test data | Remember! We're using data augmentation like we use for Train data.\n",
        "        #test = MelanomaDataset(test_df, vertical_flip=vertical_flip, horizontal_flip=horizontal_flip,\n",
        "        #                       is_train=False, is_valid=False, is_test=True, generate_skin_transform=True, mask_path=mask_path)\n",
        "\n",
        "        # Dataloaders\n",
        "        train_loader = DataLoader(train, batch_size=batch_size1, shuffle=True, num_workers=num_workers)\n",
        "        # shuffle=False! Otherwise function won't work!!!\n",
        "                # how do I know? ^^\n",
        "        valid_loader = DataLoader(valid, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n",
        "        #test_loader = DataLoader(test, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "\n",
        "        # === EPOCHS ===\n",
        "        for epoch in range(epochs):            \n",
        "            start_time = time.time()\n",
        "            correct = 0\n",
        "            train_losses = 0\n",
        "\n",
        "            # === TRAIN ===\n",
        "            # Sets the module in training mode.\n",
        "            model.train()\n",
        "\n",
        "            for (images, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask), labels in train_loader:\n",
        "                # Save them to device\n",
        "                images = torch.tensor(images, device=device, dtype=torch.float32)\n",
        "                #csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
        "                labels = torch.tensor(labels, device=device, dtype=torch.float32)\n",
        "\n",
        "                # Clear gradients first; very important, usually done BEFORE prediction\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Log Probabilities & Backpropagation\n",
        "                #out = model(images, csv_data)\n",
        "                out, image_features = model(images)\n",
        "                if use_regularization:           \n",
        "                    transformed_image = transformed_image.to(device)         \n",
        "                    out, image_features_from_transformed_image = model(transformed_image)\n",
        "                    regularization = regularization_alpha * F.mse_loss(image_features_from_transformed_image, image_features)\n",
        "                else:\n",
        "                    regularization = 0\n",
        "                loss = criterion(out, labels.unsqueeze(1))\n",
        "                (loss + regularization).backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # --- Save information after this batch ---\n",
        "                # Save loss\n",
        "                train_losses += loss.item()\n",
        "                # From log probabilities to actual probabilities\n",
        "                train_preds = torch.round(torch.sigmoid(out)) # 0 and 1\n",
        "                # Number of correct predictions\n",
        "                correct += (train_preds.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
        "\n",
        "            # Compute Train Accuracy\n",
        "            train_acc = correct / len(train_index)\n",
        "\n",
        "\n",
        "            # === EVAL ===\n",
        "            # Sets the model in evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Create matrix to store evaluation predictions (for accuracy)\n",
        "            valid_preds = torch.zeros(size = (len(valid_index), 1), device=device, dtype=torch.float32)\n",
        "\n",
        "\n",
        "            # Disables gradients (we need to be sure no optimization happens)\n",
        "            with torch.no_grad():\n",
        "                for k, ((images, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask), labels) in enumerate(valid_loader):\n",
        "                    images = torch.tensor(images, device=device, dtype=torch.float32)\n",
        "                    #csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
        "                    labels = torch.tensor(labels, device=device, dtype=torch.float32)\n",
        "\n",
        "                    #out = model(images, csv_data)\n",
        "                    out, image_features = model(images)\n",
        "                    pred = torch.sigmoid(out)\n",
        "                    valid_preds[k*images.shape[0] : k*images.shape[0] + images.shape[0]] = pred\n",
        "\n",
        "                # Compute accuracy\n",
        "                valid_acc = accuracy_score(valid_data['target'].values, \n",
        "                                           torch.round(valid_preds.cpu()))\n",
        "                # Compute ROC\n",
        "                valid_roc = roc_auc_score(valid_data['target'].values, \n",
        "                                          valid_preds.cpu())\n",
        "\n",
        "                # Compute time on Train + Eval\n",
        "                duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
        "\n",
        "\n",
        "                # PRINT INFO\n",
        "                # Append to .txt file\n",
        "                with open(f\"logs_{version}.txt\", 'a+') as f:\n",
        "                    print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'.\\\n",
        "                     format(duration, epoch+1, epochs, train_losses, train_acc, valid_acc, valid_roc), file=f)\n",
        "                # Print to console\n",
        "                print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'.\\\n",
        "                     format(duration, epoch+1, epochs, train_losses, train_acc, valid_acc, valid_roc))\n",
        "\n",
        "\n",
        "                # === SAVE MODEL ===\n",
        "\n",
        "                # Update scheduler (for learning_rate)\n",
        "                scheduler.step(valid_roc)\n",
        "\n",
        "                # Update best_roc\n",
        "                if not best_roc: # If best_roc = None\n",
        "                    best_roc = valid_roc\n",
        "                    torch.save(model.state_dict(),\n",
        "                               f\"{model_name}-Fold{fold+1}_Epoch{epoch+1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n",
        "                    continue\n",
        "\n",
        "                if valid_roc > best_roc:\n",
        "                    best_roc = valid_roc\n",
        "                    # Reset patience (because we have improvement)\n",
        "                    patience_f = patience\n",
        "                    torch.save(model.state_dict(),\n",
        "                               f\"{model_name}-Fold{fold+1}_Epoch{epoch+1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n",
        "                else:\n",
        "                    # Decrease patience (no improvement in ROC)\n",
        "                    patience_f = patience_f - 1\n",
        "                    if patience_f == 0:\n",
        "                        with open(f\"logs_{version}.txt\", 'a+') as f:\n",
        "                            print('Early stopping (no improvement since 3 models) | Best ROC: {}'.\\\n",
        "                                  format(best_roc), file=f)\n",
        "                        print('Early stopping (no improvement since 3 models) | Best ROC: {}'.\\\n",
        "                              format(best_roc))\n",
        "                        break\n",
        "\n",
        "\n",
        "        # Dont need to do the vaidation here with the\n",
        "        \"\"\"\n",
        "        # === INFERENCE ===\n",
        "        # Choose model with best_roc in this fold\n",
        "        best_model_path = './' + [file for file in os.listdir('./') if str(round(best_roc, 3)) in file and 'Fold'+str(fold+1) in file][0]\n",
        "        # Using best model from Epoch Train\n",
        "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        #model = EfficientNetwork(output_size = output_size, no_columns=no_columns,b4=False, b2=True).to(device)\n",
        "        model = EfficientNetworkImageOnly(output_size = output_size,b4=False, b2=True).to(device)                         \n",
        "        model.load_state_dict(torch.load(best_model_path))\n",
        "        # Set the model in evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # --- EVAL ---\n",
        "            # Predicting again on Validation data to get preds for OOF\n",
        "            valid_preds = torch.zeros(size = (len(valid_index), 1), device=device, dtype=torch.float32)\n",
        "\n",
        "            for k, ((images, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask), _) in enumerate(valid_loader):\n",
        "                images = torch.tensor(images, device=device, dtype=torch.float32)\n",
        "                #csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
        "\n",
        "                #out = model(images, csv_data)\n",
        "                out = model(images)\n",
        "                pred = torch.sigmoid(out)\n",
        "                valid_preds[k*images.shape[0] : k*images.shape[0] + images.shape[0]] = pred\n",
        "\n",
        "            # Save info to OOF\n",
        "            oof[valid_index] = valid_preds.cpu().numpy()\n",
        "\n",
        "\n",
        "            # --- TEST ---\n",
        "            # Now (Finally) prediction for our TEST data\n",
        "            for i in range(TTA):\n",
        "                for k, (images, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask) in enumerate(test_loader):\n",
        "                    images = torch.tensor(images, device=device, dtype=torch.float32)\n",
        "                    #csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
        "\n",
        "                    #out = model(images, csv_data)\n",
        "                    out = model(images)\n",
        "                    # Covert to probablities\n",
        "                    out = torch.sigmoid(out)\n",
        "\n",
        "                    # ADDS! the prediction to the matrix we already created\n",
        "                    preds_submission[k*images.shape[0] : k*images.shape[0] + images.shape[0]] += out\n",
        "\n",
        "\n",
        "            # Divide Predictions by TTA (to average the results during TTA)\n",
        "            preds_submission /= TTA\n",
        "        \"\"\"\n",
        "\n",
        "        # === CLEANING ===\n",
        "        # Clear memory\n",
        "        del train, valid, train_loader, valid_loader, images, labels\n",
        "        # Garbage collector\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpafbdc8D7ES"
      },
      "source": [
        "### #Training in progress... ðŸ›« Please do not disturb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"\"\n",
        "if use_synthetic_data_percentage > 0:\n",
        "    model_name = f\"Synthetic_{use_synthetic_data_percentage:.2f}_blend_\""
      ],
      "metadata": {
        "id": "VMO68-ejq6w3"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"batch_size1: {batch_size1}\")\n",
        "print(f\"batch_size2: {batch_size1}\")\n",
        "print(f\"Optimizer: {selected_optimizer}\")\n",
        "print(f\"learning_rate: {learning_rate}\")\n",
        "print(f\"weight_decay: {weight_decay}\")"
      ],
      "metadata": {
        "id": "RWgY_9qpyvGL"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "6Kza2wnfD7ES",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "f7f84e8a-c2d1-42fc-ab7e-374cf771cf02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b2\n",
            "Starting to train\n",
            "---------- Fold: 1 ----------\n",
            "weight_decay 0.0001. learning_rate 0.0005\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-62263905014d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# # ===== Uncomment and Train =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/melanoma_masks/results1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds_submission\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# # Save OOF values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-c252c29a7ebe>\u001b[0m in \u001b[0;36mtrain_folds\u001b[0;34m(preds_submission, model, mask_path, version, model_name)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mita_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_image_wihtout_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# Save them to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if model_type == \"b2\":\n",
        "    model_name += \"EfficientNet_B2\"\n",
        "    if use_regularization:\n",
        "        model_name += \"_reg_used\"\n",
        "\n",
        "    print(f\"model_name {model_name}\")\n",
        "    # --- EffNet B2 ---\n",
        "    #model = EfficientNetwork(output_size = output_size, no_columns=no_columns, b4=False, b2=True).to(device)\n",
        "\n",
        "    model = EfficientNetworkImageOnly(output_size = output_size,b4=False, b2=True).to(device)   \n",
        "\n",
        "    # # ===== Uncomment and Train =====\n",
        "    mask_path = Path(\"../input/melanoma_masks/results1\")\n",
        "    train_folds(preds_submission = preds_submission, model=model, mask_path=mask_path, version = version,model_name=model_name)\n",
        "\n",
        "    # # Save OOF values\n",
        "    #save_oof = pd.DataFrame(data = oof, columns=['oof'])\n",
        "    #save_oof.to_csv(f'oof_{version}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI_1FhVKD7ET"
      },
      "outputs": [],
      "source": [
        "# Print the logs during training\n",
        "#f = open(f'logs_{version}.txt', \"r\")\n",
        "#contents = f.read()\n",
        "#print(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZCA0FPjD7ET"
      },
      "outputs": [],
      "source": [
        "if model_type == \"b4\":\n",
        "    # --- EffNet B4 ---\n",
        "    model_name += \"EfficientNet_B4\"\n",
        "    if use_regularization:\n",
        "        model_name += \"_reg_used\"\n",
        "    print(f\"model_name {model_name}\")\n",
        "    model = EfficientNetworkImageOnly(output_size = output_size, b4=True, b2=False).to(device)   \n",
        "\n",
        "    # Uncomment and Train\n",
        "    train_folds(preds_submission = preds_submission, model=model, mask_path=mask_path, version = version,model_name=model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKfa5dT2Ebev"
      },
      "outputs": [],
      "source": [
        "if model_type == \"b7\":\n",
        "    # --- EffNet B7 ---\n",
        "\n",
        "    model_name += \"EfficientNet_B7\"\n",
        "    if use_regularization:\n",
        "        model_name += \"_reg_used\"\n",
        "    print(f\"model_name {model_name}\")\n",
        "    model = EfficientNetworkImageOnly(output_size = output_size, b4=False, b2=False).to(device)   \n",
        "\n",
        "    # Uncomment and Train\n",
        "    train_folds(preds_submission = preds_submission, model=model, mask_path=mask_path, version = version,model_name=model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHSOfHpZD7ET"
      },
      "outputs": [],
      "source": [
        "if model_type == \"resnet\":\n",
        "    model_name = \"ResNet50\"\n",
        "    if use_regularization:\n",
        "        model_name += \"_reg_used\"\n",
        "\n",
        "    print(f\"model_name {model_name}\")\n",
        "    # --- ResNet50 ---\n",
        "    # model = ResNet50Network(output_size=output_size, no_columns=no_columns).to(device)\n",
        "    model = ResNet50NetworkImageOnly(output_size=output_size).to(device)\n",
        "\n",
        "    # Uncomment and Train\n",
        "    train_folds(preds_submission = preds_submission, model=model, mask_path=mask_path, version = version, model_name=model_name)\n",
        "    # train_folds(preds_submission = preds_submission, model = model, version = version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBn1Eq2iD7EU"
      },
      "source": [
        "## 4.2 Making the Submission ðŸ“©\n",
        "\n",
        "### #1. ROC for OOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCmryicwD7EU"
      },
      "outputs": [],
      "source": [
        "# Import OOF (pretrained)\n",
        "#oof = pd.read_csv(f'./oof_{version}.csv')\n",
        "\n",
        "# ROC on full Training data\n",
        "#print('OOF ROC: {:.3f}'.format(roc_auc_score(train_df['target'], oof)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idk7Ze6RD7EU"
      },
      "source": [
        "### #2. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUc_EV3OD7EV"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # Make OOF Binary\n",
        "    oof.loc[oof.oof >= 0.5, 'oof'] = 1\n",
        "    oof.loc[oof.oof < 0.5, 'oof'] = 0\n",
        "\n",
        "    # Create Confusion Matrix\n",
        "    cf_matrix = confusion_matrix(train_df['target'], oof)\n",
        "\n",
        "    # Pretty CM:\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    # Format of the absolute numbers\n",
        "    group_counts = ['{:,}'.format(value) for value in cf_matrix.flatten()]\n",
        "    # Format for relative numbers\n",
        "    group_percentages = ['{0:.1%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    # --- The figure ---\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Oranges',xticklabels=['benign', 'malignant'], \n",
        "                yticklabels=['benign', 'malignant'], cbar=False)\n",
        "\n",
        "    matplotlib.rcParams.update({'font.size': 15})\n",
        "    plt.tick_params(axis='both', labelsize=15)\n",
        "    plt.title('Confusion Matrix: OOF Data', fontsize=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjiKK7czD7EV"
      },
      "source": [
        "### #3. Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANr8FTYtD7EV"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # Divide predictions by the number of folds\n",
        "    preds_submission /= k\n",
        "    preds_submission = preds_submission.cpu().numpy().reshape(-1,)\n",
        "\n",
        "    # Import submission file\n",
        "    ss = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
        "\n",
        "    ss['target'] = preds_submission\n",
        "    ss.to_csv(f'submission_{version}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tUzDF4MD7EV"
      },
      "source": [
        "## Bonus: Best Single Model Function â©\n",
        "> Function that predicts solely on a pretrained model (uses TTA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Holvjp2dD7EV"
      },
      "outputs": [],
      "source": [
        "def best_single_model(model, preds_submission, TTA=3):\n",
        "    '''Function that takes an input model (trained) and makes the prediction for submission.'''\n",
        "    \n",
        "    test = MelanomaDataset(test_df, vertical_flip=0.5, horizontal_flip=0.5,\n",
        "                           is_train=False, is_valid=False, is_test=True)\n",
        "    test_loader = DataLoader(test, batch_size=16, shuffle=False, num_workers=8)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(TTA):\n",
        "            for k, (images, csv_data) in enumerate(test_loader):\n",
        "                images = torch.tensor(images, device=device, dtype=torch.float32)\n",
        "                csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
        "\n",
        "                out = model(images, csv_data)\n",
        "                # Covert to probablities\n",
        "                out = torch.sigmoid(out)\n",
        "\n",
        "                # ADDS! the prediction to the matrix we already created\n",
        "                preds_submission[k*images.shape[0] : k*images.shape[0] + images.shape[0]] += out\n",
        "\n",
        "\n",
        "        # Divide Predictions by TTA (to average the results during TTA)\n",
        "        preds_submission /= TTA\n",
        "        \n",
        "    return preds_submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnMWyhByD7EW"
      },
      "outputs": [],
      "source": [
        "# path = '../input/siim-melanoma-prep-data/Fold6_Epoch2_ValidAcc_0.981_ROC_0.986.pth'\n",
        "# best_model = EfficientNetwork(output_size = output_size, no_columns=no_columns,\n",
        "#                          b4=False, b2=True).to(device)\n",
        "# best_model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
        "\n",
        "# # Submission Preds Vector\n",
        "# preds_submission = torch.zeros(size = (test_len, 1), dtype=torch.float32, device=device)\n",
        "# x = best_single_model(model=best_model, preds_submission=preds_submission)\n",
        "# preds_submission = preds_submission.cpu().numpy().reshape(-1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq3bos7fD7EW"
      },
      "outputs": [],
      "source": [
        "# # --- Submission ---\n",
        "\n",
        "# # Import submission file\n",
        "# ss = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n",
        "\n",
        "# ss['target'] = preds_submission\n",
        "# ss.to_csv(f'submission_v7.2_SingleModel.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display transformed image samples"
      ],
      "metadata": {
        "id": "E1_mRP1brahR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGDM4icSreCp"
      },
      "source": [
        "# Validate the Transformed images\n",
        "\n",
        "Test out using the OOF images, from a saved model, to see how well the transformed images perform compared to the non transformed images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqnwWDg-JOZn"
      },
      "source": [
        "## Set up model with trained weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr_eGJuUx8-j"
      },
      "outputs": [],
      "source": [
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/model_checkpoints/*.pth /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBx0_RLRJqg6"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    path = 'Fold3_Epoch14_ValidAcc_0.979_ROC_0.988.pth'\n",
        "    best_model = EfficientNetworkImageOnly(output_size = output_size, b4=False, b2=True).to(device)\n",
        "    best_model.load_state_dict(torch.load(path, map_location=torch.device(device)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrS1ls-oshEM"
      },
      "source": [
        "## Testing loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYrCdglM5Oau"
      },
      "outputs": [],
      "source": [
        "# ----- STATICS -----\n",
        "k = 6              # number of folds in Group K Fold\n",
        "\n",
        "batch_size1 = 32\n",
        "batch_size2 = 16\n",
        "vertical_flip = 0.5\n",
        "horizontal_flip = 0.5\n",
        "\n",
        "csv_columns = ['sex', 'age', 'anatomy']\n",
        "no_columns = 3\n",
        "\n",
        "version = 'v02' \n",
        "# -------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgyh9BxPsjkC"
      },
      "outputs": [],
      "source": [
        "def test_folds(model, df, mask_path, group_key, k, version = 'v1'):\n",
        "    # Creates a .txt file that will contain the logs\n",
        "    f = open(f\"logs_{version}.txt\", \"w+\")\n",
        "    print(\"Starting testing\")\n",
        "\n",
        "    df_len = len(df)\n",
        "    # Create Object    \n",
        "    # group_fold = GroupKFold(n_splits = k)\n",
        "\n",
        "    # Generate indices to split data into training and test set.\n",
        "    # folds = group_fold.split(X = np.zeros(df_len), \n",
        "    #                         y = df['target'], \n",
        "    #                         groups = df[group_key].tolist())\n",
        "\n",
        "    oof = np.zeros(shape = (df_len, 1))\n",
        "    oof_transformed = np.zeros(shape = (df_len, 1))\n",
        "    oof_transformed_without_mask = np.zeros(shape = (df_len, 1))\n",
        "        \n",
        "    print(\"1 - No image edits\")\n",
        "    print(\"2 - Transformed image using mask\")\n",
        "    print(\"3 - Transformed image without mask\")\n",
        "    print(f\"|{'Runtime':^9}|\\\n",
        "{'1 Acc':^7}|{'2 Acc':^7}|{'3 Acc':^7}|\\\n",
        "{'1 ROC':^7}|{'2 ROC':^7}|{'3 ROC':^7}|\\\n",
        "{'1 recall':^10}|{'2 recall':^10}|{'3 recall':^10}|\\\n",
        "{'1 specificity':^15}|{'2 specificity':^15}|{'3 specificity':^15}|\\\n",
        "{'1 precision':^13}|{'2 precision':^13}|{'3 precision':^13}|\\\n",
        "{'1 f1':^6}|{'2 f1':^6}|{'3 f1':^6}|\")\n",
        "\n",
        "    #for fold, (train_index, valid_index) in enumerate(folds):\n",
        "    # Append to .txt\n",
        "    # with open(f\"logs_{version}.txt\", 'a+') as f:\n",
        "    #     print('-'*10, 'Fold:', fold+1, '-'*10, file=f)\n",
        "    #print('-'*10, 'Fold:', fold+1, '-'*10)\n",
        "\n",
        "    # --- Read in Data ---\n",
        "    valid_data = df\n",
        "\n",
        "    # Create Data instances        \n",
        "    valid = MelanomaDataset(valid_data, vertical_flip=vertical_flip, horizontal_flip=horizontal_flip, \n",
        "                            is_train=False, is_valid=True, is_test=False, generate_skin_transform=True, mask_path=mask_path)\n",
        "\n",
        "\n",
        "    # Dataloaders\n",
        "    # shuffle=False! Otherwise function won't work!!!\n",
        "            # how do I know? ^^\n",
        "    valid_loader = DataLoader(valid, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "        \n",
        "    # Set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        # --- EVAL ---\n",
        "        # Predicting again on Validation data to get preds for OOF\n",
        "        valid_preds = torch.zeros(size = (df_len, 1), device=device, dtype=torch.float32)\n",
        "        valid_preds_transformed = torch.zeros(size = (df_len, 1), device=device, dtype=torch.float32)\n",
        "        valid_preds_transformed_wihtout_mask = torch.zeros(size = (df_len, 1), device=device, dtype=torch.float32)\n",
        "\n",
        "        for k, ((images, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask), _) in enumerate(valid_loader):\n",
        "            images = torch.tensor(images, device=device, dtype=torch.float32)                \n",
        "            transformed_image = torch.tensor(transformed_image, device=device, dtype=torch.float32)\n",
        "            transformed_image_wihtout_mask = torch.tensor(transformed_image_wihtout_mask, device=device, dtype=torch.float32)                \n",
        "\n",
        "            #out = model(images, csv_data)\n",
        "            out, _ = model(images)\n",
        "            pred = torch.sigmoid(out)\n",
        "            valid_preds[k*images.shape[0] : k*images.shape[0] + images.shape[0]] = pred\n",
        "\n",
        "            out_transformed, _ = model(transformed_image)\n",
        "            pred_transformed = torch.sigmoid(out_transformed)\n",
        "            valid_preds_transformed[k*transformed_image.shape[0] : k*transformed_image.shape[0] + transformed_image.shape[0]] = pred_transformed\n",
        "\n",
        "            out_transformed_without_mask, _ = model(transformed_image_wihtout_mask)\n",
        "            pred_transformed_wihtout_mask = torch.sigmoid(out_transformed_without_mask)\n",
        "            valid_preds_transformed_wihtout_mask[k*transformed_image_wihtout_mask.shape[0] : k*transformed_image_wihtout_mask.shape[0] + transformed_image_wihtout_mask.shape[0]] = pred_transformed_wihtout_mask\n",
        "                        \n",
        "        #print(\"Time to compute metrics\")\n",
        "        # Compute accuracy\n",
        "        #print(\"Computing accuracy\")            \n",
        "        valid_acc = accuracy_score(valid_data['target'].values, \n",
        "                                    torch.round(valid_preds.cpu()))\n",
        "        valid_trans_acc = accuracy_score(valid_data['target'].values, \n",
        "                                    torch.round(valid_preds_transformed.cpu()))\n",
        "        valid_trans_wihtout_mask_acc = accuracy_score(valid_data['target'].values, \n",
        "                                    torch.round(valid_preds_transformed_wihtout_mask.cpu()))\n",
        "        # Compute metrics\n",
        "        #print(\"Computing roc\")\n",
        "        valid_roc = roc_auc_score(valid_data['target'].values, valid_preds.cpu())\n",
        "        valid_trans_roc = roc_auc_score(valid_data['target'].values, valid_preds_transformed.cpu())\n",
        "        valid_trans_wihtout_mask_roc = roc_auc_score(valid_data['target'].values, valid_preds_transformed_wihtout_mask.cpu())\n",
        "        \n",
        "        #print(\"Computing confusion matrix\")            \n",
        "        valid_preds_binary = (valid_preds > 0.5).long().cpu().numpy().flatten()\n",
        "        valid_preds_trans_binary = (valid_preds_transformed > 0.5).long().cpu().numpy().flatten()\n",
        "        valid_preds_trans_without_mask_binary = (valid_preds_transformed_wihtout_mask > 0.5).long().cpu().numpy().flatten()\n",
        "        \n",
        "        cm = confusion_matrix(valid_data['target'].values, valid_preds_binary)\n",
        "        cm_trans = confusion_matrix(valid_data['target'].values, valid_preds_trans_binary)\n",
        "        cm_trans_without_mask = confusion_matrix(valid_data['target'].values, valid_preds_trans_without_mask_binary)\n",
        "\n",
        "        #print(\"Computing recall\")\n",
        "        recall = recall_score(valid_data['target'].values, valid_preds_binary)\n",
        "        recall_trans = recall_score(valid_data['target'].values, valid_preds_trans_binary)\n",
        "        recall_trans_without_mask = recall_score(valid_data['target'].values, valid_preds_trans_without_mask_binary)\n",
        "\n",
        "        #print(\"Computing specificity\")\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "\n",
        "        tn, fp, fn, tp = cm_trans.ravel()\n",
        "        specificity_trans = tn / (tn + fp)\n",
        "\n",
        "        tn, fp, fn, tp = cm_trans_without_mask.ravel()\n",
        "        specificity_trans_wihtout_mask = tn / (tn + fp)\n",
        "\n",
        "        #print(\"Computing precision\")\n",
        "        precision = precision_score(valid_data['target'].values, valid_preds_binary)\n",
        "        precision_trans = precision_score(valid_data['target'].values, valid_preds_trans_binary)\n",
        "        precision_trans_without_mask = precision_score(valid_data['target'].values, valid_preds_trans_without_mask_binary)\n",
        "\n",
        "        # Compute F1 Score\n",
        "        #print(\"Computing f1\")\n",
        "        f1 = f1_score(valid_data['target'].values, valid_preds_binary)\n",
        "        f1_trans = f1_score(valid_data['target'].values, valid_preds_trans_binary)\n",
        "        f1_trans_without_mask = f1_score(valid_data['target'].values, valid_preds_trans_without_mask_binary)\n",
        "\n",
        "\n",
        "        # Compute time on Train + Eval\n",
        "        duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
        "        \n",
        "\n",
        "        # PRINT INFO\n",
        "        # Append to .txt file\n",
        "        with open(f\"logs_{version}.txt\", 'a+') as f:  \n",
        "            print(f\"|{duration:^9}|\\\n",
        "{valid_acc:^7.3f}|{valid_trans_acc:^7.3f}|{valid_trans_wihtout_mask_acc:^7.3f}|\\\n",
        "{valid_roc:^7.3f}|{valid_trans_roc:^7.3f}|{valid_trans_wihtout_mask_roc:^7.3f}|\\\n",
        "{recall:^10.3f}|{recall_trans:^10.3f}|{recall_trans_without_mask:^10.3f}|\\\n",
        "{specificity:^15.3f}|{specificity_trans:^15.3f}|{specificity_trans_wihtout_mask:^15.3f}|\\\n",
        "{precision:^13.3f}|{precision_trans:^13.3f}|{precision_trans_without_mask:^13.3f}|\\\n",
        "{f1:^6.3f}|{f1_trans:^6.3f}|{f1_trans_without_mask:^6.3f}|\", file=f)\n",
        "            #print(f'{duration} | Valid Acc: {valid_acc:.3} | ROC: {valid_roc:.3} | Transform Valid Acc: {valid_trans_acc:.3} | Transform ROC: {valid_trans_roc:.3} | Transform w/o mask Valid Acc: {valid_trans_wihtout_mask_acc:.3} | Transform w/o mask  ROC: {valid_trans_wihtout_mask_roc:.3}', file=f)\n",
        "        # Print to console                                    \n",
        "        print(f\"|{duration:^9}|\\\n",
        "{valid_acc:^7.3f}|{valid_trans_acc:^7.3f}|{valid_trans_wihtout_mask_acc:^7.3f}|\\\n",
        "{valid_roc:^7.3f}|{valid_trans_roc:^7.3f}|{valid_trans_wihtout_mask_roc:^7.3f}|\\\n",
        "{recall:^10.3f}|{recall_trans:^10.3f}|{recall_trans_without_mask:^10.3f}|\\\n",
        "{specificity:^15.3f}|{specificity_trans:^15.3f}|{specificity_trans_wihtout_mask:^15.3f}|\\\n",
        "{precision:^13.3f}|{precision_trans:^13.3f}|{precision_trans_without_mask:^13.3f}|\\\n",
        "{f1:^6.3f}|{f1_trans:^6.3f}|{f1_trans_without_mask:^6.3f}|\")\n",
        "        #print(f'{duration} | Valid Acc: {valid_acc:.3} | ROC: {valid_roc:.3} | Transform Valid Acc: {valid_trans_acc:.3} | Transform ROC: {valid_trans_roc:.3} | Transform w/o mask Valid Acc: {valid_trans_wihtout_mask_acc:.3} | Transform w/o mask  ROC: {valid_trans_wihtout_mask_roc:.3}')\n",
        "\n",
        "\n",
        "        # Save info to OOF\n",
        "        \n",
        "        oof = valid_preds.cpu().numpy()            \n",
        "        oof_transformed = valid_preds_transformed.cpu().numpy()\n",
        "        oof_transformed_without_mask = valid_preds_transformed_wihtout_mask.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # === CLEANING ===\n",
        "        # Clear memory\n",
        "        del valid, valid_loader, images\n",
        "        # Garbage collector\n",
        "        gc.collect()\n",
        "    return oof, oof_transformed, oof_transformed_without_mask "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCheFGjj3iUa"
      },
      "source": [
        "### Test transformed images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyYXzTGM2m4L"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    mask_path = Path(\"../input/melanoma_masks/results1\")\n",
        "    group_key = \"ID\"\n",
        "    k = 6\n",
        "    oof, oof_transformed, oof_transformed_without_mask  = test_folds(best_model, valid_df, mask_path, group_key, k, \"v00\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68pGNU7_YMb1"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    mask_path = Path(\"../input/isic_2016_masks/results2\")\n",
        "    group_key = \"fitzpatrick_skin_type\"\n",
        "    k = 6\n",
        "    oof_2016, oof_transformed_2016, oof_transformed_without_mask_2016  = test_folds(best_model, test_isic_2016_df, mask_path, group_key, k, \"v00\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvo9DFGbuJ_y"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcMma8SGuNpI"
      },
      "outputs": [],
      "source": [
        "#fitzpatrick_skin_type\n",
        "def create_confusion_matrix(oof_testing, title):\n",
        "\n",
        "    # Make OOF Binary\n",
        "    oof_testing_copy = oof_testing\n",
        "    oof_testing_copy.loc[oof_testing_copy.oof >= 0.45, 'oof'] = 1\n",
        "    oof_testing_copy.loc[oof_testing_copy.oof < 0.45, 'oof'] = 0\n",
        "\n",
        "    # Create Confusion Matrix\n",
        "    cf_matrix = confusion_matrix(train_df['target'], oof_testing_copy)\n",
        "\n",
        "    # Pretty CM:\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    # Format of the absolute numbers\n",
        "    group_counts = ['{:,}'.format(value) for value in cf_matrix.flatten()]\n",
        "    # Format for relative numbers\n",
        "    group_percentages = ['{0:.1%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    # --- The figure ---\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Oranges',xticklabels=['benign', 'malignant'], \n",
        "                yticklabels=['benign', 'malignant'], cbar=False)\n",
        "\n",
        "    matplotlib.rcParams.update({'font.size': 15})\n",
        "    plt.tick_params(axis='both', labelsize=15)\n",
        "    plt.title(title, fontsize=20);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAd8sz8buAHF"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "        \n",
        "    y_pred_binary = np.where(y_pred >= 0.45, 1, 0)\n",
        "    acc = accuracy_score(y_true,y_pred_binary)\n",
        "\n",
        "    roc = roc_auc_score(y_true, y_pred)     \n",
        "\n",
        "    #oof_testing_copy.loc[oof_testing_copy.oof >= 0.5, 'oof'] = 1\n",
        "    #oof_testing_copy.loc[oof_testing_copy.oof < 0.5, 'oof'] = 0       \n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)    \n",
        "    recall = recall_score(y_true, y_pred_binary)\n",
        "    \n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp)    \n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "\n",
        "    return roc, acc, cm, recall, specificity, precision, f1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J-XZgnnYzZk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def create_confusion_matrix_agg(oof_testing, df, title):\n",
        "    \n",
        "    # Aggregate the results based on the fitzpatrick_skin_type column\n",
        "    unique_skin_types = df['fitzpatrick_skin_type'].unique()\n",
        "    unique_skin_types.sort()\n",
        "    cf_matrices = []\n",
        "    print(f\"|{f'FKT':^9}|{'Accuracy':9}|{'ROC':^7}|{'Sensitivity':^10}|{'specificity':^15}|{'precision':^13}|{'f1':^6}|\")\n",
        "    ROC_array = []\n",
        "    for skin_type in unique_skin_types:\n",
        "        idx = df[df['fitzpatrick_skin_type'] == skin_type].index\n",
        "        y_true = df.loc[idx, 'target']\n",
        "        y_pred = oof_testing.loc[idx, 'oof']\n",
        "        roc, acc, cm, recall, specificity, precision, f1 = compute_metrics(y_true, y_pred)\n",
        "        ROC_array.append(roc)\n",
        "        print(f\"|{f'FKT({skin_type})':^9}|{acc:^7.3f}|{roc:^7.3f}|{recall:^10.3f}|{specificity:^15.3f}|{precision:^13.3f}{f1:^6.3f}|\")\n",
        "        cf_matrices.append(cm)\n",
        "\n",
        "    y_true = df['target']\n",
        "    y_pred = oof_testing['oof']\n",
        "    roc_overall, acc, cm, recall, specificity, precision, f1 = compute_metrics(y_true, y_pred)\n",
        "    print(f\"|{f'Overall':^9}|{acc:^9.3f}|{roc_overall:^7.3f}|{recall:^10.3f}|{specificity:^15.3f}|{precision:^13.3f}|{f1:^6.3f}|\")\n",
        "    print(\"ROC_array\", ROC_array)\n",
        "\n",
        "    # Plot Confusion Matrix for each skin type\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    print(cf_matrices)\n",
        "        \n",
        "    # Format for relative numbers\n",
        "    group_counts = []\n",
        "    group_percentages = []\n",
        "    labels = []\n",
        "    for matrix in cf_matrices:\n",
        "        table_label = []\n",
        "        total = np.sum(matrix)\n",
        "        tn, fp, fn, tp = matrix.ravel()\n",
        "        #print(total)\n",
        "        group_counts.append(f\"{total:,}\")\n",
        "        group_prec = []\n",
        "        #print(matrix)\n",
        "        for i, item in enumerate(matrix.flatten()):\n",
        "            percentage = item / total\n",
        "            table_label.append(f\"{group_names[i]}\\n{item}\\n{percentage*100:.3f}%\")\n",
        "            #print(item)\n",
        "            \n",
        "            #print(percentage)\n",
        "            group_prec.append(f\"{percentage:.3f}\")\n",
        "        labels.append(table_label)\n",
        "        group_percentages.append(group_prec)\n",
        "\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(10, 7))\n",
        "    axs = axs.ravel()\n",
        "    for i, cf_matrix in enumerate(cf_matrices):\n",
        "        #plt.figure(figsize=(3, 3))\n",
        "        label = labels[i]\n",
        "        #print(\"label\", label)\n",
        "        label = np.asarray(label).reshape(2,2)\n",
        "        #print(\"label\", label)\n",
        "\n",
        "        sns.heatmap(cf_matrix, annot=label, fmt='', cmap='Oranges',xticklabels=['benign', 'malignant'], \n",
        "                    yticklabels=['benign', 'malignant'], ax=axs[i], cbar=False)\n",
        "\n",
        "        matplotlib.rcParams.update({'font.size': 15})\n",
        "        axs[i].tick_params(axis='both', labelsize=15)\n",
        "        axs[i].set_title(f\"{title} ({unique_skin_types[i]})\", fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return roc_overall, ROC_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXuMFfDb48Eu"
      },
      "source": [
        "## Fairness metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8EAy9yb5Hmv"
      },
      "outputs": [],
      "source": [
        "def compute_fairness_metrics(all_testing_results, df):\n",
        "    unique_skin_types = df['fitzpatrick_skin_type'].unique()\n",
        "    unique_skin_types.sort()\n",
        "    cf_matrices = []\n",
        "    #Comparing skin types 1-3 vs 4-6\n",
        "    light_skin = [1,2,3]\n",
        "    darker_skin = [4,5,6]\n",
        "    group1_idx = df[df['fitzpatrick_skin_type'].isin(light_skin)].index\n",
        "\n",
        "    group2_idx = df[df['fitzpatrick_skin_type'].isin(darker_skin)].index\n",
        "    \n",
        "    y_true_group1 = df.loc[group1_idx, 'target']\n",
        "    y_pred_group1 = all_testing_results.loc[group1_idx, 'oof']\n",
        "    roc_light, acc, cm_light, recall, specificity, precision, f1 = compute_metrics(y_true_group1, y_pred_group1)\n",
        "    tn_A, fp_A, fn_A, tp_A = cm_light.ravel()\n",
        "    fpr_A = fp_A / (fp_A + tn_A)\n",
        "    fnr_A = fn_A / (tp_A + fn_A)\n",
        "    positive_outcomes_A = tp_A + fp_A\n",
        "\n",
        "    y_true_group2 = df.loc[group2_idx, 'target']\n",
        "    y_pred_group2 = all_testing_results.loc[group2_idx, 'oof']\n",
        "    roc_dark, acc, cm_dark, recall, specificity, precision, f1 = compute_metrics(y_true_group2, y_pred_group2)\n",
        "    tn_B, fp_B, fn_B, tp_B = cm_dark.ravel()\n",
        "    fpr_B = fp_B / (fp_B + tn_B)\n",
        "    fnr_B = fn_B / (tp_B + fn_B)\n",
        "    positive_outcomes_B = tp_B + fp_B\n",
        "\n",
        "    disparate_impact = positive_outcomes_A / positive_outcomes_B\n",
        "\n",
        "    statistical_parity = abs(fpr_A - fpr_B) + abs(fnr_A - fnr_B)\n",
        "    equalized_odds = abs(fpr_A - fpr_B) + abs(fnr_A - fnr_B)\n",
        "    \n",
        "    # Equal opportunity\n",
        "    tpr_A = tp_A / (tp_A + fn_A)\n",
        "    tpr_B = tp_B / (tp_B + fn_B)\n",
        "    equal_opportunity = abs(tpr_A - tpr_B)\n",
        "    \n",
        "    # Predictive rate parity\n",
        "    ppv_A = tp_A / (tp_A + fp_A)\n",
        "    ppv_B = tp_B / (tp_B + fp_B)\n",
        "    predictive_rate_parity = abs(ppv_A - ppv_B)\n",
        "\n",
        "    # Absolute Between-ROC Area \n",
        "    # Compute the ROC AUC for both classifiers\n",
        "    roc_auc1 = roc_auc_score(y_true_group1, y_pred_group1)\n",
        "    roc_auc2 = roc_auc_score(y_true_group2, y_pred_group2)\n",
        "    abra = np.abs(roc_auc1 - roc_auc2)\n",
        "\n",
        "\n",
        "    return disparate_impact, statistical_parity, equalized_odds, predictive_rate_parity, abra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Eif3gMX78bw"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9TdBp8crcT8"
      },
      "source": [
        "## Compute ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gUMDip4rtga"
      },
      "outputs": [],
      "source": [
        "def display_roc(labels, predictions, title):\n",
        "    # Compute the ROC curve and AUC\n",
        "    print(\"prediction count \", predictions)\n",
        "\n",
        "  \n",
        "    # Plot the distribution of the selected column\n",
        "    n, bins, patches = plt.hist(predictions, bins=30, color='blue', alpha=0.7)\n",
        "\n",
        "    # Add labels and titles to your plot\n",
        "    plt.xlabel('Fitzpatrick Skin Type Values')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of predictions')\n",
        "\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
        "    roc_auc = roc_auc_score(labels, predictions)\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # random classifier\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{title}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjecUQxQ0eJR"
      },
      "source": [
        "\n",
        "## ---Test all models---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5EU3Lhgf2j_"
      },
      "source": [
        "### Distribution betwen FST measurements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcLUjehY00Vr"
      },
      "outputs": [],
      "source": [
        "# Select the column you want to plot\n",
        "column = valid_df['fitzpatrick_skin_type']\n",
        "\n",
        "# Plot the distribution of the selected column\n",
        "n, bins, patches = plt.hist(column, bins=30, color='blue', alpha=0.7)\n",
        "\n",
        "# Add labels and titles to your plot\n",
        "plt.xlabel('Fitzpatrick Skin Type Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Fitzpatrick Skin Type')\n",
        "\n",
        "# Add the counts on top of the bars\n",
        "for i, patch in enumerate(patches):\n",
        "    if n[i] != 0:\n",
        "        plt.text(patch.get_x() + patch.get_width()/2,\n",
        "             patch.get_height() + 70,\n",
        "             int(n[i]),\n",
        "             ha='center', color='black')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRUFTYBubBR5"
      },
      "outputs": [],
      "source": [
        "# Select the column you want to plot\n",
        "column = valid_df['fitzpatrick_skin_type_masked']\n",
        "\n",
        "# Plot the distribution of the selected column\n",
        "n, bins, patches = plt.hist(column, bins=30, color='blue', alpha=0.7)\n",
        "\n",
        "# Add labels and titles to your plot\n",
        "plt.xlabel('Fitzpatrick Skin Type Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Fitzpatrick Skin Type')\n",
        "\n",
        "# Add the counts on top of the bars\n",
        "for i, patch in enumerate(patches):\n",
        "    if n[i] != 0:\n",
        "        plt.text(patch.get_x() + patch.get_width()/2,\n",
        "             patch.get_height() + 70,\n",
        "             int(n[i]),\n",
        "             ha='center', color='black')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAfYRDEkmaqH"
      },
      "outputs": [],
      "source": [
        "\n",
        "counts = valid_df[['fitzpatrick_skin_type', 'fitzpatrick_skin_type_masked']].rename(columns={'fitzpatrick_skin_type': 'Fitzpatrick Skin Type', 'fitzpatrick_skin_type_masked': 'Fitzpatrick Skin Type Masked'}).apply(pd.Series.value_counts)\n",
        "counts = counts.sort_index()\n",
        "bar_order = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
        "\n",
        "# Plot the bar chart\n",
        "ax = counts.plot.bar(rot=0,fontsize=\"small\", figsize=(8,6))\n",
        "for i, p in enumerate(ax.patches):\n",
        "    #print(p.get_x(), p.get_height(), i)\n",
        "    x_offset = -.12\n",
        "    if i > 5:\n",
        "        x_offset  += .13\n",
        "    #print(p.get_x(), p.get_height(), i, x_offset)\n",
        "    ax.annotate(str(p.get_height()), (p.get_x() * 1.005 + x_offset, p.get_height() + 90),fontsize=\"small\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLvN3jYPgE1-"
      },
      "source": [
        "### Samples "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_min_count_df.head()"
      ],
      "metadata": {
        "id": "FpUIyL7lkfn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hand_picked_images = [[\"ISIC_0057406\", \"ISIC_8454818\", \"ISIC_1255336\", \"ISIC_4099058\", \"ISIC_8705729\", \"ISIC_0070805\"], # good\n",
        "                      [\"ISIC_3824206\", \"ISIC_0067758\", \"ISIC_2720971\", \"ISIC_6577551\", \"ISIC_6682360\", \"ISIC_0068048\"], # good\n",
        "                      [\"ISIC_3380009\", \"ISIC_8686847\", \"ISIC_2366196\", \"ISIC_7750443\", \"ISIC_4752273\", \"ISIC_9665763\"]] # not good "
      ],
      "metadata": {
        "id": "WKzOZK3rgP31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribution = valid_min_count_df.groupby('fitzpatrick_skin_type_masked')[\"target\"].value_counts()\n",
        "distribution.plot(kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "45iCnEM8tkVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI-x7uVQwXSM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "sample_count = 3\n",
        "#23\n",
        "np.random.seed(23) # used to get reproducable. sample images\n",
        "# Sample 12 rows of each category within the Fitzpatrick Skin Type column\n",
        "sampled_valid_min_count_df = valid_min_count_df.groupby('fitzpatrick_skin_type_masked').apply(lambda x: x.sample(sample_count))\n",
        "\n",
        "# Initialize the figure\n",
        "fig, axs = plt.subplots(sample_count, 6, figsize=(10, sample_count * 2))\n",
        "\n",
        "row_idx = 0 \n",
        "col_idx = 0\n",
        "# Loop through the sampled DataFrame and display the images\n",
        "for i, row in sampled_valid_min_count_df.iterrows():\n",
        "    if row_idx < 3:\n",
        "        hand_picked_id = hand_picked_images[row_idx][col_idx]\n",
        "        if len(hand_picked_id) > 1:\n",
        "            hand_picked = valid_min_count_df[valid_min_count_df[\"dcm_name\"] == hand_picked_id]\n",
        "            #print(hand_picked)\n",
        "            hand_picked_path = hand_picked['path_jpg'].values[0]\n",
        "            print(hand_picked['target'],\"picked\", hand_picked['target'].values[0])            \n",
        "            target = hand_picked['target'].values[0]\n",
        "        else:\n",
        "            hand_picked_path = \"\"\n",
        "            target = row['target']\n",
        "\n",
        "        #print(\"hand_picked_path\", hand_picked_path)\n",
        "    else:\n",
        "        hand_picked_path = \"\"\n",
        "        target = row['target']\n",
        "    \n",
        "    if len(hand_picked_path) > 0:\n",
        "        img_path = hand_picked_path\n",
        "    else:\n",
        "        img_path = row['path_jpg']\n",
        "    #print(\"row['path_jpg']\", row['path_jpg'])\n",
        "\n",
        "\n",
        "\n",
        "    print(i, row[\"dcm_name\"], row_idx, col_idx, hand_picked_id)\n",
        "    img = mpimg.imread(img_path)\n",
        "    \n",
        "    axs[row_idx, col_idx].imshow(img)\n",
        "    if row_idx == 0:\n",
        "        axs[row_idx, col_idx].set_title(row['fitzpatrick_skin_type_masked'])\n",
        "    #else:\n",
        "    #    axs[row_idx, col_idx].set_title(f\"[{target}]{row_idx}\" )\n",
        "    axs[row_idx, col_idx].axis('off')\n",
        "    \n",
        "    row_idx += 1   # Compute the column index\n",
        "    if row_idx == sample_count:\n",
        "        row_idx = 0  # Compute the row index\n",
        "        col_idx += 1\n",
        "    \n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample showing masks and transformed images\n"
      ],
      "metadata": {
        "id": "24jj2B_WrwwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "sample_count = 3\n",
        "#23\n",
        "np.random.seed(23) # used to get reproducable. sample images\n",
        "# Sample 12 rows of each category within the Fitzpatrick Skin Type column\n",
        "sampled_valid_min_count_df = valid_min_count_df.groupby('fitzpatrick_skin_type_masked').apply(lambda x: x.sample(sample_count))\n",
        "\n",
        "# Initialize the figure\n",
        "fig, axs = plt.subplots(sample_count, 6, figsize=(10, sample_count * 2))\n",
        "\n",
        "row_idx = 0 \n",
        "col_idx = 0\n",
        "# Loop through the sampled DataFrame and display the images\n",
        "for i, row in sampled_valid_min_count_df.iterrows():\n",
        "    if row_idx < 3:\n",
        "        hand_picked_id = hand_picked_images[1][col_idx]\n",
        "        if len(hand_picked_id) > 1:\n",
        "            hand_picked = valid_min_count_df[valid_min_count_df[\"dcm_name\"] == hand_picked_id]\n",
        "            #print(hand_picked)\n",
        "            hand_picked_path = hand_picked['path_jpg'].values[0]\n",
        "            print(hand_picked['target'],\"picked\", hand_picked['target'].values[0], hand_picked_path)            \n",
        "            target = hand_picked['target'].values[0]\n",
        "        else:\n",
        "            hand_picked_path = \"\"\n",
        "            target = row['target']\n",
        "\n",
        "        #print(\"hand_picked_path\", hand_picked_path)\n",
        "    else:\n",
        "        hand_picked_path = \"\"\n",
        "        target = row['target']\n",
        "    \n",
        "    if len(hand_picked_path) > 0:\n",
        "        img_path = hand_picked_path\n",
        "        #2nd row is going to be the transformed image\n",
        "        \n",
        "        if row_idx == 2:\n",
        "            file_name = Path(hand_picked_path).name \n",
        "            org_image = Image.open(hand_picked_path)\n",
        "            org_image = org_image.resize((224, 224))\n",
        "\n",
        "            updated_mask_path = mask_path / str(file_name).replace(\"jpg\", \"png\") \n",
        "            mask = Image.open(updated_mask_path)\n",
        "            mask = mask.resize((224, 224))\n",
        "            mask = mask.rotate(270)\n",
        "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "            image_ita = everything_df.loc[everything_df[\"dcm_name\"] == hand_picked_id,\"ita\"].values[0]\n",
        "            current_fst = row['fitzpatrick_skin_type_masked']\n",
        "            if current_fst == 1:\n",
        "                desired_fst = Fitzpatrick_Skin_Type._4\n",
        "            elif current_fst == 2:\n",
        "                desired_fst = Fitzpatrick_Skin_Type._5\n",
        "            elif current_fst == 3:\n",
        "                desired_fst = Fitzpatrick_Skin_Type._6\n",
        "            elif current_fst == 4:\n",
        "                desired_fst = Fitzpatrick_Skin_Type._6\n",
        "            elif current_fst == 5:\n",
        "                desired_fst = Fitzpatrick_Skin_Type._2\n",
        "            elif current_fst == 6:\n",
        "                desired_fst = Fitzpatrick_Skin_Type._4\n",
        "\n",
        "\n",
        "            transformed_image = transform_image(org_image, mask, image_ita=image_ita,desired_fst=desired_fst, verbose=True)\n",
        "            \n",
        "            cv2.imwrite(os.path.join(\"test.jpg\"), transformed_image)            \n",
        "            img_path = \"test.jpg\"\n",
        "        if row_idx == 1:\n",
        "            # Get the mask and resize it to model input size\n",
        "            file_name = Path(hand_picked_path).name            \n",
        "            updated_mask_path = mask_path / str(file_name).replace(\"jpg\", \"png\")                      \n",
        "            im = Image.open(str(updated_mask_path))            \n",
        "            mask = im.resize((224, 224))\n",
        "            # Due to how the mask was created, it needs to be rotated and flipped\n",
        "            mask = mask.rotate(270)\n",
        "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            #mask = ImageOps.invert(mask)\n",
        "            mask = mask.convert(\"1\")\n",
        "\n",
        "            #image = Image.open(str(hand_picked_path))\n",
        "            org_image = Image.open(hand_picked_path)\n",
        "            org_image = org_image.resize((224, 224))\n",
        "            image_array = np.array(org_image)\n",
        "\n",
        "                          \n",
        "            #image_array = np.array(image_rgb)\n",
        "            if image_array.shape[2] == 3:\n",
        "                lab_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2LAB)\n",
        "            else:\n",
        "                lab_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2LAB)\n",
        "            lab_image = safe_convert(lab_image, np.int64)\n",
        "            \n",
        "            image_array[:, :, 2][mask] = 0\n",
        "            image_array[:, :, 1][mask] = 0\n",
        "            image_array[:, :, 0][mask] = 0\n",
        "\n",
        "\n",
        "            # convert back to uint8's as thats what cv2 expects\n",
        "            lab_image = safe_convert(lab_image, np.uint8)\n",
        "\n",
        "            # Convert the image back to the original color space\n",
        "            adjusted_image = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            cv2.imwrite(\"test.jpg\", adjusted_image)\n",
        "            img_path = \"test.jpg\"\n",
        "            \n",
        "\n",
        "    else:\n",
        "        img_path = row['path_jpg']\n",
        "    #print(\"row['path_jpg']\", row['path_jpg'])\n",
        "\n",
        "    #image_ita = get_cropped_center_ita(image)\n",
        "    #if verbose: print(f\"ITA {image_ita}\")\n",
        "\n",
        "    #fst = Fitzpatrick_Skin_Type[f\"_{get_kinyanjui_groh_type(image_ita)}\"]\n",
        "\n",
        "\n",
        "\n",
        "    print(i, row[\"dcm_name\"], row_idx, col_idx, hand_picked_id)\n",
        "    img = mpimg.imread(img_path)\n",
        "    \n",
        "    axs[row_idx, col_idx].imshow(img)\n",
        "    if row_idx == 0:\n",
        "        axs[row_idx, col_idx].set_title(row['fitzpatrick_skin_type_masked'])\n",
        "    if row_idx == 2:\n",
        "        image_ita = get_cropped_center_ita(Image.open(\"test.jpg\"))        \n",
        "        axs[row_idx, col_idx].set_title(get_kinyanjui_groh_type(image_ita))\n",
        "    #else:\n",
        "    #    axs[row_idx, col_idx].set_title(f\"[{target}]{row_idx}\" )\n",
        "    axs[row_idx, col_idx].axis('off')\n",
        "    \n",
        "    row_idx += 1   # Compute the column index\n",
        "    if row_idx == sample_count:\n",
        "        row_idx = 0  # Compute the row index\n",
        "        col_idx += 1\n",
        "    \n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9RF2M02_r8Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg0uRPQBgA8v"
      },
      "source": [
        "### Setting up models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccuen4_00vAC"
      },
      "outputs": [],
      "source": [
        "# get all models from Google drive\n",
        "%cp /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/model_checkpoints/*.pth /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALs9i_390qwy"
      },
      "outputs": [],
      "source": [
        "# get all the different model checkpoints and save them off\n",
        "resNet50 = \"\"\n",
        "efficientNet_B2 = \"\"\n",
        "efficientNet_B4 = \"\"\n",
        "efficientNet_B7 = \"\"\n",
        "\n",
        "resNet50_used_reg = \"\"\n",
        "efficientNet_B2_used_reg = \"\"\n",
        "efficientNet_B4_used_reg = \"\"\n",
        "efficientNet_B7_used_reg = \"\"\n",
        "\n",
        "resNet50_synthetic_50_percent = \"\"\n",
        "efficientNet_B2_synthetic_50_percent = \"\"\n",
        "efficientNet_B4_synthetic_50_percent = \"\"\n",
        "efficientNet_B7_synthetic_50_percent = \"\"\n",
        "\n",
        "for fn in Path(\"\").glob(\"*.pth\"):\n",
        "    fn_lower = str(fn).lower()\n",
        "    print(fn_lower)\n",
        "    if \"resnet50\" in fn_lower and \"synthetic\" in fn_lower:\n",
        "        resNet50_synthetic_50_percent = fn\n",
        "    elif \"resnet50\" in fn_lower and \"reg_used\" in fn_lower:\n",
        "        resNet50_used_reg = fn\n",
        "    elif \"resnet50\" in fn_lower and \"reg_used\" not in fn_lower:\n",
        "        resNet50 = fn\n",
        "    elif \"efficientnet\" in fn_lower and \"b2\" in fn_lower and \"synthetic\" in fn_lower:\n",
        "        efficientNet_B2_synthetic_50_percent = fn    \n",
        "    elif \"efficientnet\" in fn_lower and \"b2\" in fn_lower and \"reg_used\" not in fn_lower:\n",
        "        efficientNet_B2 = fn    \n",
        "    elif \"efficientnet\" in fn_lower and \"b2\" in fn_lower and \"reg_used\" in fn_lower:\n",
        "        efficientNet_B2_used_reg = fn    \n",
        "    elif \"efficientnet\" in fn_lower and \"b4\" in fn_lower and \"synthetic\" in fn_lower:\n",
        "        efficientNet_B4_synthetic_50_percent = fn\n",
        "    elif \"efficientnet\" in fn_lower and \"b4\" in fn_lower and \"reg_used\" not in fn_lower:\n",
        "        efficientNet_B4 = fn\n",
        "    elif \"efficientnet\" in fn_lower and \"b4\" in fn_lower and \"reg_used\" in fn_lower:\n",
        "        efficientNet_B4_used_reg = fn    \n",
        "    elif \"efficientnet\" in fn_lower and \"b7\" in fn_lower and \"synthetic\" in fn_lower:\n",
        "        efficientNet_B7_synthetic_50_percent = fn\n",
        "    elif \"efficientnet\" in fn_lower and \"b7\" in fn_lower and \"reg_used\" not in fn_lower:\n",
        "        efficientNet_B7 = fn\n",
        "    elif \"efficientnet\" in fn_lower and \"b7\" in fn_lower and \"reg_used\" in fn_lower:\n",
        "        efficientNet_B7_used_reg = fn\n",
        "    \n",
        "\n",
        "print(f\"\"\"resNet50 = {resNet50}\n",
        "efficientNet_B2 = {efficientNet_B2}\n",
        "efficientNet_B4 = {efficientNet_B4}\n",
        "efficientNet_B7 = {efficientNet_B7}\n",
        "resNet50_used_reg = {resNet50_used_reg}\n",
        "efficientNet_B2_used_reg = {efficientNet_B2_used_reg}\n",
        "efficientNet_B4_used_reg = {efficientNet_B4_used_reg}\n",
        "efficientNet_B7_used_reg = {efficientNet_B7_used_reg}\n",
        "resNet50_synthetic_50_percent = {resNet50_synthetic_50_percent}\n",
        "efficientNet_B2_synthetic_50_percent = {efficientNet_B2_synthetic_50_percent}\n",
        "efficientNet_B4_synthetic_50_percent = {efficientNet_B4_synthetic_50_percent}\n",
        "efficientNet_B7_synthetic_50_percent = {efficientNet_B7_synthetic_50_percent} \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXmlCNn-0zXn"
      },
      "outputs": [],
      "source": [
        "# create an array of all the models so we can loop over them\n",
        "models = [ \\\n",
        "          resNet50, \\\n",
        "          efficientNet_B2, \\\n",
        "          efficientNet_B4, \\\n",
        "          efficientNet_B7, \\\n",
        "          resNet50_used_reg, \\\n",
        "          efficientNet_B2_used_reg, \\\n",
        "          efficientNet_B4_used_reg, \\\n",
        "          efficientNet_B7_used_reg, \\\n",
        "          resNet50_synthetic_50_percent, \\\n",
        "          efficientNet_B2_synthetic_50_percent, \\\n",
        "          efficientNet_B4_synthetic_50_percent, \\\n",
        "          efficientNet_B7_synthetic_50_percent \\\n",
        "          ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of8OhFKwKVLq"
      },
      "outputs": [],
      "source": [
        "validation_dataset_df = valid_min_count_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVRYk4OBL_Xs"
      },
      "source": [
        "#### Run tests"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knHhLM6JPTWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT73Ym3T1A98"
      },
      "outputs": [],
      "source": [
        "all_model_results = []\n",
        "for model_fn in models:\n",
        "    print(f\"### Running {model_fn}####\")\n",
        "    # set up model\n",
        "\n",
        "    model_fn = str(model_fn)\n",
        "\n",
        "    if \"EfficientNet\" in model_fn:        \n",
        "        if \"B2\" in model_fn:\n",
        "            model_name = \"EfficientNet-B2\"\n",
        "            b2 = True\n",
        "            b4 = False\n",
        "        elif \"B4\" in model_fn:\n",
        "            model_name = \"EfficientNet-B4\"\n",
        "            b2 = False\n",
        "            b4 = True\n",
        "        else:\n",
        "            model_name = \"EfficientNet-B7\"\n",
        "            b2 = False\n",
        "            b4 = False\n",
        "        best_model = EfficientNetworkImageOnly(output_size = output_size, b4=b4, b2=b2).to(device)\n",
        "        \n",
        "    else:\n",
        "        model_name = \"ResNet50\"\n",
        "        best_model = ResNet50NetworkImageOnly(output_size=output_size).to(device)\n",
        "\n",
        "    if \"reg_used\" in model_fn:\n",
        "        traning_method = \"Regularization\"\n",
        "    elif \"Synthetic\" in model_fn:\n",
        "        traning_method = \"Synthetic blend\"\n",
        "    else:\n",
        "        traning_method = \"Baseline\"\n",
        "\n",
        "    best_model.load_state_dict(torch.load(model_fn, map_location=torch.device(device)))\n",
        "    mask_path = Path(\"../input/melanoma_masks/results1\")\n",
        "    group_key = \"ID\"\n",
        "    k = 6\n",
        "    oof, oof_transformed, oof_transformed_without_mask  = test_folds(best_model, validation_dataset_df, mask_path, group_key, k, \"v00\")\n",
        "\n",
        "    oof_df = pd.DataFrame(data = oof, columns=['oof'])    \n",
        "    oof_transformed_df = pd.DataFrame(data = oof_transformed, columns=['oof'])\n",
        "    oof_transformed_without_mask_df = pd.DataFrame(data = oof_transformed_without_mask, columns=['oof']) \n",
        "    print(\"---Fairness metrics---\")    \n",
        "    disp_imp_1, stat_par_1, eq_odds_1, pred_rate_par_1, abra_1 = compute_fairness_metrics(oof_df, validation_dataset_df)\n",
        "    disp_imp_2, stat_par_2, eq_odds_2, pred_rate_par_2, abra_2 = compute_fairness_metrics(oof_transformed_df, validation_dataset_df)\n",
        "    disp_imp_3, stat_par_3, eq_odds_3, pred_rate_par_3, abra_3 = compute_fairness_metrics(oof_transformed_without_mask_df, validation_dataset_df)\n",
        "\n",
        "    results = {\n",
        "        \"Original\": {\"Disparate Impact\": disp_imp_1, \n",
        "                \"Statistical Parity\": stat_par_1, \n",
        "                \"Equalized Odds\": eq_odds_1, \n",
        "                \"Predictive Rate Parity\": pred_rate_par_1,\n",
        "                \"Absolute Between-ROC Area\": abra_1},\n",
        "        \"Transformed with masks\": {\"Disparate Impact\": disp_imp_2, \n",
        "                \"Statistical Parity\": stat_par_2, \n",
        "                \"Equalized Odds\": eq_odds_2, \n",
        "                \"Predictive Rate Parity\": pred_rate_par_2,\n",
        "                \"Absolute Between-ROC Area\": abra_2},\n",
        "        \"Transformed w/o masks\": {\"Disparate Impact\": disp_imp_3, \n",
        "                \"Statistical Parity\": stat_par_3, \n",
        "                \"Equalized Odds\": eq_odds_3, \n",
        "                \"Predictive Rate Parity\": pred_rate_par_3,\n",
        "                \"Absolute Between-ROC Area\": abra_3}\n",
        "    }\n",
        "    df = pd.DataFrame.from_dict(results, orient='index')\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "    print(\"---Confusion metrics---\")\n",
        "    oof_df = pd.DataFrame(data = oof, columns=['oof'])    \n",
        "    oof_transformed_df = pd.DataFrame(data = oof_transformed, columns=['oof'])\n",
        "    oof_transformed_without_mask_df = pd.DataFrame(data = oof_transformed_without_mask, columns=['oof'])    \n",
        "    roc_overall, ROC_array = create_confusion_matrix_agg(oof_df, validation_dataset_df, \"FST\")\n",
        "    create_confusion_matrix_agg(oof_transformed_df, validation_dataset_df, \"Confusion Matrix:\\nTransformed & \\n Masked\")\n",
        "    create_confusion_matrix_agg(oof_transformed_without_mask_df, validation_dataset_df, \"Confusion Matrix:\\nTransformed\\nw/o mask\")\n",
        "\n",
        "    oof_df = pd.DataFrame(data = oof, columns=['oof'])    \n",
        "    oof_transformed_df = pd.DataFrame(data = oof_transformed, columns=['oof'])\n",
        "    oof_transformed_without_mask_df = pd.DataFrame(data = oof_transformed_without_mask, columns=['oof'])\n",
        "    \n",
        "    print(\"---ROC Curves---\")\n",
        "    labels = validation_dataset_df['target']\n",
        "    y_pred = oof_df['oof']\n",
        "    display_roc(labels, y_pred, \"Original\")\n",
        "\n",
        "    y_pred = oof_transformed_df['oof']\n",
        "    display_roc(labels, y_pred, \"Transformed with masks\")\n",
        "\n",
        "    y_pred = oof_transformed_without_mask_df['oof']\n",
        "    display_roc(labels, y_pred, \"Transformed w/o\")\n",
        "\n",
        "    model_results = {\n",
        "        \"Model\":model_name,\n",
        "        \"Training method\":traning_method,\n",
        "        \"Overall AUC\":roc_overall,\n",
        "        \"Type 1\": ROC_array[0],\n",
        "        \"Type 2\": ROC_array[1],\n",
        "        \"Type 3\": ROC_array[2],\n",
        "        \"Type 4\": ROC_array[3],\n",
        "        \"Type 5\": ROC_array[4],\n",
        "        \"Type 6\": ROC_array[5],\n",
        "        \"Disparate Impact\": disp_imp_1,          \n",
        "        \"Equalized Odds\": eq_odds_1, \n",
        "        \"Predictive Rate Parity\": pred_rate_par_1,\n",
        "        \"Absolute Between-ROC Area\": abra_1\n",
        "    }\n",
        "    all_model_results.append(model_results)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    print(\"______________________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Display graphs\n",
        "This section will display graph results based on the validation dataset\n",
        "\n",
        "1. line graph comparing overall AUC vs traning methods per model\n",
        "1. line graphs based on FSK accross the different  training methods\n"
      ],
      "metadata": {
        "id": "Fbz_VxBSNAp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_results"
      ],
      "metadata": {
        "id": "h1zmXPqZdVr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model_results = pd.DataFrame(all_model_results)\n",
        "filtered_columnes = [\"Model\",\"Training method\", \"Disparate Impact\", \"Equalized Odds\", \"Predictive Rate Parity\", \"Absolute Between-ROC Area\"]\n",
        "df_selected = df_model_results[filtered_columnes]\n",
        "\n",
        "grouped_df = df_selected.groupby(['Training method', 'Model']).mean()\n",
        "\n",
        "grouped_df"
      ],
      "metadata": {
        "id": "sn5o_RFPNQk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model_results = pd.DataFrame(all_model_results)\n",
        "\n",
        "grouped_df = df_model_results.groupby(['Training method','Model']).mean().reset_index()\n",
        "\n",
        "grouped_df"
      ],
      "metadata": {
        "id": "b8KtUiMNnPEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a line plot\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "# Define a list of symbols to use for each model\n",
        "symbols = ['o', 's', '^', 'v']\n",
        "\n",
        "for i, model in enumerate(grouped_df['Model'].unique()):\n",
        "    if \"B7\" in model:\n",
        "        continue\n",
        "    model_df = grouped_df[grouped_df['Model'] == model]\n",
        "    # Use a different symbol for each model\n",
        "    symbol = symbols[i % len(symbols)]\n",
        "    \n",
        "    # Use a dotted line style\n",
        "    ax.plot(model_df['Training method'], model_df['Overall AUC'], linestyle='--', marker=symbol, markersize=12, alpha=0.6, label=model)\n",
        "\n",
        "#ax.set_xlabel('Training Method', fontsize=10)\n",
        "ax.set_ylabel('AUC', fontsize=10)\n",
        "ax.set_title('Overall AUC Model Performance by Training Method', fontsize=12)\n",
        "ax.legend(loc='upper left', fontsize=10)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2QqDFQYlWX60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a line plot\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "for model in grouped_df['Model'].unique():\n",
        "    model_df = grouped_df[grouped_df['Model'] == model]\n",
        "    ax.plot(model_df['Training method'], model_df['Overall AUC'], label=model)\n",
        "ax.set_xlabel('Training Method')\n",
        "ax.set_ylabel('AUC')\n",
        "ax.set_title('Overall AUC Model Performance by Training Method')\n",
        "ax.legend(loc='upper left', fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sNt37LFxvXND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_df = df_model_results[df_model_results['Model'] == 'EfficientNet-B2']\n",
        "\n",
        "# set the X axis to be the Type columns\n",
        "x_cols = ['Type 1', 'Type 2', 'Type 3', 'Type 4', 'Type 5', 'Type 6']\n",
        "\n",
        "\n",
        "# create a line plot for each training method\n",
        "fig, ax = plt.subplots()\n",
        "for method in effnet_df['Training method'].unique():\n",
        "    method_df = effnet_df[effnet_df['Training method'] == method]\n",
        "    ax.plot(x_cols, method_df[x_cols].values[0], label=method)\n",
        "\n",
        "# set the title and labels\n",
        "ax.set_title('EfficientNet-B2 Model Performance by Training Method')\n",
        "ax.set_xlabel('Type')\n",
        "ax.set_ylabel('AUC')\n",
        "\n",
        "# set the legend to the top left and make it smaller\n",
        "ax.legend(loc='lower left', fontsize='small')\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MkrPXY-CdM_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_df = df_model_results[df_model_results['Model'] == 'EfficientNet-B2']\n",
        "\n",
        "# set the X axis to be the Type columns\n",
        "x_cols = ['Type 1', 'Type 2', 'Type 3', 'Type 4', 'Type 5', 'Type 6']\n",
        "\n",
        "\n",
        "# create a line plot for each training method\n",
        "fig, ax = plt.subplots()\n",
        "for method in effnet_df[effnet_df['Training method'] != 'Regularization']['Training method'].unique():\n",
        "    method_df = effnet_df[effnet_df['Training method'] == method]\n",
        "    ax.plot(x_cols, method_df[x_cols].values[0], label=method)\n",
        "\n",
        "# set the title and labels\n",
        "ax.set_title('EfficientNet-B2 Model Performance by Training Method')\n",
        "ax.set_xlabel('Type')\n",
        "ax.set_ylabel('AUC')\n",
        "\n",
        "# set the legend to the top left and make it smaller\n",
        "ax.legend(loc='lower left', fontsize='small')\n",
        "\n",
        "# display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WKclo33jkyrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select data for EfficientNet-B2 model\n",
        "model_data = df_model_results[df_model_results['Model'] == 'EfficientNet-B2']\n",
        "\n",
        "# select the columns for the plot\n",
        "plot_data = model_data[['Training method', 'Type 1', 'Type 2', 'Type 3', 'Type 4', 'Type 5', 'Type 6']]\n",
        "\n",
        "# set the index to 'Training method'\n",
        "plot_data = plot_data.set_index('Training method')\n",
        "\n",
        "# create the plot with transparency\n",
        "plot_data.plot(kind='line', alpha=0.5, figsize=(5, 5))\n",
        "\n",
        "# set the x and y axis labels\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Overall AUC')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zHRmMeZZfHZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyFWwLrJbidu"
      },
      "source": [
        "# XAI\n",
        "This section will go over how to implement Captum's libary with different XAI tecniques "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd8J6DNOcxlp"
      },
      "outputs": [],
      "source": [
        "mask_path = Path(\"/input/melanoma_masks/results1\")\n",
        "example_data = MelanomaDataset(valid_df, vertical_flip=0.5, horizontal_flip=0.5, \n",
        "                               is_train=True, is_valid=False, is_test=False, generate_skin_transform=True, mask_path=mask_path)\n",
        "example_loader = torch.utils.data.DataLoader(example_data, batch_size = 3, shuffle=True)\n",
        "\n",
        "\n",
        "def get_an_example():    \n",
        "    # Get a sample\n",
        "    for (images, csv_data, image_path, ita_value, fst, transformed_image, transformed_image_wihtout_mask), labels in example_loader:\n",
        "        mask_path = Path(\"/input/melanoma_masks/results1\")\n",
        "        image_example, csv_data_example = images, csv_data\n",
        "        \n",
        "        image_rgb = cv2.imread(image_path[0])\n",
        "        image_rgb = cv2.resize(image_rgb, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "        file_name = Path(image_path[0]).name\n",
        "        print(file_name)\n",
        "\n",
        "        updated_mask_path = mask_path / str(file_name).replace(\"jpg\", \"png\")\n",
        "        print(updated_mask_path)\n",
        "\n",
        "        \n",
        "        im = Image.open(str(updated_mask_path))\n",
        "        \n",
        "        #mask_rgb = cv2.imread(str(updated_mask_path))\n",
        "        #mask_rgb = cv2.resize(mask_rgb, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
        "        mask = im.resize((224, 224))\n",
        "        \n",
        "        mask = mask.rotate(270)\n",
        "        #im = im.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "        #mask = mask.rotate(180)\n",
        "        mask = mask.transpose(Image.FLIP_LEFT_RIGHT)        \n",
        "\n",
        "        \n",
        "        #csv_data_example = csv_data_example.to(device)\n",
        "        image_example = torch.tensor(image_example, device=device, dtype=torch.float32)\n",
        "        labels_example = torch.tensor(labels, device=device, dtype=torch.int64)\n",
        "        return image_example, image_path, image_rgb, labels_example, ita_value, fst, mask, transformed_image, transformed_image_wihtout_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXIRkkv9_MVo"
      },
      "outputs": [],
      "source": [
        "file_count = len(list(mask_path.glob(\"*.png\")))\n",
        "file_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4AodOCUV7Xl"
      },
      "source": [
        "## pytorch_grad_cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXdm9IYNeQ5l"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmLBez3wf6g8"
      },
      "outputs": [],
      "source": [
        "for i, n in enumerate(models):\n",
        "    print(i, str(n))\n",
        "\n",
        "model_fn = str(models[9])\n",
        "selected_model = \"\"\n",
        "\n",
        "if \"EfficientNet\" in model_fn:\n",
        "    selected_model = \"EfficientNet\"\n",
        "    if \"B2\" in model_fn:\n",
        "        b2 = True\n",
        "        b4 = False\n",
        "    elif \"B4\" in model_fn:\n",
        "        b2 = False\n",
        "        b4 = True\n",
        "    else:\n",
        "        b2 = False\n",
        "        b4 = False\n",
        "    best_model = EfficientNetworkImageOnly(output_size = output_size, b4=b4, b2=b2).to(device)\n",
        "    \n",
        "else:\n",
        "    selected_model = \"ResNet50\"\n",
        "    best_model = ResNet50NetworkImageOnly(output_size=output_size).to(device)\n",
        "best_model.load_state_dict(torch.load(model_fn, map_location=torch.device(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V8TLhBfVmvq"
      },
      "outputs": [],
      "source": [
        "target_layers = [best_model.features._conv_head] # EfficientNet \n",
        "#target_layers = [best_model.features._conv_head, best_model.features._bn1, best_model.features._bn0] # EfficientNet \n",
        "#target_layers = [best_model.features.layer4[-1]] # ResNet50 layers\n",
        "cam = GradCAM(model=best_model, target_layers=target_layers, use_cuda=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k-d0EVMgpCg"
      },
      "outputs": [],
      "source": [
        "#TODO- Fix get example to pass in params of things that we want\n",
        "image_example, image_path, image_rgb, labels_example, ita_value, fst, mask, transformed_image, transformed_image_wihtout_mask = get_an_example()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVt8ZeYz_mvv"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mask)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXZ3SqGsgbhX"
      },
      "outputs": [],
      "source": [
        "cv2_img = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
        "plt.imshow(cv2_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9vhqh_o5Hwd"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMC0N-Ln5QRU"
      },
      "outputs": [],
      "source": [
        "transformed_image = transform_image(cv2_img, mask, image_ita=ita_value[0], verbose=True)\n",
        "transformed_image_tensor = torch.tensor(transformed_image, device=device, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUTOnD7XCB-P"
      },
      "outputs": [],
      "source": [
        "def get_visualization(input_image, input_image_tensor, model, target_layers=target_layers,  use_cuda=True):\n",
        "    print(\"input_image.shape\", input_image.shape)\n",
        "    print(\"input_image_tensor.shape\", input_image_tensor.shape)\n",
        "    print(\"target_layers\", target_layers)\n",
        "    visualization_negative, visualization_positive, out = None, None, None\n",
        "    model_to_use_for_grad_cam = SingleOutputModel(model)\n",
        "    with GradCAM(model=model_to_use_for_grad_cam, target_layers=target_layers, use_cuda=True) as cam:\n",
        "        #cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
        "        out, _ = model(input_image_tensor)\n",
        "        print(out[0])\n",
        "        out = torch.sigmoid(out)\n",
        "        print(out[0])\n",
        "        targets_neg = [BinaryClassifierOutputTarget(0)]\n",
        "        targets_pos = [BinaryClassifierOutputTarget(1)]\n",
        "\n",
        "        # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "        #print(\"input_image_tensor\", input_image_tensor)\n",
        "        #print(\"targets_neg\", targets_neg)\n",
        "        grayscale_cam_neg = cam(input_tensor=input_image_tensor, targets=targets_neg, aug_smooth=True )\n",
        "        grayscale_cam_pos = cam(input_tensor=input_image_tensor, targets=targets_pos, aug_smooth=True )\n",
        "\n",
        "        #print(grayscale_cam_neg.shape)\n",
        "        np_image = np.array(input_image)\n",
        "        np_image = np_image.astype(np.float32) / 255\n",
        "        #print(np_image.shape)\n",
        "\n",
        "        # In this example grayscale_cam has only one image in the batch:\n",
        "        grayscale_cam_negative = grayscale_cam_neg[0, :]\n",
        "        visualization_negative = show_cam_on_image(np_image, grayscale_cam_negative, use_rgb=True)\n",
        "        grayscale_cam_positive = grayscale_cam_pos[0, :]\n",
        "        visualization_positive = show_cam_on_image(np_image, grayscale_cam_positive, use_rgb=True)\n",
        "\n",
        "    return visualization_negative, visualization_positive, out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEHob4m2WBv_"
      },
      "outputs": [],
      "source": [
        "def run_map(image, image_tensor, transformed_image, transformed_image_tensor, transformed_no_mask_image, transformed_no_mask_image_tensor, model, target_layers, labels_example): \n",
        "\n",
        "    visualization_negative, visualization_positive, out = get_visualization(cv2_img, image_tensor, model=model)   \n",
        "    print(\"Out:\", out[0].item())\n",
        "    trans_visualization_negative, trans_visualization_positive, trans_out = get_visualization(transformed_image, transformed_image_tensor, model=model)   \n",
        "    trans_no_mask_visualization_negative, trans_no_mask_visualization_positive, trans_no_mask_out = get_visualization(transformed_no_mask_image, transformed_no_mask_image_tensor, model=model)   \n",
        "              \n",
        "    #print(visualization_negative)\n",
        "    fig, axs = plt.subplots(3, 2, figsize=(10, 7))\n",
        "    label = \"Benign\" if labels_example[0].item() == 0 else \"Malignant\"\n",
        "    axs[0][0].imshow(image)\n",
        "    axs[0][0].set_title(f\"Orig({label})\")\n",
        "    \n",
        "    if out[0].item() <.5:\n",
        "        axs[0][1].imshow(visualization_positive)\n",
        "        axs[0][1].set_title(f\"Benign({out[0].item():.3f})\" )\n",
        "    else:\n",
        "    \n",
        "        axs[0][1].imshow(visualization_positive)\n",
        "        axs[0][1].set_title(f\"Malignant({out[0].item():.3f})\")\n",
        "\n",
        "\n",
        "    axs[1][0].imshow(transformed_image)\n",
        "    axs[1][0].set_title(f\"Transformed\")\n",
        "    if trans_out[0].item() <.5:\n",
        "        axs[1][1].imshow(trans_visualization_positive)\n",
        "        axs[1][1].set_title(f\"Benign({trans_out[0].item():.3f})\")\n",
        "    else:\n",
        "        axs[1][1].imshow(trans_visualization_positive)\n",
        "        axs[1][1].set_title(f\"Malignant({trans_out[0].item():.3f})\")\n",
        "\n",
        "    axs[2][0].imshow(transformed_no_mask_image)\n",
        "    axs[2][0].set_title(f\"Transformed\\nno mask\")\n",
        "    if trans_no_mask_out[0].item() <.5:    \n",
        "        axs[2][1].imshow(trans_no_mask_visualization_positive)\n",
        "        axs[2][1].set_title(f\"Benign({trans_no_mask_out[0].item():.3f})\")\n",
        "    else:\n",
        "        axs[2][1].imshow(trans_no_mask_visualization_positive)\n",
        "        axs[2][1].set_title(f\"Malignant({trans_no_mask_out[0].item():.3f})\")\n",
        "    plt.tight_layout()\n",
        "    del visualization_negative, visualization_positive, trans_visualization_negative, trans_visualization_positive, trans_no_mask_visualization_negative, trans_no_mask_visualization_positive\n",
        "\n",
        "    #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_f-4bwzIi1F"
      },
      "outputs": [],
      "source": [
        "transform = Compose([Normalize(),ToTensorV2()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWwD76f3hnEj"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdHZCXmQtwKd"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_example, image_path, image_rgb, labels_example, ita_value, fst, mask, transformed_image, transformed_image_wihtout_mask = get_an_example()\n",
        "\n",
        "print(\"Label\", labels_example[0])\n",
        "transformed_image = transform_image(image_rgb, mask, image_ita=ita_value[0], verbose=False)\n",
        "transformed_image_no_mask = transform_image(image_rgb, image_ita=ita_value[0], verbose=False)\n",
        "#transformed_image = cv2.imread(\"test.jpg\") \n",
        "#if transformed_image.shape[2] == 3:##\n",
        "\n",
        "    #print(\"Converting\",transformed_image.shape)\n",
        "    #transformed_image = cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR)\n",
        "    #print(\"to\",transformed_image.shape)\n",
        "\n",
        "# Format transformed image to tensor\n",
        "transformed_image_tensor = transform(image=transformed_image)\n",
        "transformed_image_tensor = transformed_image_tensor['image']\n",
        "transformed_image_tensor = transformed_image_tensor.reshape(1, 3, 224, 224)\n",
        "transformed_image_tensor = torch.tensor(transformed_image_tensor, device=device, dtype=torch.float32)\n",
        "\n",
        "cv2_img = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Format transformed image to tensor\n",
        "transformed_image_no_mask_tensor = transform(image=transformed_image_no_mask)\n",
        "transformed_image_no_mask_tensor = transformed_image_no_mask_tensor['image']\n",
        "transformed_image_no_mask_tensor = transformed_image_no_mask_tensor.reshape(1, 3, 224, 224)\n",
        "transformed_image_no_mask_tensor = torch.tensor(transformed_image_no_mask_tensor, device=device, dtype=torch.float32)\n",
        "\n",
        "if selected_model == \"ResNet50\":\n",
        "    target_layers = [best_model.features._conv_head] # this is for EfficientNet\n",
        "else:\n",
        "    target_layers = [best_model.features._conv_head, best_model.features._bn1, best_model.features._bn0] # this is for EfficientNet\n",
        "#target_layers = [best_model.features.layer4[-1]] # this is for ResNet50\n",
        "print(\"target layers\", target_layers)\n",
        "run_map(cv2_img, image_example, transformed_image, transformed_image_tensor, transformed_image_no_mask, transformed_image_no_mask_tensor, best_model, target_layers, labels_example)\n",
        "\n",
        "del transformed_image_no_mask_tensor, transformed_image_tensor\n",
        "del image_example, image_path, image_rgb, labels_example, ita_value, fst, mask, transformed_image, transformed_image_wihtout_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yde6snHa9kyE"
      },
      "source": [
        "## Test sample set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAgiKVmxB0Et"
      },
      "outputs": [],
      "source": [
        "valid_min_count_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DPgVJ5qleMD"
      },
      "outputs": [],
      "source": [
        "#def display_sample_grid_grad_cam(best_model, valid_min_count_df):\n",
        "np.random.seed(23) # used to get reproducable. sample images\n",
        "# Sample 12 rows of each category within the Fitzpatrick Skin Type column\n",
        "sample_count = 3\n",
        "sampled_valid_min_count_df = valid_min_count_df.groupby('fitzpatrick_skin_type_masked').apply(lambda x: x.sample(sample_count))\n",
        "\n",
        "# Initialize the figure\n",
        "fig, axs = plt.subplots(sample_count, 6, figsize=(18, 10))\n",
        "\n",
        "row_idx = 0 \n",
        "col_idx = 0\n",
        "# Loop through the sampled DataFrame and display the images\n",
        "for i, row in sampled_valid_min_count_df.iterrows():\n",
        "\n",
        "    if row_idx < 3:\n",
        "        hand_picked_id = hand_picked_images[row_idx][col_idx]\n",
        "        if len(hand_picked_id) > 1:\n",
        "            print(\"hand_picked_id\", hand_picked_id)\n",
        "            hand_picked = valid_min_count_df[valid_min_count_df[\"dcm_name\"] == hand_picked_id]\n",
        "            hand_picked_path = hand_picked['path_jpg'].values[0]\n",
        "            target = hand_picked['target'].values[0]\n",
        "        else:\n",
        "            hand_picked_path = \"\"\n",
        "            target = row['target']\n",
        "        print(\"hand_picked_path\", hand_picked_path)\n",
        "    else:\n",
        "        hand_picked_path = \"\"\n",
        "    \n",
        "    if len(hand_picked_path) > 0:\n",
        "        img_path = hand_picked_path\n",
        "    else:\n",
        "        img_path = row['path_jpg']\n",
        "    print(\"row['path_jpg']\", row['path_jpg'])\n",
        "\n",
        "    print(i, row[\"ID\"], row_idx, col_idx,f\"Target: {row['target']}\")\n",
        "    #img_path = row['path_jpg']\n",
        "    \n",
        "    if selected_model == \"ResNet50\":\n",
        "        target_layers = [best_model.features._conv_head] \n",
        "    else:\n",
        "        target_layers = [best_model.features._conv_head, best_model.features._bn1, best_model.features._bn0] # this is for EfficientNet\n",
        "        #target_layers = [best_model.features._bn0] # this is for EfficientNet\n",
        "    #target_layers = [best_model.features.layer4[-1]] # this is for ResNet50\n",
        "#    print(\"target layers\", target_layers)\n",
        "\n",
        "\n",
        "    #Setting up image\n",
        "    image_rgb = cv2.imread(img_path) \n",
        "    if image_rgb.shape[:2] != (224,224):\n",
        "        image_rgb = cv2.resize(image_rgb, (224,224))\n",
        "    \n",
        "    \n",
        "    image_tensor = transform(image=image_rgb)\n",
        "    image_tensor = image_tensor['image']\n",
        "    image_tensor = torch.tensor(image_tensor, device=device, dtype=torch.float32)\n",
        "    image_tensor = image_tensor.unsqueeze(dim=0) # this converts to 3d to 4d with batch size 1\n",
        "    \n",
        "    cv2_img = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    visualization_negative, visualization_positive, out = get_visualization(cv2_img, image_tensor, model=best_model, target_layers=target_layers)\n",
        "\n",
        "    #print(\"visualization_negative\", visualization_negative)\n",
        "    if out[0].item() <.5:\n",
        "        axs[row_idx, col_idx].imshow(visualization_positive)\n",
        "        axs[row_idx, col_idx].set_title(f\"B({out[0].item():.2f})[{target}]\" )\n",
        "    else:\n",
        "    \n",
        "        axs[row_idx, col_idx].imshow(visualization_positive)\n",
        "        axs[row_idx, col_idx].set_title(f\"M({out[0].item():.3f})[{target}]\")  \n",
        "    \n",
        "    #run_map(cv2_img, image_example, transformed_image, transformed_image_tensor, transformed_image_no_mask, transformed_image_no_mask_tensor, best_model, target_layers, labels_example)\n",
        "\n",
        "        \n",
        "    #axs[row_idx, col_idx].imshow(img)\n",
        "    #if row_idx == 0:\n",
        "        #axs[row_idx, col_idx].set_title(row['fitzpatrick_skin_type_masked'])\n",
        "    axs[row_idx, col_idx].axis('off')\n",
        "    \n",
        "    row_idx += 1   # Compute the column index\n",
        "    if row_idx == sample_count:\n",
        "        row_idx = 0  # Compute the row index\n",
        "        col_idx += 1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou02hjbu8J5m"
      },
      "outputs": [],
      "source": [
        "#Examples of weird cases\n",
        "#../input/melanoma-external-malignant-256/train/train/ISIC_0183256.jpg\n",
        "#/input/melanoma_masks/results1/ISIC_6624321.png\n",
        "#/input/melanoma_masks/results1/ISIC_8716644.png\n",
        "#/input/melanoma_masks/results1/ISIC_8622941.png\n",
        "\n",
        "#Org benign but classifier labled malignant\n",
        "\"\"\"\n",
        "/input/melanoma_masks/results1/ISIC_1197692.png\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNgXnaPb6e3q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUfVaoAePJoL"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "model = resnet50(pretrained=True)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPYIAr6tPL0u"
      },
      "outputs": [],
      "source": [
        "summary(model, (3,224,224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuNcfycLD7EW"
      },
      "source": [
        "# References: ðŸ“‡\n",
        "* [Melanoma. Pytorch starter. EfficientNet](https://www.kaggle.com/nroman/melanoma-pytorch-starter-efficientnet/output)\n",
        "* [melanoma external malignant 256](https://www.kaggle.com/nroman/melanoma-external-malignant-256)\n",
        "* [Understanding ResNet50 architecture](https://iq.opengenus.org/resnet50-architecture/#:~:text=ResNet50%20is%20a%20variant%20of,have%20explored%20this%20in%20depth.)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KeK-VVPdD7EO"
      ],
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}